{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Visualizer, TrainIntervalLogger, TestLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 4    observations: (128,)\n"
     ]
    }
   ],
   "source": [
    "# makes the enviroment\n",
    "env = gym.make('Breakout-ram-v4')\n",
    "# old rom name: 'SpaceInvaders-ram-v4'\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "nb_obs = env.observation_space.shape\n",
    "\n",
    "print(\"actions:\", nb_actions, \"   observations:\", nb_obs)\n",
    "\n",
    "file_name = \"test_1\"\n",
    "\n",
    "NB_STEPS = 1200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "#file logger\n",
    "logger = FileLogger(f'training_logs_{file_name}.txt', interval=1) \n",
    "\n",
    "# saver callback\n",
    "weights_filename = f\"model/{file_name}_weights.h5f\"\n",
    "checkpoint_filename = f\"model/{file_name}_checkpoint.h5f\"\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_filename,interval=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 50,052\n",
      "Trainable params: 50,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + nb_obs))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the agent\n",
    "\n",
    "# setup the memory buffer\n",
    "memory = SequentialMemory(limit=NB_STEPS,window_length=1)\n",
    "\n",
    "# create the policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=NB_STEPS) \n",
    "# create the agent\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=50000,\n",
    "               target_model_update=10000, policy=policy, gamma=0.99) #removed batch size thing, maybe add back later if its actually important ig idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1200000 steps ...\n",
      "WARNING:tensorflow:From c:\\Users\\Zaid Salahuddin\\.conda\\envs\\eeenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "     251/1200000: episode: 1, duration: 0.590s, episode steps: 251, steps per second: 426, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     557/1200000: episode: 2, duration: 0.588s, episode steps: 306, steps per second: 521, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     795/1200000: episode: 3, duration: 0.464s, episode steps: 238, steps per second: 513, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1066/1200000: episode: 4, duration: 0.528s, episode steps: 271, steps per second: 514, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1368/1200000: episode: 5, duration: 0.567s, episode steps: 302, steps per second: 533, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1633/1200000: episode: 6, duration: 0.472s, episode steps: 265, steps per second: 561, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2073/1200000: episode: 7, duration: 0.820s, episode steps: 440, steps per second: 537, episode reward:  8.000, mean reward:  0.018 [ 0.000,  4.000], mean action: 1.502 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2350/1200000: episode: 8, duration: 0.525s, episode steps: 277, steps per second: 527, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2578/1200000: episode: 9, duration: 0.448s, episode steps: 228, steps per second: 509, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2923/1200000: episode: 10, duration: 0.656s, episode steps: 345, steps per second: 526, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3223/1200000: episode: 11, duration: 0.590s, episode steps: 300, steps per second: 509, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.343 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3460/1200000: episode: 12, duration: 0.470s, episode steps: 237, steps per second: 505, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3734/1200000: episode: 13, duration: 0.608s, episode steps: 274, steps per second: 451, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3935/1200000: episode: 14, duration: 0.402s, episode steps: 201, steps per second: 500, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4237/1200000: episode: 15, duration: 0.586s, episode steps: 302, steps per second: 515, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4579/1200000: episode: 16, duration: 0.645s, episode steps: 342, steps per second: 530, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4747/1200000: episode: 17, duration: 0.340s, episode steps: 168, steps per second: 494, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4990/1200000: episode: 18, duration: 0.467s, episode steps: 243, steps per second: 521, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5222/1200000: episode: 19, duration: 0.439s, episode steps: 232, steps per second: 529, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5523/1200000: episode: 20, duration: 0.574s, episode steps: 301, steps per second: 524, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5744/1200000: episode: 21, duration: 0.442s, episode steps: 221, steps per second: 500, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6044/1200000: episode: 22, duration: 0.582s, episode steps: 300, steps per second: 516, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6315/1200000: episode: 23, duration: 0.523s, episode steps: 271, steps per second: 518, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6485/1200000: episode: 24, duration: 0.329s, episode steps: 170, steps per second: 516, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6654/1200000: episode: 25, duration: 0.333s, episode steps: 169, steps per second: 508, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6937/1200000: episode: 26, duration: 0.543s, episode steps: 283, steps per second: 521, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7106/1200000: episode: 27, duration: 0.344s, episode steps: 169, steps per second: 491, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7317/1200000: episode: 28, duration: 0.411s, episode steps: 211, steps per second: 513, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7490/1200000: episode: 29, duration: 0.338s, episode steps: 173, steps per second: 512, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7737/1200000: episode: 30, duration: 0.472s, episode steps: 247, steps per second: 524, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8111/1200000: episode: 31, duration: 0.738s, episode steps: 374, steps per second: 507, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8361/1200000: episode: 32, duration: 0.493s, episode steps: 250, steps per second: 507, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8592/1200000: episode: 33, duration: 0.451s, episode steps: 231, steps per second: 512, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8763/1200000: episode: 34, duration: 0.341s, episode steps: 171, steps per second: 502, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9013/1200000: episode: 35, duration: 0.488s, episode steps: 250, steps per second: 512, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9191/1200000: episode: 36, duration: 0.365s, episode steps: 178, steps per second: 487, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9425/1200000: episode: 37, duration: 0.468s, episode steps: 234, steps per second: 500, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9720/1200000: episode: 38, duration: 0.558s, episode steps: 295, steps per second: 529, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9906/1200000: episode: 39, duration: 0.363s, episode steps: 186, steps per second: 512, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10179/1200000: episode: 40, duration: 0.529s, episode steps: 273, steps per second: 516, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10412/1200000: episode: 41, duration: 0.471s, episode steps: 233, steps per second: 494, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10583/1200000: episode: 42, duration: 0.341s, episode steps: 171, steps per second: 501, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10802/1200000: episode: 43, duration: 0.419s, episode steps: 219, steps per second: 522, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11053/1200000: episode: 44, duration: 0.489s, episode steps: 251, steps per second: 514, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11361/1200000: episode: 45, duration: 0.597s, episode steps: 308, steps per second: 516, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11787/1200000: episode: 46, duration: 0.791s, episode steps: 426, steps per second: 539, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12198/1200000: episode: 47, duration: 0.769s, episode steps: 411, steps per second: 534, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12430/1200000: episode: 48, duration: 0.457s, episode steps: 232, steps per second: 508, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12793/1200000: episode: 49, duration: 0.687s, episode steps: 363, steps per second: 528, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12966/1200000: episode: 50, duration: 0.338s, episode steps: 173, steps per second: 511, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13267/1200000: episode: 51, duration: 0.587s, episode steps: 301, steps per second: 513, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13648/1200000: episode: 52, duration: 0.817s, episode steps: 381, steps per second: 466, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13894/1200000: episode: 53, duration: 0.473s, episode steps: 246, steps per second: 520, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14121/1200000: episode: 54, duration: 0.450s, episode steps: 227, steps per second: 505, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14297/1200000: episode: 55, duration: 0.353s, episode steps: 176, steps per second: 498, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14514/1200000: episode: 56, duration: 0.431s, episode steps: 217, steps per second: 504, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14753/1200000: episode: 57, duration: 0.473s, episode steps: 239, steps per second: 505, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14982/1200000: episode: 58, duration: 0.444s, episode steps: 229, steps per second: 516, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15160/1200000: episode: 59, duration: 0.353s, episode steps: 178, steps per second: 504, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15401/1200000: episode: 60, duration: 0.469s, episode steps: 241, steps per second: 514, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15579/1200000: episode: 61, duration: 0.370s, episode steps: 178, steps per second: 481, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15823/1200000: episode: 62, duration: 0.487s, episode steps: 244, steps per second: 501, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16099/1200000: episode: 63, duration: 0.552s, episode steps: 276, steps per second: 500, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16341/1200000: episode: 64, duration: 0.543s, episode steps: 242, steps per second: 445, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16625/1200000: episode: 65, duration: 0.574s, episode steps: 284, steps per second: 495, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16826/1200000: episode: 66, duration: 0.419s, episode steps: 201, steps per second: 480, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17051/1200000: episode: 67, duration: 0.452s, episode steps: 225, steps per second: 498, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17429/1200000: episode: 68, duration: 0.755s, episode steps: 378, steps per second: 501, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17704/1200000: episode: 69, duration: 0.555s, episode steps: 275, steps per second: 496, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17940/1200000: episode: 70, duration: 0.490s, episode steps: 236, steps per second: 481, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18129/1200000: episode: 71, duration: 0.368s, episode steps: 189, steps per second: 514, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18299/1200000: episode: 72, duration: 0.354s, episode steps: 170, steps per second: 480, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18518/1200000: episode: 73, duration: 0.447s, episode steps: 219, steps per second: 490, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18702/1200000: episode: 74, duration: 0.360s, episode steps: 184, steps per second: 512, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18873/1200000: episode: 75, duration: 0.350s, episode steps: 171, steps per second: 489, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19148/1200000: episode: 76, duration: 0.545s, episode steps: 275, steps per second: 505, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19340/1200000: episode: 77, duration: 0.390s, episode steps: 192, steps per second: 492, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19706/1200000: episode: 78, duration: 0.698s, episode steps: 366, steps per second: 525, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19987/1200000: episode: 79, duration: 0.529s, episode steps: 281, steps per second: 532, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20167/1200000: episode: 80, duration: 0.383s, episode steps: 180, steps per second: 470, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20345/1200000: episode: 81, duration: 0.355s, episode steps: 178, steps per second: 501, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20522/1200000: episode: 82, duration: 0.355s, episode steps: 177, steps per second: 499, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20694/1200000: episode: 83, duration: 0.355s, episode steps: 172, steps per second: 485, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20859/1200000: episode: 84, duration: 0.335s, episode steps: 165, steps per second: 492, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21119/1200000: episode: 85, duration: 0.501s, episode steps: 260, steps per second: 519, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21434/1200000: episode: 86, duration: 0.634s, episode steps: 315, steps per second: 497, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21674/1200000: episode: 87, duration: 0.477s, episode steps: 240, steps per second: 503, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21845/1200000: episode: 88, duration: 0.352s, episode steps: 171, steps per second: 486, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.690 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22126/1200000: episode: 89, duration: 0.560s, episode steps: 281, steps per second: 502, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22316/1200000: episode: 90, duration: 0.406s, episode steps: 190, steps per second: 468, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22493/1200000: episode: 91, duration: 0.389s, episode steps: 177, steps per second: 455, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22829/1200000: episode: 92, duration: 0.671s, episode steps: 336, steps per second: 501, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23020/1200000: episode: 93, duration: 0.399s, episode steps: 191, steps per second: 479, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23266/1200000: episode: 94, duration: 0.484s, episode steps: 246, steps per second: 508, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23573/1200000: episode: 95, duration: 0.625s, episode steps: 307, steps per second: 491, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23813/1200000: episode: 96, duration: 0.477s, episode steps: 240, steps per second: 503, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23992/1200000: episode: 97, duration: 0.356s, episode steps: 179, steps per second: 503, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24227/1200000: episode: 98, duration: 0.472s, episode steps: 235, steps per second: 498, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24390/1200000: episode: 99, duration: 0.335s, episode steps: 163, steps per second: 487, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24696/1200000: episode: 100, duration: 0.627s, episode steps: 306, steps per second: 488, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24859/1200000: episode: 101, duration: 0.350s, episode steps: 163, steps per second: 466, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25027/1200000: episode: 102, duration: 0.360s, episode steps: 168, steps per second: 467, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25319/1200000: episode: 103, duration: 0.579s, episode steps: 292, steps per second: 504, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25529/1200000: episode: 104, duration: 0.418s, episode steps: 210, steps per second: 502, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25823/1200000: episode: 105, duration: 0.569s, episode steps: 294, steps per second: 517, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26047/1200000: episode: 106, duration: 0.442s, episode steps: 224, steps per second: 506, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26501/1200000: episode: 107, duration: 0.897s, episode steps: 454, steps per second: 506, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26668/1200000: episode: 108, duration: 0.373s, episode steps: 167, steps per second: 448, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26902/1200000: episode: 109, duration: 0.454s, episode steps: 234, steps per second: 516, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27150/1200000: episode: 110, duration: 0.480s, episode steps: 248, steps per second: 517, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27314/1200000: episode: 111, duration: 0.338s, episode steps: 164, steps per second: 485, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27558/1200000: episode: 112, duration: 0.467s, episode steps: 244, steps per second: 522, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27765/1200000: episode: 113, duration: 0.426s, episode steps: 207, steps per second: 486, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27961/1200000: episode: 114, duration: 0.387s, episode steps: 196, steps per second: 506, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28185/1200000: episode: 115, duration: 0.446s, episode steps: 224, steps per second: 503, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28452/1200000: episode: 116, duration: 0.519s, episode steps: 267, steps per second: 514, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28655/1200000: episode: 117, duration: 0.393s, episode steps: 203, steps per second: 516, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.744 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28886/1200000: episode: 118, duration: 0.461s, episode steps: 231, steps per second: 501, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29101/1200000: episode: 119, duration: 0.423s, episode steps: 215, steps per second: 508, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29280/1200000: episode: 120, duration: 0.360s, episode steps: 179, steps per second: 497, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29798/1200000: episode: 121, duration: 0.987s, episode steps: 518, steps per second: 525, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30024/1200000: episode: 122, duration: 0.457s, episode steps: 226, steps per second: 494, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30272/1200000: episode: 123, duration: 0.495s, episode steps: 248, steps per second: 501, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30562/1200000: episode: 124, duration: 0.608s, episode steps: 290, steps per second: 477, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30732/1200000: episode: 125, duration: 0.349s, episode steps: 170, steps per second: 486, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30903/1200000: episode: 126, duration: 0.352s, episode steps: 171, steps per second: 486, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31141/1200000: episode: 127, duration: 0.475s, episode steps: 238, steps per second: 501, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31438/1200000: episode: 128, duration: 0.572s, episode steps: 297, steps per second: 520, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31602/1200000: episode: 129, duration: 0.357s, episode steps: 164, steps per second: 459, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31781/1200000: episode: 130, duration: 0.367s, episode steps: 179, steps per second: 488, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32185/1200000: episode: 131, duration: 0.797s, episode steps: 404, steps per second: 507, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32570/1200000: episode: 132, duration: 0.739s, episode steps: 385, steps per second: 521, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32883/1200000: episode: 133, duration: 0.600s, episode steps: 313, steps per second: 521, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33254/1200000: episode: 134, duration: 0.734s, episode steps: 371, steps per second: 505, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33542/1200000: episode: 135, duration: 0.564s, episode steps: 288, steps per second: 511, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33843/1200000: episode: 136, duration: 0.575s, episode steps: 301, steps per second: 524, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34243/1200000: episode: 137, duration: 0.755s, episode steps: 400, steps per second: 530, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34582/1200000: episode: 138, duration: 0.655s, episode steps: 339, steps per second: 518, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34872/1200000: episode: 139, duration: 0.552s, episode steps: 290, steps per second: 526, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35048/1200000: episode: 140, duration: 0.358s, episode steps: 176, steps per second: 491, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35298/1200000: episode: 141, duration: 0.498s, episode steps: 250, steps per second: 502, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35464/1200000: episode: 142, duration: 0.337s, episode steps: 166, steps per second: 493, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35652/1200000: episode: 143, duration: 0.368s, episode steps: 188, steps per second: 511, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35938/1200000: episode: 144, duration: 0.543s, episode steps: 286, steps per second: 527, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36278/1200000: episode: 145, duration: 0.646s, episode steps: 340, steps per second: 526, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36552/1200000: episode: 146, duration: 0.561s, episode steps: 274, steps per second: 488, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36793/1200000: episode: 147, duration: 0.500s, episode steps: 241, steps per second: 482, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36961/1200000: episode: 148, duration: 0.340s, episode steps: 168, steps per second: 494, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37233/1200000: episode: 149, duration: 0.538s, episode steps: 272, steps per second: 506, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37618/1200000: episode: 150, duration: 0.738s, episode steps: 385, steps per second: 522, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37803/1200000: episode: 151, duration: 0.370s, episode steps: 185, steps per second: 499, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38089/1200000: episode: 152, duration: 0.545s, episode steps: 286, steps per second: 525, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38322/1200000: episode: 153, duration: 0.461s, episode steps: 233, steps per second: 506, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38645/1200000: episode: 154, duration: 0.627s, episode steps: 323, steps per second: 515, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38822/1200000: episode: 155, duration: 0.364s, episode steps: 177, steps per second: 486, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38989/1200000: episode: 156, duration: 0.337s, episode steps: 167, steps per second: 496, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.677 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39220/1200000: episode: 157, duration: 0.449s, episode steps: 231, steps per second: 515, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39501/1200000: episode: 158, duration: 0.539s, episode steps: 281, steps per second: 521, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39736/1200000: episode: 159, duration: 0.459s, episode steps: 235, steps per second: 512, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39967/1200000: episode: 160, duration: 0.462s, episode steps: 231, steps per second: 500, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40202/1200000: episode: 161, duration: 0.464s, episode steps: 235, steps per second: 507, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40367/1200000: episode: 162, duration: 0.337s, episode steps: 165, steps per second: 489, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40542/1200000: episode: 163, duration: 0.361s, episode steps: 175, steps per second: 484, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40910/1200000: episode: 164, duration: 0.700s, episode steps: 368, steps per second: 525, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41075/1200000: episode: 165, duration: 0.350s, episode steps: 165, steps per second: 471, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41319/1200000: episode: 166, duration: 0.476s, episode steps: 244, steps per second: 513, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41556/1200000: episode: 167, duration: 0.473s, episode steps: 237, steps per second: 501, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41728/1200000: episode: 168, duration: 0.340s, episode steps: 172, steps per second: 506, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41953/1200000: episode: 169, duration: 0.438s, episode steps: 225, steps per second: 514, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42130/1200000: episode: 170, duration: 0.357s, episode steps: 177, steps per second: 496, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42549/1200000: episode: 171, duration: 0.786s, episode steps: 419, steps per second: 533, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42724/1200000: episode: 172, duration: 0.352s, episode steps: 175, steps per second: 497, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42994/1200000: episode: 173, duration: 0.535s, episode steps: 270, steps per second: 504, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43208/1200000: episode: 174, duration: 0.431s, episode steps: 214, steps per second: 496, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43479/1200000: episode: 175, duration: 0.527s, episode steps: 271, steps per second: 514, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43649/1200000: episode: 176, duration: 0.344s, episode steps: 170, steps per second: 494, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44099/1200000: episode: 177, duration: 0.834s, episode steps: 450, steps per second: 540, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44378/1200000: episode: 178, duration: 0.535s, episode steps: 279, steps per second: 521, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44576/1200000: episode: 179, duration: 0.392s, episode steps: 198, steps per second: 505, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44825/1200000: episode: 180, duration: 0.473s, episode steps: 249, steps per second: 527, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45000/1200000: episode: 181, duration: 0.344s, episode steps: 175, steps per second: 509, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45267/1200000: episode: 182, duration: 0.524s, episode steps: 267, steps per second: 510, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45490/1200000: episode: 183, duration: 0.469s, episode steps: 223, steps per second: 476, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45698/1200000: episode: 184, duration: 0.409s, episode steps: 208, steps per second: 509, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45874/1200000: episode: 185, duration: 0.351s, episode steps: 176, steps per second: 501, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46056/1200000: episode: 186, duration: 0.362s, episode steps: 182, steps per second: 503, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46273/1200000: episode: 187, duration: 0.415s, episode steps: 217, steps per second: 523, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46533/1200000: episode: 188, duration: 0.495s, episode steps: 260, steps per second: 525, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46711/1200000: episode: 189, duration: 0.369s, episode steps: 178, steps per second: 482, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46925/1200000: episode: 190, duration: 0.410s, episode steps: 214, steps per second: 522, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47100/1200000: episode: 191, duration: 0.344s, episode steps: 175, steps per second: 509, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47274/1200000: episode: 192, duration: 0.336s, episode steps: 174, steps per second: 517, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47435/1200000: episode: 193, duration: 0.328s, episode steps: 161, steps per second: 491, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47616/1200000: episode: 194, duration: 0.362s, episode steps: 181, steps per second: 500, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47825/1200000: episode: 195, duration: 0.434s, episode steps: 209, steps per second: 481, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48110/1200000: episode: 196, duration: 0.553s, episode steps: 285, steps per second: 515, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48315/1200000: episode: 197, duration: 0.402s, episode steps: 205, steps per second: 510, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48489/1200000: episode: 198, duration: 0.350s, episode steps: 174, steps per second: 498, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48702/1200000: episode: 199, duration: 0.428s, episode steps: 213, steps per second: 497, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49074/1200000: episode: 200, duration: 0.730s, episode steps: 372, steps per second: 510, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49282/1200000: episode: 201, duration: 0.412s, episode steps: 208, steps per second: 504, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49641/1200000: episode: 202, duration: 0.691s, episode steps: 359, steps per second: 520, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49963/1200000: episode: 203, duration: 0.628s, episode steps: 322, steps per second: 513, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   50250/1200000: episode: 204, duration: 1.940s, episode steps: 287, steps per second: 148, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 53.191151, mae: 37.811604, mean_q: -42.460410, mean_eps: 0.962406\n",
      "   50542/1200000: episode: 205, duration: 1.863s, episode steps: 292, steps per second: 157, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 13.209128, mae: 36.966946, mean_q: -45.557302, mean_eps: 0.962203\n",
      "   50807/1200000: episode: 206, duration: 1.679s, episode steps: 265, steps per second: 158, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 13.601885, mae: 36.962998, mean_q: -45.686162, mean_eps: 0.961995\n",
      "   51009/1200000: episode: 207, duration: 1.308s, episode steps: 202, steps per second: 154, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 12.763071, mae: 37.173315, mean_q: -46.087877, mean_eps: 0.961819\n",
      "   51321/1200000: episode: 208, duration: 2.006s, episode steps: 312, steps per second: 156, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 10.868631, mae: 36.970233, mean_q: -46.226378, mean_eps: 0.961627\n",
      "   51622/1200000: episode: 209, duration: 1.910s, episode steps: 301, steps per second: 158, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 13.184867, mae: 36.844886, mean_q: -46.005399, mean_eps: 0.961397\n",
      "   51987/1200000: episode: 210, duration: 2.342s, episode steps: 365, steps per second: 156, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 10.575449, mae: 36.767574, mean_q: -46.290217, mean_eps: 0.961147\n",
      "   52313/1200000: episode: 211, duration: 2.181s, episode steps: 326, steps per second: 149, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 11.566715, mae: 36.988007, mean_q: -46.718007, mean_eps: 0.960888\n",
      "   52525/1200000: episode: 212, duration: 1.389s, episode steps: 212, steps per second: 153, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.349 [0.000, 3.000],  loss: 12.369177, mae: 37.177299, mean_q: -46.824318, mean_eps: 0.960686\n",
      "   52743/1200000: episode: 213, duration: 1.420s, episode steps: 218, steps per second: 154, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 12.323318, mae: 36.833712, mean_q: -46.630519, mean_eps: 0.960525\n",
      "   53009/1200000: episode: 214, duration: 1.747s, episode steps: 266, steps per second: 152, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 11.930361, mae: 36.888546, mean_q: -46.698873, mean_eps: 0.960343\n",
      "   53226/1200000: episode: 215, duration: 1.421s, episode steps: 217, steps per second: 153, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 9.453154, mae: 36.755164, mean_q: -46.731800, mean_eps: 0.960162\n",
      "   53418/1200000: episode: 216, duration: 1.279s, episode steps: 192, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 10.175967, mae: 36.955931, mean_q: -46.984781, mean_eps: 0.960009\n",
      "   53612/1200000: episode: 217, duration: 1.314s, episode steps: 194, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 10.842754, mae: 36.735068, mean_q: -46.755013, mean_eps: 0.959864\n",
      "   53778/1200000: episode: 218, duration: 1.080s, episode steps: 166, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 11.418701, mae: 36.886967, mean_q: -46.524795, mean_eps: 0.959729\n",
      "   54116/1200000: episode: 219, duration: 2.130s, episode steps: 338, steps per second: 159, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 7.895699, mae: 36.817461, mean_q: -47.000082, mean_eps: 0.959540\n",
      "   54433/1200000: episode: 220, duration: 2.000s, episode steps: 317, steps per second: 159, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 10.531841, mae: 36.851155, mean_q: -46.769697, mean_eps: 0.959294\n",
      "   54602/1200000: episode: 221, duration: 1.093s, episode steps: 169, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.349 [0.000, 3.000],  loss: 7.441253, mae: 36.609670, mean_q: -46.945862, mean_eps: 0.959112\n",
      "   54873/1200000: episode: 222, duration: 1.788s, episode steps: 271, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 9.814618, mae: 36.634096, mean_q: -46.723263, mean_eps: 0.958947\n",
      "   55149/1200000: episode: 223, duration: 1.851s, episode steps: 276, steps per second: 149, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 9.946206, mae: 36.812470, mean_q: -46.984509, mean_eps: 0.958742\n",
      "   55327/1200000: episode: 224, duration: 1.155s, episode steps: 178, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 13.130460, mae: 36.683797, mean_q: -46.604177, mean_eps: 0.958572\n",
      "   55494/1200000: episode: 225, duration: 1.067s, episode steps: 167, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 13.214880, mae: 36.682852, mean_q: -46.546537, mean_eps: 0.958443\n",
      "   55734/1200000: episode: 226, duration: 1.596s, episode steps: 240, steps per second: 150, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.342 [0.000, 3.000],  loss: 11.611974, mae: 36.865547, mean_q: -47.103977, mean_eps: 0.958290\n",
      "   55898/1200000: episode: 227, duration: 1.081s, episode steps: 164, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 9.092758, mae: 36.746130, mean_q: -47.056475, mean_eps: 0.958138\n",
      "   56065/1200000: episode: 228, duration: 1.095s, episode steps: 167, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.671 [0.000, 3.000],  loss: 10.119059, mae: 36.483311, mean_q: -46.674654, mean_eps: 0.958014\n",
      "   56248/1200000: episode: 229, duration: 1.176s, episode steps: 183, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 11.783452, mae: 36.874712, mean_q: -47.113124, mean_eps: 0.957883\n",
      "   56598/1200000: episode: 230, duration: 2.306s, episode steps: 350, steps per second: 152, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 12.317666, mae: 36.802914, mean_q: -47.023298, mean_eps: 0.957683\n",
      "   56779/1200000: episode: 231, duration: 1.167s, episode steps: 181, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.597 [0.000, 3.000],  loss: 9.632297, mae: 36.913685, mean_q: -47.300117, mean_eps: 0.957484\n",
      "   57265/1200000: episode: 232, duration: 3.093s, episode steps: 486, steps per second: 157, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 10.905963, mae: 36.506777, mean_q: -46.706018, mean_eps: 0.957234\n",
      "   57474/1200000: episode: 233, duration: 1.370s, episode steps: 209, steps per second: 153, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 14.004879, mae: 36.718131, mean_q: -46.928852, mean_eps: 0.956973\n",
      "   57852/1200000: episode: 234, duration: 2.432s, episode steps: 378, steps per second: 155, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 10.684579, mae: 36.772416, mean_q: -47.255970, mean_eps: 0.956753\n",
      "   58075/1200000: episode: 235, duration: 1.480s, episode steps: 223, steps per second: 151, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 11.913351, mae: 36.835014, mean_q: -47.312253, mean_eps: 0.956528\n",
      "   58256/1200000: episode: 236, duration: 1.187s, episode steps: 181, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 7.416701, mae: 36.866330, mean_q: -47.489756, mean_eps: 0.956376\n",
      "   58417/1200000: episode: 237, duration: 1.067s, episode steps: 161, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 9.282689, mae: 36.562768, mean_q: -46.861277, mean_eps: 0.956248\n",
      "   58592/1200000: episode: 238, duration: 1.149s, episode steps: 175, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 13.295486, mae: 36.649282, mean_q: -46.921622, mean_eps: 0.956122\n",
      "   58829/1200000: episode: 239, duration: 1.621s, episode steps: 237, steps per second: 146, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 9.330769, mae: 36.484272, mean_q: -46.944616, mean_eps: 0.955968\n",
      "   58998/1200000: episode: 240, duration: 1.125s, episode steps: 169, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 10.696422, mae: 36.642702, mean_q: -46.977933, mean_eps: 0.955815\n",
      "   59175/1200000: episode: 241, duration: 1.190s, episode steps: 177, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: 9.388958, mae: 36.524171, mean_q: -47.020121, mean_eps: 0.955686\n",
      "   59342/1200000: episode: 242, duration: 1.187s, episode steps: 167, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 7.389428, mae: 36.749229, mean_q: -47.497723, mean_eps: 0.955556\n",
      "   59649/1200000: episode: 243, duration: 2.025s, episode steps: 307, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 11.183840, mae: 36.780479, mean_q: -47.087483, mean_eps: 0.955379\n",
      "   59814/1200000: episode: 244, duration: 1.077s, episode steps: 165, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 8.956082, mae: 36.498217, mean_q: -46.996330, mean_eps: 0.955202\n",
      "   60087/1200000: episode: 245, duration: 1.729s, episode steps: 273, steps per second: 158, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 10.052199, mae: 36.219310, mean_q: -46.632635, mean_eps: 0.955037\n",
      "   60359/1200000: episode: 246, duration: 1.765s, episode steps: 272, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 8.283614, mae: 35.707791, mean_q: -46.169091, mean_eps: 0.954833\n",
      "   60606/1200000: episode: 247, duration: 1.592s, episode steps: 247, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 6.613967, mae: 35.991972, mean_q: -46.564626, mean_eps: 0.954638\n",
      "   60853/1200000: episode: 248, duration: 1.570s, episode steps: 247, steps per second: 157, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 7.180623, mae: 35.844759, mean_q: -46.433902, mean_eps: 0.954453\n",
      "   61091/1200000: episode: 249, duration: 1.544s, episode steps: 238, steps per second: 154, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 7.348223, mae: 35.911881, mean_q: -46.647470, mean_eps: 0.954271\n",
      "   61262/1200000: episode: 250, duration: 1.134s, episode steps: 171, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 6.164296, mae: 35.587215, mean_q: -46.115766, mean_eps: 0.954118\n",
      "   61519/1200000: episode: 251, duration: 1.686s, episode steps: 257, steps per second: 152, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 7.555025, mae: 35.773289, mean_q: -46.428225, mean_eps: 0.953958\n",
      "   61759/1200000: episode: 252, duration: 1.626s, episode steps: 240, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 6.029129, mae: 35.919038, mean_q: -46.654945, mean_eps: 0.953771\n",
      "   61929/1200000: episode: 253, duration: 1.104s, episode steps: 170, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 6.968563, mae: 35.703516, mean_q: -46.086857, mean_eps: 0.953617\n",
      "   62210/1200000: episode: 254, duration: 1.839s, episode steps: 281, steps per second: 153, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 9.137975, mae: 35.952367, mean_q: -46.332073, mean_eps: 0.953448\n",
      "   62406/1200000: episode: 255, duration: 1.276s, episode steps: 196, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 7.020542, mae: 35.691359, mean_q: -46.180271, mean_eps: 0.953269\n",
      "   62729/1200000: episode: 256, duration: 2.087s, episode steps: 323, steps per second: 155, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: 7.723058, mae: 35.766144, mean_q: -46.282773, mean_eps: 0.953075\n",
      "   63051/1200000: episode: 257, duration: 2.063s, episode steps: 322, steps per second: 156, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 7.660215, mae: 35.543360, mean_q: -46.100810, mean_eps: 0.952833\n",
      "   63229/1200000: episode: 258, duration: 1.180s, episode steps: 178, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 7.887950, mae: 35.770105, mean_q: -46.342901, mean_eps: 0.952645\n",
      "   63403/1200000: episode: 259, duration: 1.159s, episode steps: 174, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 10.003415, mae: 35.932383, mean_q: -46.293523, mean_eps: 0.952513\n",
      "   63652/1200000: episode: 260, duration: 1.663s, episode steps: 249, steps per second: 150, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 9.698248, mae: 35.750663, mean_q: -46.167374, mean_eps: 0.952355\n",
      "   63960/1200000: episode: 261, duration: 1.970s, episode steps: 308, steps per second: 156, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 6.003634, mae: 35.733214, mean_q: -46.370791, mean_eps: 0.952146\n",
      "   64202/1200000: episode: 262, duration: 1.565s, episode steps: 242, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 6.960149, mae: 35.715571, mean_q: -46.276588, mean_eps: 0.951940\n",
      "   64460/1200000: episode: 263, duration: 1.680s, episode steps: 258, steps per second: 154, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 7.797228, mae: 35.847551, mean_q: -46.460635, mean_eps: 0.951752\n",
      "   64745/1200000: episode: 264, duration: 1.879s, episode steps: 285, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 8.716123, mae: 35.695878, mean_q: -46.073354, mean_eps: 0.951548\n",
      "   64925/1200000: episode: 265, duration: 1.206s, episode steps: 180, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 8.766964, mae: 35.850660, mean_q: -46.452478, mean_eps: 0.951374\n",
      "   65202/1200000: episode: 266, duration: 1.829s, episode steps: 277, steps per second: 151, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 7.974126, mae: 35.570314, mean_q: -46.065592, mean_eps: 0.951203\n",
      "   65378/1200000: episode: 267, duration: 1.140s, episode steps: 176, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 6.569643, mae: 35.858553, mean_q: -46.491131, mean_eps: 0.951033\n",
      "   65546/1200000: episode: 268, duration: 1.084s, episode steps: 168, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 7.605422, mae: 35.793771, mean_q: -46.344744, mean_eps: 0.950904\n",
      "   65732/1200000: episode: 269, duration: 1.201s, episode steps: 186, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 6.470649, mae: 36.030331, mean_q: -46.726284, mean_eps: 0.950771\n",
      "   65896/1200000: episode: 270, duration: 1.077s, episode steps: 164, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 6.883088, mae: 35.295266, mean_q: -45.775704, mean_eps: 0.950640\n",
      "   66101/1200000: episode: 271, duration: 1.361s, episode steps: 205, steps per second: 151, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 6.585945, mae: 35.629560, mean_q: -46.145484, mean_eps: 0.950501\n",
      "   66267/1200000: episode: 272, duration: 1.075s, episode steps: 166, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.410 [0.000, 3.000],  loss: 6.061960, mae: 35.695780, mean_q: -46.219608, mean_eps: 0.950362\n",
      "   66535/1200000: episode: 273, duration: 1.739s, episode steps: 268, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 7.754885, mae: 35.614467, mean_q: -46.227281, mean_eps: 0.950200\n",
      "   66811/1200000: episode: 274, duration: 1.775s, episode steps: 276, steps per second: 155, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 8.751150, mae: 35.492772, mean_q: -45.847539, mean_eps: 0.949996\n",
      "   67151/1200000: episode: 275, duration: 2.204s, episode steps: 340, steps per second: 154, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 7.290409, mae: 35.504246, mean_q: -46.029043, mean_eps: 0.949765\n",
      "   67356/1200000: episode: 276, duration: 1.330s, episode steps: 205, steps per second: 154, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 9.669949, mae: 35.578361, mean_q: -45.840695, mean_eps: 0.949560\n",
      "   67547/1200000: episode: 277, duration: 1.270s, episode steps: 191, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.487 [0.000, 3.000],  loss: 9.196842, mae: 35.469585, mean_q: -45.777908, mean_eps: 0.949412\n",
      "   67758/1200000: episode: 278, duration: 1.356s, episode steps: 211, steps per second: 156, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 7.332364, mae: 35.748556, mean_q: -46.407978, mean_eps: 0.949261\n",
      "   68042/1200000: episode: 279, duration: 1.802s, episode steps: 284, steps per second: 158, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 7.285286, mae: 35.709698, mean_q: -46.340133, mean_eps: 0.949075\n",
      "   68379/1200000: episode: 280, duration: 2.127s, episode steps: 337, steps per second: 158, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 8.752697, mae: 35.631439, mean_q: -46.142968, mean_eps: 0.948843\n",
      "   68549/1200000: episode: 281, duration: 1.085s, episode steps: 170, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 9.113974, mae: 35.629833, mean_q: -46.087293, mean_eps: 0.948652\n",
      "   68959/1200000: episode: 282, duration: 2.587s, episode steps: 410, steps per second: 158, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 7.851625, mae: 35.657370, mean_q: -46.215026, mean_eps: 0.948435\n",
      "   69307/1200000: episode: 283, duration: 2.203s, episode steps: 348, steps per second: 158, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 7.943033, mae: 35.681609, mean_q: -46.276488, mean_eps: 0.948151\n",
      "   69543/1200000: episode: 284, duration: 1.506s, episode steps: 236, steps per second: 157, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 5.855600, mae: 35.586121, mean_q: -46.352233, mean_eps: 0.947932\n",
      "   69710/1200000: episode: 285, duration: 1.070s, episode steps: 167, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 6.415771, mae: 35.843410, mean_q: -46.543319, mean_eps: 0.947780\n",
      "   69898/1200000: episode: 286, duration: 1.205s, episode steps: 188, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 6.994718, mae: 35.824938, mean_q: -46.448965, mean_eps: 0.947647\n",
      "   70090/1200000: episode: 287, duration: 1.240s, episode steps: 192, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 6.712297, mae: 35.219346, mean_q: -45.771582, mean_eps: 0.947505\n",
      "   70618/1200000: episode: 288, duration: 3.310s, episode steps: 528, steps per second: 159, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 6.545661, mae: 34.713553, mean_q: -45.113580, mean_eps: 0.947235\n",
      "   70866/1200000: episode: 289, duration: 1.573s, episode steps: 248, steps per second: 158, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 8.085815, mae: 34.696480, mean_q: -45.003139, mean_eps: 0.946944\n",
      "   71050/1200000: episode: 290, duration: 1.171s, episode steps: 184, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 5.606512, mae: 34.558891, mean_q: -45.112235, mean_eps: 0.946782\n",
      "   71328/1200000: episode: 291, duration: 1.779s, episode steps: 278, steps per second: 156, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 8.315219, mae: 34.600684, mean_q: -44.900540, mean_eps: 0.946609\n",
      "   71656/1200000: episode: 292, duration: 2.098s, episode steps: 328, steps per second: 156, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 5.857088, mae: 34.713590, mean_q: -45.228892, mean_eps: 0.946381\n",
      "   71865/1200000: episode: 293, duration: 1.344s, episode steps: 209, steps per second: 156, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 8.917539, mae: 34.866708, mean_q: -45.035156, mean_eps: 0.946180\n",
      "   72198/1200000: episode: 294, duration: 2.129s, episode steps: 333, steps per second: 156, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 6.842444, mae: 34.701404, mean_q: -45.059465, mean_eps: 0.945977\n",
      "   72508/1200000: episode: 295, duration: 1.987s, episode steps: 310, steps per second: 156, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 6.386460, mae: 34.555595, mean_q: -44.987205, mean_eps: 0.945736\n",
      "   72820/1200000: episode: 296, duration: 2.013s, episode steps: 312, steps per second: 155, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 6.387294, mae: 34.789067, mean_q: -45.199428, mean_eps: 0.945502\n",
      "   73177/1200000: episode: 297, duration: 2.290s, episode steps: 357, steps per second: 156, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 6.661627, mae: 34.810150, mean_q: -45.268019, mean_eps: 0.945252\n",
      "   73523/1200000: episode: 298, duration: 2.216s, episode steps: 346, steps per second: 156, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 7.578909, mae: 34.826081, mean_q: -45.229864, mean_eps: 0.944988\n",
      "   73833/1200000: episode: 299, duration: 1.991s, episode steps: 310, steps per second: 156, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 8.035661, mae: 34.633237, mean_q: -44.886298, mean_eps: 0.944742\n",
      "   74066/1200000: episode: 300, duration: 1.500s, episode steps: 233, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 7.377564, mae: 34.859547, mean_q: -45.187468, mean_eps: 0.944538\n",
      "   74333/1200000: episode: 301, duration: 1.722s, episode steps: 267, steps per second: 155, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 7.097522, mae: 34.678327, mean_q: -45.028475, mean_eps: 0.944351\n",
      "   74510/1200000: episode: 302, duration: 1.158s, episode steps: 177, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 4.969723, mae: 34.789876, mean_q: -45.313985, mean_eps: 0.944184\n",
      "   74730/1200000: episode: 303, duration: 1.423s, episode steps: 220, steps per second: 155, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 5.479992, mae: 34.869341, mean_q: -45.511401, mean_eps: 0.944035\n",
      "   75166/1200000: episode: 304, duration: 2.761s, episode steps: 436, steps per second: 158, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 7.287768, mae: 34.762728, mean_q: -45.126339, mean_eps: 0.943789\n",
      "   75341/1200000: episode: 305, duration: 1.142s, episode steps: 175, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 5.000910, mae: 34.708400, mean_q: -45.226348, mean_eps: 0.943560\n",
      "   75584/1200000: episode: 306, duration: 1.564s, episode steps: 243, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 7.431502, mae: 34.680982, mean_q: -44.971949, mean_eps: 0.943403\n",
      "   75816/1200000: episode: 307, duration: 1.498s, episode steps: 232, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 6.448392, mae: 34.812702, mean_q: -45.236716, mean_eps: 0.943225\n",
      "   76130/1200000: episode: 308, duration: 2.012s, episode steps: 314, steps per second: 156, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 6.871509, mae: 34.711769, mean_q: -45.178048, mean_eps: 0.943021\n",
      "   76404/1200000: episode: 309, duration: 1.776s, episode steps: 274, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 5.626988, mae: 34.765171, mean_q: -45.293425, mean_eps: 0.942800\n",
      "   76654/1200000: episode: 310, duration: 1.611s, episode steps: 250, steps per second: 155, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 6.234229, mae: 34.964312, mean_q: -45.508493, mean_eps: 0.942604\n",
      "   76889/1200000: episode: 311, duration: 1.549s, episode steps: 235, steps per second: 152, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 6.561683, mae: 34.952019, mean_q: -45.510215, mean_eps: 0.942422\n",
      "   77273/1200000: episode: 312, duration: 2.446s, episode steps: 384, steps per second: 157, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 6.310268, mae: 34.912581, mean_q: -45.317858, mean_eps: 0.942190\n",
      "   77504/1200000: episode: 313, duration: 1.500s, episode steps: 231, steps per second: 154, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 6.601725, mae: 34.850213, mean_q: -45.294626, mean_eps: 0.941959\n",
      "   77717/1200000: episode: 314, duration: 1.383s, episode steps: 213, steps per second: 154, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 5.161282, mae: 35.128600, mean_q: -45.837346, mean_eps: 0.941792\n",
      "   77886/1200000: episode: 315, duration: 1.105s, episode steps: 169, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 5.698983, mae: 34.726290, mean_q: -45.182187, mean_eps: 0.941649\n",
      "   78217/1200000: episode: 316, duration: 2.147s, episode steps: 331, steps per second: 154, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 6.359505, mae: 34.840886, mean_q: -45.278214, mean_eps: 0.941462\n",
      "   78392/1200000: episode: 317, duration: 1.156s, episode steps: 175, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 8.300721, mae: 34.686303, mean_q: -44.969288, mean_eps: 0.941272\n",
      "   78601/1200000: episode: 318, duration: 1.365s, episode steps: 209, steps per second: 153, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 8.501895, mae: 34.948057, mean_q: -45.279752, mean_eps: 0.941128\n",
      "   78763/1200000: episode: 319, duration: 1.075s, episode steps: 162, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 6.158317, mae: 34.871675, mean_q: -45.353066, mean_eps: 0.940989\n",
      "   79054/1200000: episode: 320, duration: 1.886s, episode steps: 291, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 6.928369, mae: 34.808907, mean_q: -45.250666, mean_eps: 0.940819\n",
      "   79226/1200000: episode: 321, duration: 1.131s, episode steps: 172, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 5.710467, mae: 34.812947, mean_q: -45.338169, mean_eps: 0.940645\n",
      "   79505/1200000: episode: 322, duration: 1.793s, episode steps: 279, steps per second: 156, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 6.133420, mae: 34.725661, mean_q: -45.289541, mean_eps: 0.940476\n",
      "   79840/1200000: episode: 323, duration: 2.164s, episode steps: 335, steps per second: 155, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 5.118992, mae: 34.828697, mean_q: -45.469085, mean_eps: 0.940246\n",
      "   80122/1200000: episode: 324, duration: 1.819s, episode steps: 282, steps per second: 155, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 7.801502, mae: 34.126548, mean_q: -44.309826, mean_eps: 0.940015\n",
      "   80390/1200000: episode: 325, duration: 1.752s, episode steps: 268, steps per second: 153, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 5.374863, mae: 33.244788, mean_q: -43.389881, mean_eps: 0.939808\n",
      "   80560/1200000: episode: 326, duration: 1.114s, episode steps: 170, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 7.315396, mae: 33.369513, mean_q: -43.271281, mean_eps: 0.939644\n",
      "   80876/1200000: episode: 327, duration: 2.037s, episode steps: 316, steps per second: 155, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 6.926474, mae: 33.315287, mean_q: -43.153320, mean_eps: 0.939462\n",
      "   81159/1200000: episode: 328, duration: 1.859s, episode steps: 283, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 6.957459, mae: 33.324440, mean_q: -43.359939, mean_eps: 0.939237\n",
      "   81561/1200000: episode: 329, duration: 2.609s, episode steps: 402, steps per second: 154, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 6.894522, mae: 33.288449, mean_q: -43.311102, mean_eps: 0.938980\n",
      "   81817/1200000: episode: 330, duration: 1.674s, episode steps: 256, steps per second: 153, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 4.913938, mae: 33.177860, mean_q: -43.310921, mean_eps: 0.938734\n",
      "   82091/1200000: episode: 331, duration: 1.808s, episode steps: 274, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 5.025903, mae: 33.401868, mean_q: -43.605384, mean_eps: 0.938535\n",
      "   82275/1200000: episode: 332, duration: 1.235s, episode steps: 184, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 5.038928, mae: 33.190334, mean_q: -43.006992, mean_eps: 0.938363\n",
      "   82446/1200000: episode: 333, duration: 1.120s, episode steps: 171, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 6.522388, mae: 33.361793, mean_q: -43.409794, mean_eps: 0.938230\n",
      "   82725/1200000: episode: 334, duration: 1.824s, episode steps: 279, steps per second: 153, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 5.808980, mae: 33.058127, mean_q: -43.034576, mean_eps: 0.938061\n",
      "   83024/1200000: episode: 335, duration: 1.961s, episode steps: 299, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 5.126224, mae: 33.366955, mean_q: -43.470765, mean_eps: 0.937844\n",
      "   83194/1200000: episode: 336, duration: 1.122s, episode steps: 170, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 5.807619, mae: 33.295169, mean_q: -43.272105, mean_eps: 0.937669\n",
      "   83373/1200000: episode: 337, duration: 1.178s, episode steps: 179, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 6.142635, mae: 32.945268, mean_q: -42.798726, mean_eps: 0.937538\n",
      "   83675/1200000: episode: 338, duration: 1.964s, episode steps: 302, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 5.222320, mae: 33.211123, mean_q: -43.244530, mean_eps: 0.937357\n",
      "   83864/1200000: episode: 339, duration: 1.242s, episode steps: 189, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 5.086812, mae: 33.199395, mean_q: -43.253274, mean_eps: 0.937173\n",
      "   84219/1200000: episode: 340, duration: 2.295s, episode steps: 355, steps per second: 155, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 5.358526, mae: 33.259833, mean_q: -43.360961, mean_eps: 0.936969\n",
      "   84425/1200000: episode: 341, duration: 1.368s, episode steps: 206, steps per second: 151, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 7.580204, mae: 33.064983, mean_q: -42.952638, mean_eps: 0.936759\n",
      "   84765/1200000: episode: 342, duration: 2.229s, episode steps: 340, steps per second: 153, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 6.132112, mae: 33.198134, mean_q: -43.231672, mean_eps: 0.936554\n",
      "   84937/1200000: episode: 343, duration: 1.141s, episode steps: 172, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 5.614675, mae: 33.150691, mean_q: -43.228336, mean_eps: 0.936362\n",
      "   85139/1200000: episode: 344, duration: 1.317s, episode steps: 202, steps per second: 153, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 5.934707, mae: 33.058486, mean_q: -43.011156, mean_eps: 0.936222\n",
      "   85432/1200000: episode: 345, duration: 1.851s, episode steps: 293, steps per second: 158, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 5.482805, mae: 33.327583, mean_q: -43.411745, mean_eps: 0.936036\n",
      "   85674/1200000: episode: 346, duration: 1.592s, episode steps: 242, steps per second: 152, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 4.819518, mae: 33.236543, mean_q: -43.449410, mean_eps: 0.935836\n",
      "   85971/1200000: episode: 347, duration: 1.966s, episode steps: 297, steps per second: 151, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 5.770798, mae: 33.291279, mean_q: -43.375612, mean_eps: 0.935634\n",
      "   86259/1200000: episode: 348, duration: 1.897s, episode steps: 288, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 5.302249, mae: 33.151246, mean_q: -43.197797, mean_eps: 0.935414\n",
      "   86494/1200000: episode: 349, duration: 1.551s, episode steps: 235, steps per second: 152, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 5.758390, mae: 33.331195, mean_q: -43.465493, mean_eps: 0.935218\n",
      "   86671/1200000: episode: 350, duration: 1.179s, episode steps: 177, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 5.387472, mae: 33.319729, mean_q: -43.279364, mean_eps: 0.935063\n",
      "   86852/1200000: episode: 351, duration: 1.197s, episode steps: 181, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 5.023541, mae: 33.245870, mean_q: -43.323734, mean_eps: 0.934929\n",
      "   87043/1200000: episode: 352, duration: 1.309s, episode steps: 191, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 5.437486, mae: 33.187879, mean_q: -43.156399, mean_eps: 0.934790\n",
      "   87212/1200000: episode: 353, duration: 1.125s, episode steps: 169, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 4.322667, mae: 33.293191, mean_q: -43.464529, mean_eps: 0.934655\n",
      "   87391/1200000: episode: 354, duration: 1.196s, episode steps: 179, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.380 [0.000, 3.000],  loss: 5.895585, mae: 33.111578, mean_q: -43.093559, mean_eps: 0.934524\n",
      "   87632/1200000: episode: 355, duration: 1.603s, episode steps: 241, steps per second: 150, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 6.179264, mae: 33.138802, mean_q: -43.178247, mean_eps: 0.934367\n",
      "   87931/1200000: episode: 356, duration: 1.994s, episode steps: 299, steps per second: 150, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 4.432181, mae: 32.943831, mean_q: -42.928970, mean_eps: 0.934164\n",
      "   88209/1200000: episode: 357, duration: 1.832s, episode steps: 278, steps per second: 152, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 5.668127, mae: 33.224274, mean_q: -43.335763, mean_eps: 0.933948\n",
      "   88440/1200000: episode: 358, duration: 1.529s, episode steps: 231, steps per second: 151, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 7.156404, mae: 33.159623, mean_q: -43.047990, mean_eps: 0.933757\n",
      "   88647/1200000: episode: 359, duration: 1.378s, episode steps: 207, steps per second: 150, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 6.692759, mae: 33.257677, mean_q: -43.378597, mean_eps: 0.933593\n",
      "   89008/1200000: episode: 360, duration: 2.346s, episode steps: 361, steps per second: 154, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 5.018388, mae: 33.097369, mean_q: -43.116634, mean_eps: 0.933380\n",
      "   89373/1200000: episode: 361, duration: 2.402s, episode steps: 365, steps per second: 152, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 5.692619, mae: 33.126828, mean_q: -43.063100, mean_eps: 0.933107\n",
      "   89547/1200000: episode: 362, duration: 1.166s, episode steps: 174, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 5.287791, mae: 33.212455, mean_q: -43.382550, mean_eps: 0.932905\n",
      "   89716/1200000: episode: 363, duration: 1.132s, episode steps: 169, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 5.227075, mae: 33.037925, mean_q: -43.028201, mean_eps: 0.932777\n",
      "   89880/1200000: episode: 364, duration: 1.112s, episode steps: 164, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 6.046660, mae: 32.996974, mean_q: -42.951793, mean_eps: 0.932652\n",
      "   90121/1200000: episode: 365, duration: 1.611s, episode steps: 241, steps per second: 150, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 5.902060, mae: 32.642659, mean_q: -42.441572, mean_eps: 0.932500\n",
      "   90307/1200000: episode: 366, duration: 1.243s, episode steps: 186, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 6.392962, mae: 32.205885, mean_q: -42.071274, mean_eps: 0.932340\n",
      "   90586/1200000: episode: 367, duration: 1.853s, episode steps: 279, steps per second: 151, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 5.553523, mae: 32.221917, mean_q: -41.946922, mean_eps: 0.932166\n",
      "   90794/1200000: episode: 368, duration: 1.388s, episode steps: 208, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.361 [0.000, 3.000],  loss: 4.694874, mae: 32.507856, mean_q: -42.480990, mean_eps: 0.931983\n",
      "   90967/1200000: episode: 369, duration: 1.164s, episode steps: 173, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.387 [0.000, 3.000],  loss: 4.879181, mae: 32.302063, mean_q: -42.216482, mean_eps: 0.931840\n",
      "   91303/1200000: episode: 370, duration: 2.249s, episode steps: 336, steps per second: 149, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 5.063795, mae: 32.303147, mean_q: -42.087720, mean_eps: 0.931649\n",
      "   91542/1200000: episode: 371, duration: 1.677s, episode steps: 239, steps per second: 143, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 4.205004, mae: 32.181311, mean_q: -42.013962, mean_eps: 0.931434\n",
      "   91931/1200000: episode: 372, duration: 2.784s, episode steps: 389, steps per second: 140, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 5.451904, mae: 32.342154, mean_q: -42.121111, mean_eps: 0.931198\n",
      "   92094/1200000: episode: 373, duration: 1.157s, episode steps: 163, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 5.846976, mae: 32.260751, mean_q: -42.158975, mean_eps: 0.930991\n",
      "   92372/1200000: episode: 374, duration: 1.902s, episode steps: 278, steps per second: 146, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 4.808860, mae: 32.394067, mean_q: -42.299740, mean_eps: 0.930826\n",
      "   92620/1200000: episode: 375, duration: 1.768s, episode steps: 248, steps per second: 140, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 5.742460, mae: 32.475754, mean_q: -42.309379, mean_eps: 0.930628\n",
      "   92967/1200000: episode: 376, duration: 2.302s, episode steps: 347, steps per second: 151, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: 5.788972, mae: 32.258416, mean_q: -42.092322, mean_eps: 0.930405\n",
      "   93202/1200000: episode: 377, duration: 1.581s, episode steps: 235, steps per second: 149, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 5.100307, mae: 32.294888, mean_q: -42.099038, mean_eps: 0.930187\n",
      "   93483/1200000: episode: 378, duration: 1.877s, episode steps: 281, steps per second: 150, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 4.397011, mae: 32.184935, mean_q: -42.075221, mean_eps: 0.929994\n",
      "   93862/1200000: episode: 379, duration: 2.520s, episode steps: 379, steps per second: 150, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 5.669716, mae: 32.290146, mean_q: -42.103950, mean_eps: 0.929746\n",
      "   94093/1200000: episode: 380, duration: 1.560s, episode steps: 231, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.286 [0.000, 3.000],  loss: 4.749206, mae: 32.324721, mean_q: -42.185697, mean_eps: 0.929517\n",
      "   94253/1200000: episode: 381, duration: 1.099s, episode steps: 160, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 5.149544, mae: 32.474513, mean_q: -42.363747, mean_eps: 0.929371\n",
      "   94421/1200000: episode: 382, duration: 1.128s, episode steps: 168, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 5.084709, mae: 32.342522, mean_q: -42.314112, mean_eps: 0.929248\n",
      "   94652/1200000: episode: 383, duration: 1.566s, episode steps: 231, steps per second: 147, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 6.133332, mae: 32.390323, mean_q: -42.182145, mean_eps: 0.929098\n",
      "   94889/1200000: episode: 384, duration: 1.601s, episode steps: 237, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 4.636631, mae: 32.340430, mean_q: -42.263133, mean_eps: 0.928922\n",
      "   95102/1200000: episode: 385, duration: 1.440s, episode steps: 213, steps per second: 148, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 6.147698, mae: 32.275060, mean_q: -41.973066, mean_eps: 0.928754\n",
      "   95275/1200000: episode: 386, duration: 1.162s, episode steps: 173, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 6.062442, mae: 32.185124, mean_q: -41.864764, mean_eps: 0.928609\n",
      "   95653/1200000: episode: 387, duration: 2.536s, episode steps: 378, steps per second: 149, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 4.908579, mae: 32.248979, mean_q: -42.074688, mean_eps: 0.928402\n",
      "   95856/1200000: episode: 388, duration: 1.388s, episode steps: 203, steps per second: 146, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 4.683889, mae: 32.365800, mean_q: -42.321305, mean_eps: 0.928185\n",
      "   96023/1200000: episode: 389, duration: 1.136s, episode steps: 167, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 4.685722, mae: 32.142983, mean_q: -41.945069, mean_eps: 0.928046\n",
      "   96338/1200000: episode: 390, duration: 2.118s, episode steps: 315, steps per second: 149, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 4.377262, mae: 32.340882, mean_q: -42.300784, mean_eps: 0.927865\n",
      "   96511/1200000: episode: 391, duration: 1.186s, episode steps: 173, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 5.294430, mae: 32.251095, mean_q: -41.921416, mean_eps: 0.927682\n",
      "   96703/1200000: episode: 392, duration: 1.298s, episode steps: 192, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 5.023425, mae: 32.023012, mean_q: -41.791200, mean_eps: 0.927545\n",
      "   96987/1200000: episode: 393, duration: 1.908s, episode steps: 284, steps per second: 149, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 4.800939, mae: 32.327017, mean_q: -42.209996, mean_eps: 0.927367\n",
      "   97277/1200000: episode: 394, duration: 1.957s, episode steps: 290, steps per second: 148, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 4.625999, mae: 32.110141, mean_q: -41.898490, mean_eps: 0.927151\n",
      "   97453/1200000: episode: 395, duration: 1.201s, episode steps: 176, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 4.080842, mae: 32.096382, mean_q: -41.932697, mean_eps: 0.926977\n",
      "   97641/1200000: episode: 396, duration: 1.263s, episode steps: 188, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.388 [0.000, 3.000],  loss: 4.526228, mae: 32.220406, mean_q: -42.060223, mean_eps: 0.926840\n",
      "   97812/1200000: episode: 397, duration: 1.160s, episode steps: 171, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 5.394946, mae: 32.109323, mean_q: -41.905063, mean_eps: 0.926705\n",
      "   98108/1200000: episode: 398, duration: 2.007s, episode steps: 296, steps per second: 147, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 4.672225, mae: 32.233165, mean_q: -42.034594, mean_eps: 0.926530\n",
      "   98445/1200000: episode: 399, duration: 2.254s, episode steps: 337, steps per second: 150, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 6.153098, mae: 32.277725, mean_q: -42.004483, mean_eps: 0.926293\n",
      "   98684/1200000: episode: 400, duration: 1.612s, episode steps: 239, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 5.634168, mae: 32.168371, mean_q: -41.878313, mean_eps: 0.926077\n",
      "   98985/1200000: episode: 401, duration: 2.032s, episode steps: 301, steps per second: 148, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 4.962912, mae: 32.437469, mean_q: -42.327972, mean_eps: 0.925875\n",
      "   99245/1200000: episode: 402, duration: 1.755s, episode steps: 260, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 5.661927, mae: 32.328374, mean_q: -42.087603, mean_eps: 0.925664\n",
      "   99421/1200000: episode: 403, duration: 1.197s, episode steps: 176, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 5.713888, mae: 32.407815, mean_q: -42.309834, mean_eps: 0.925501\n",
      "   99718/1200000: episode: 404, duration: 2.001s, episode steps: 297, steps per second: 148, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 5.446061, mae: 32.220073, mean_q: -41.982133, mean_eps: 0.925323\n",
      "  100005/1200000: episode: 405, duration: 1.971s, episode steps: 287, steps per second: 146, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 4.027811, mae: 32.177888, mean_q: -42.039800, mean_eps: 0.925104\n",
      "  100250/1200000: episode: 406, duration: 1.666s, episode steps: 245, steps per second: 147, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 4.019639, mae: 31.715629, mean_q: -41.453701, mean_eps: 0.924905\n",
      "  100425/1200000: episode: 407, duration: 1.220s, episode steps: 175, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 4.793051, mae: 31.840989, mean_q: -41.593562, mean_eps: 0.924747\n",
      "  100593/1200000: episode: 408, duration: 1.181s, episode steps: 168, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: 3.933881, mae: 31.820392, mean_q: -41.587167, mean_eps: 0.924619\n",
      "  100932/1200000: episode: 409, duration: 2.223s, episode steps: 339, steps per second: 152, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 4.288650, mae: 31.664317, mean_q: -41.323064, mean_eps: 0.924428\n",
      "  101179/1200000: episode: 410, duration: 1.687s, episode steps: 247, steps per second: 146, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 4.646175, mae: 31.652448, mean_q: -41.253676, mean_eps: 0.924209\n",
      "  101426/1200000: episode: 411, duration: 1.674s, episode steps: 247, steps per second: 148, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 4.618373, mae: 31.744739, mean_q: -41.402796, mean_eps: 0.924024\n",
      "  101743/1200000: episode: 412, duration: 2.138s, episode steps: 317, steps per second: 148, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 4.287516, mae: 31.697224, mean_q: -41.408644, mean_eps: 0.923812\n",
      "  102074/1200000: episode: 413, duration: 2.255s, episode steps: 331, steps per second: 147, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 4.196275, mae: 31.909526, mean_q: -41.666133, mean_eps: 0.923569\n",
      "  102245/1200000: episode: 414, duration: 1.178s, episode steps: 171, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 3.354195, mae: 31.696748, mean_q: -41.524645, mean_eps: 0.923381\n",
      "  102465/1200000: episode: 415, duration: 1.509s, episode steps: 220, steps per second: 146, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 5.115012, mae: 31.858833, mean_q: -41.543395, mean_eps: 0.923234\n",
      "  102748/1200000: episode: 416, duration: 1.901s, episode steps: 283, steps per second: 149, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 4.233407, mae: 31.775341, mean_q: -41.456171, mean_eps: 0.923046\n",
      "  103051/1200000: episode: 417, duration: 2.062s, episode steps: 303, steps per second: 147, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 3.981344, mae: 31.838579, mean_q: -41.611612, mean_eps: 0.922826\n",
      "  103391/1200000: episode: 418, duration: 2.296s, episode steps: 340, steps per second: 148, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 5.186832, mae: 31.754900, mean_q: -41.287248, mean_eps: 0.922585\n",
      "  103598/1200000: episode: 419, duration: 1.437s, episode steps: 207, steps per second: 144, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 4.064902, mae: 31.769088, mean_q: -41.557707, mean_eps: 0.922380\n",
      "  103773/1200000: episode: 420, duration: 1.210s, episode steps: 175, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 6.230028, mae: 31.790037, mean_q: -41.378579, mean_eps: 0.922236\n",
      "  104030/1200000: episode: 421, duration: 1.772s, episode steps: 257, steps per second: 145, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 4.431213, mae: 31.772972, mean_q: -41.541416, mean_eps: 0.922074\n",
      "  104352/1200000: episode: 422, duration: 2.181s, episode steps: 322, steps per second: 148, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 5.703689, mae: 31.794420, mean_q: -41.363258, mean_eps: 0.921857\n",
      "  104526/1200000: episode: 423, duration: 1.199s, episode steps: 174, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 5.678792, mae: 31.561362, mean_q: -41.212480, mean_eps: 0.921671\n",
      "  104798/1200000: episode: 424, duration: 1.866s, episode steps: 272, steps per second: 146, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 4.590286, mae: 31.896209, mean_q: -41.686069, mean_eps: 0.921504\n",
      "  104993/1200000: episode: 425, duration: 1.417s, episode steps: 195, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: 4.826232, mae: 31.838757, mean_q: -41.426830, mean_eps: 0.921329\n",
      "  105167/1200000: episode: 426, duration: 1.201s, episode steps: 174, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 4.285996, mae: 31.988856, mean_q: -41.730929, mean_eps: 0.921190\n",
      "  105361/1200000: episode: 427, duration: 1.328s, episode steps: 194, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 4.678408, mae: 31.744026, mean_q: -41.415240, mean_eps: 0.921052\n",
      "  105533/1200000: episode: 428, duration: 1.195s, episode steps: 172, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.430 [0.000, 3.000],  loss: 5.029763, mae: 31.795628, mean_q: -41.495575, mean_eps: 0.920915\n",
      "  105716/1200000: episode: 429, duration: 1.258s, episode steps: 183, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 4.115139, mae: 31.763692, mean_q: -41.503318, mean_eps: 0.920782\n",
      "  105968/1200000: episode: 430, duration: 1.726s, episode steps: 252, steps per second: 146, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 4.224342, mae: 31.703692, mean_q: -41.377597, mean_eps: 0.920619\n",
      "  106136/1200000: episode: 431, duration: 1.158s, episode steps: 168, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 4.532650, mae: 31.656844, mean_q: -41.369464, mean_eps: 0.920461\n",
      "  106319/1200000: episode: 432, duration: 1.287s, episode steps: 183, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 4.714698, mae: 31.786763, mean_q: -41.367149, mean_eps: 0.920330\n",
      "  106719/1200000: episode: 433, duration: 2.721s, episode steps: 400, steps per second: 147, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 4.005542, mae: 31.725894, mean_q: -41.423884, mean_eps: 0.920111\n",
      "  107096/1200000: episode: 434, duration: 2.578s, episode steps: 377, steps per second: 146, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 4.344508, mae: 31.610021, mean_q: -41.269138, mean_eps: 0.919820\n",
      "  107391/1200000: episode: 435, duration: 2.026s, episode steps: 295, steps per second: 146, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 5.123108, mae: 31.632190, mean_q: -41.270241, mean_eps: 0.919568\n",
      "  107556/1200000: episode: 436, duration: 1.177s, episode steps: 165, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.570 [0.000, 3.000],  loss: 3.640189, mae: 31.602046, mean_q: -41.366609, mean_eps: 0.919395\n",
      "  107730/1200000: episode: 437, duration: 1.197s, episode steps: 174, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 5.882499, mae: 31.766495, mean_q: -41.338582, mean_eps: 0.919268\n",
      "  108060/1200000: episode: 438, duration: 2.283s, episode steps: 330, steps per second: 145, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 3.957867, mae: 31.658918, mean_q: -41.306468, mean_eps: 0.919079\n",
      "  108336/1200000: episode: 439, duration: 1.903s, episode steps: 276, steps per second: 145, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 3.869353, mae: 31.701340, mean_q: -41.401018, mean_eps: 0.918852\n",
      "  108612/1200000: episode: 440, duration: 1.900s, episode steps: 276, steps per second: 145, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 4.718229, mae: 31.794474, mean_q: -41.432093, mean_eps: 0.918645\n",
      "  108868/1200000: episode: 441, duration: 1.773s, episode steps: 256, steps per second: 144, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 5.054052, mae: 31.786119, mean_q: -41.440365, mean_eps: 0.918445\n",
      "  109262/1200000: episode: 442, duration: 2.727s, episode steps: 394, steps per second: 144, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 3.881583, mae: 31.645420, mean_q: -41.381819, mean_eps: 0.918202\n",
      "  109525/1200000: episode: 443, duration: 1.831s, episode steps: 263, steps per second: 144, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 3.743591, mae: 31.779151, mean_q: -41.493409, mean_eps: 0.917955\n",
      "  109723/1200000: episode: 444, duration: 1.396s, episode steps: 198, steps per second: 142, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 4.402805, mae: 31.805278, mean_q: -41.557882, mean_eps: 0.917782\n",
      "  110005/1200000: episode: 445, duration: 1.971s, episode steps: 282, steps per second: 143, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 3.973515, mae: 31.693125, mean_q: -41.359703, mean_eps: 0.917602\n",
      "  110186/1200000: episode: 446, duration: 1.251s, episode steps: 181, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 4.075971, mae: 30.614390, mean_q: -39.929417, mean_eps: 0.917429\n",
      "  110452/1200000: episode: 447, duration: 1.862s, episode steps: 266, steps per second: 143, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 3.793449, mae: 30.580670, mean_q: -39.988045, mean_eps: 0.917261\n",
      "  110681/1200000: episode: 448, duration: 1.603s, episode steps: 229, steps per second: 143, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 3.217304, mae: 30.586316, mean_q: -39.979844, mean_eps: 0.917076\n",
      "  110900/1200000: episode: 449, duration: 1.522s, episode steps: 219, steps per second: 144, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 3.541314, mae: 30.582502, mean_q: -39.901842, mean_eps: 0.916908\n",
      "  111213/1200000: episode: 450, duration: 2.177s, episode steps: 313, steps per second: 144, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 3.812724, mae: 30.635287, mean_q: -39.994919, mean_eps: 0.916708\n",
      "  111455/1200000: episode: 451, duration: 1.677s, episode steps: 242, steps per second: 144, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 3.385408, mae: 30.612780, mean_q: -39.978722, mean_eps: 0.916500\n",
      "  111623/1200000: episode: 452, duration: 1.176s, episode steps: 168, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 3.846859, mae: 30.731467, mean_q: -40.099143, mean_eps: 0.916346\n",
      "  111981/1200000: episode: 453, duration: 2.485s, episode steps: 358, steps per second: 144, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 3.256726, mae: 30.579423, mean_q: -39.955212, mean_eps: 0.916149\n",
      "  112211/1200000: episode: 454, duration: 1.597s, episode steps: 230, steps per second: 144, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 4.253313, mae: 30.461937, mean_q: -39.727269, mean_eps: 0.915928\n",
      "  112381/1200000: episode: 455, duration: 1.195s, episode steps: 170, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 4.343544, mae: 30.482502, mean_q: -39.751873, mean_eps: 0.915778\n",
      "  112674/1200000: episode: 456, duration: 2.041s, episode steps: 293, steps per second: 144, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 3.640712, mae: 30.618261, mean_q: -39.980548, mean_eps: 0.915605\n",
      "  113071/1200000: episode: 457, duration: 2.837s, episode steps: 397, steps per second: 140, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 3.219429, mae: 30.627547, mean_q: -40.003001, mean_eps: 0.915346\n",
      "  113345/1200000: episode: 458, duration: 1.777s, episode steps: 274, steps per second: 154, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 4.424815, mae: 30.671789, mean_q: -40.014268, mean_eps: 0.915094\n",
      "  113635/1200000: episode: 459, duration: 1.985s, episode steps: 290, steps per second: 146, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 3.693210, mae: 30.540860, mean_q: -39.863292, mean_eps: 0.914883\n",
      "  113978/1200000: episode: 460, duration: 2.390s, episode steps: 343, steps per second: 143, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 3.612619, mae: 30.678929, mean_q: -40.099007, mean_eps: 0.914646\n",
      "  114270/1200000: episode: 461, duration: 2.037s, episode steps: 292, steps per second: 143, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 4.309747, mae: 30.632077, mean_q: -39.957518, mean_eps: 0.914407\n",
      "  114614/1200000: episode: 462, duration: 2.403s, episode steps: 344, steps per second: 143, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 3.597792, mae: 30.706484, mean_q: -40.158655, mean_eps: 0.914169\n",
      "  114829/1200000: episode: 463, duration: 1.504s, episode steps: 215, steps per second: 143, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.381 [0.000, 3.000],  loss: 5.039292, mae: 30.682465, mean_q: -40.015239, mean_eps: 0.913959\n",
      "  115340/1200000: episode: 464, duration: 3.561s, episode steps: 511, steps per second: 143, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 4.018081, mae: 30.712395, mean_q: -40.094657, mean_eps: 0.913687\n",
      "  115686/1200000: episode: 465, duration: 2.431s, episode steps: 346, steps per second: 142, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 4.110986, mae: 30.670490, mean_q: -40.015991, mean_eps: 0.913366\n",
      "  115857/1200000: episode: 466, duration: 1.213s, episode steps: 171, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 3.925082, mae: 30.657564, mean_q: -39.952002, mean_eps: 0.913172\n",
      "  116133/1200000: episode: 467, duration: 1.954s, episode steps: 276, steps per second: 141, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 3.586471, mae: 30.722703, mean_q: -40.138467, mean_eps: 0.913004\n",
      "  116452/1200000: episode: 468, duration: 2.234s, episode steps: 319, steps per second: 143, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 4.249305, mae: 30.646629, mean_q: -40.039691, mean_eps: 0.912781\n",
      "  116672/1200000: episode: 469, duration: 1.560s, episode steps: 220, steps per second: 141, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 2.784624, mae: 30.666346, mean_q: -40.121417, mean_eps: 0.912579\n",
      "  116926/1200000: episode: 470, duration: 1.789s, episode steps: 254, steps per second: 142, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 4.373554, mae: 30.777484, mean_q: -40.156027, mean_eps: 0.912401\n",
      "  117099/1200000: episode: 471, duration: 1.224s, episode steps: 173, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 3.031976, mae: 30.687788, mean_q: -40.167345, mean_eps: 0.912241\n",
      "  117262/1200000: episode: 472, duration: 1.165s, episode steps: 163, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.479 [0.000, 3.000],  loss: 3.303681, mae: 30.636475, mean_q: -40.054445, mean_eps: 0.912115\n",
      "  117618/1200000: episode: 473, duration: 2.511s, episode steps: 356, steps per second: 142, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 3.595027, mae: 30.675550, mean_q: -40.079519, mean_eps: 0.911920\n",
      "  117902/1200000: episode: 474, duration: 2.005s, episode steps: 284, steps per second: 142, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 2.767202, mae: 30.674574, mean_q: -40.143266, mean_eps: 0.911680\n",
      "  118199/1200000: episode: 475, duration: 2.105s, episode steps: 297, steps per second: 141, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 3.210689, mae: 30.760181, mean_q: -40.206437, mean_eps: 0.911462\n",
      "  118488/1200000: episode: 476, duration: 1.945s, episode steps: 289, steps per second: 149, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 3.291757, mae: 30.801913, mean_q: -40.296804, mean_eps: 0.911243\n",
      "  118831/1200000: episode: 477, duration: 2.407s, episode steps: 343, steps per second: 142, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 3.249598, mae: 30.595737, mean_q: -39.978639, mean_eps: 0.911006\n",
      "  119095/1200000: episode: 478, duration: 2.102s, episode steps: 264, steps per second: 126, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 3.837904, mae: 30.671799, mean_q: -40.079317, mean_eps: 0.910778\n",
      "  119405/1200000: episode: 479, duration: 2.302s, episode steps: 310, steps per second: 135, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 4.277156, mae: 30.748823, mean_q: -40.127716, mean_eps: 0.910563\n",
      "  119654/1200000: episode: 480, duration: 1.757s, episode steps: 249, steps per second: 142, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 3.479222, mae: 30.821471, mean_q: -40.222614, mean_eps: 0.910353\n",
      "  119860/1200000: episode: 481, duration: 1.463s, episode steps: 206, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.505 [0.000, 3.000],  loss: 3.004991, mae: 30.713740, mean_q: -40.208036, mean_eps: 0.910183\n",
      "  120052/1200000: episode: 482, duration: 1.369s, episode steps: 192, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.359 [0.000, 3.000],  loss: 3.608604, mae: 30.631603, mean_q: -40.103778, mean_eps: 0.910033\n",
      "  120292/1200000: episode: 483, duration: 1.739s, episode steps: 240, steps per second: 138, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 4.219649, mae: 30.441005, mean_q: -39.852289, mean_eps: 0.909871\n",
      "  120455/1200000: episode: 484, duration: 1.168s, episode steps: 163, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.374 [0.000, 3.000],  loss: 4.052950, mae: 30.309264, mean_q: -39.490921, mean_eps: 0.909720\n",
      "  120760/1200000: episode: 485, duration: 2.162s, episode steps: 305, steps per second: 141, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.338 [0.000, 3.000],  loss: 4.183573, mae: 30.423421, mean_q: -39.780469, mean_eps: 0.909545\n",
      "  121094/1200000: episode: 486, duration: 2.370s, episode steps: 334, steps per second: 141, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 4.894800, mae: 30.469404, mean_q: -39.782099, mean_eps: 0.909305\n",
      "  121328/1200000: episode: 487, duration: 1.665s, episode steps: 234, steps per second: 141, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 4.439578, mae: 30.317440, mean_q: -39.528036, mean_eps: 0.909092\n",
      "  121606/1200000: episode: 488, duration: 1.973s, episode steps: 278, steps per second: 141, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 3.970636, mae: 30.362799, mean_q: -39.653856, mean_eps: 0.908900\n",
      "  121918/1200000: episode: 489, duration: 2.227s, episode steps: 312, steps per second: 140, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 3.397152, mae: 30.371484, mean_q: -39.734650, mean_eps: 0.908679\n",
      "  122136/1200000: episode: 490, duration: 1.552s, episode steps: 218, steps per second: 140, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 4.475419, mae: 30.298518, mean_q: -39.623365, mean_eps: 0.908480\n",
      "  122329/1200000: episode: 491, duration: 1.383s, episode steps: 193, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 4.468321, mae: 30.428980, mean_q: -39.717781, mean_eps: 0.908326\n",
      "  122649/1200000: episode: 492, duration: 2.268s, episode steps: 320, steps per second: 141, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 4.315554, mae: 30.418355, mean_q: -39.680326, mean_eps: 0.908134\n",
      "  122840/1200000: episode: 493, duration: 1.373s, episode steps: 191, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 3.650893, mae: 30.215234, mean_q: -39.504101, mean_eps: 0.907942\n",
      "  123071/1200000: episode: 494, duration: 1.662s, episode steps: 231, steps per second: 139, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 3.600134, mae: 30.243933, mean_q: -39.515113, mean_eps: 0.907784\n",
      "  123335/1200000: episode: 495, duration: 1.906s, episode steps: 264, steps per second: 139, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 4.084763, mae: 30.402158, mean_q: -39.783309, mean_eps: 0.907598\n",
      "  123600/1200000: episode: 496, duration: 1.905s, episode steps: 265, steps per second: 139, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 3.686665, mae: 30.348862, mean_q: -39.677321, mean_eps: 0.907400\n",
      "  123924/1200000: episode: 497, duration: 2.292s, episode steps: 324, steps per second: 141, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 5.301265, mae: 30.481913, mean_q: -39.730458, mean_eps: 0.907179\n",
      "  124225/1200000: episode: 498, duration: 2.150s, episode steps: 301, steps per second: 140, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 4.668766, mae: 30.207692, mean_q: -39.416654, mean_eps: 0.906945\n",
      "  124450/1200000: episode: 499, duration: 1.629s, episode steps: 225, steps per second: 138, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 4.694235, mae: 30.312924, mean_q: -39.527913, mean_eps: 0.906747\n",
      "  124629/1200000: episode: 500, duration: 1.281s, episode steps: 179, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 6.216609, mae: 30.491655, mean_q: -39.801358, mean_eps: 0.906596\n",
      "  124878/1200000: episode: 501, duration: 1.791s, episode steps: 249, steps per second: 139, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 4.338529, mae: 30.258768, mean_q: -39.593302, mean_eps: 0.906435\n",
      "  125352/1200000: episode: 502, duration: 3.370s, episode steps: 474, steps per second: 141, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 3.684725, mae: 30.246713, mean_q: -39.572696, mean_eps: 0.906164\n",
      "  125641/1200000: episode: 503, duration: 2.058s, episode steps: 289, steps per second: 140, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 3.781630, mae: 30.333194, mean_q: -39.706270, mean_eps: 0.905878\n",
      "  125835/1200000: episode: 504, duration: 1.390s, episode steps: 194, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.521 [0.000, 3.000],  loss: 4.567933, mae: 30.508344, mean_q: -39.794033, mean_eps: 0.905697\n",
      "  126079/1200000: episode: 505, duration: 1.762s, episode steps: 244, steps per second: 138, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 4.232209, mae: 30.273910, mean_q: -39.543812, mean_eps: 0.905533\n",
      "  126256/1200000: episode: 506, duration: 1.292s, episode steps: 177, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.362 [0.000, 3.000],  loss: 3.344851, mae: 30.493260, mean_q: -39.931988, mean_eps: 0.905375\n",
      "  126441/1200000: episode: 507, duration: 1.329s, episode steps: 185, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 2.902119, mae: 30.339082, mean_q: -39.704712, mean_eps: 0.905239\n",
      "  126623/1200000: episode: 508, duration: 1.316s, episode steps: 182, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: 3.940864, mae: 30.401732, mean_q: -39.728090, mean_eps: 0.905101\n",
      "  126882/1200000: episode: 509, duration: 1.865s, episode steps: 259, steps per second: 139, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 3.527265, mae: 30.183360, mean_q: -39.451901, mean_eps: 0.904936\n",
      "  127060/1200000: episode: 510, duration: 1.287s, episode steps: 178, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 5.913022, mae: 30.235843, mean_q: -39.325912, mean_eps: 0.904772\n",
      "  127291/1200000: episode: 511, duration: 1.686s, episode steps: 231, steps per second: 137, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 3.528229, mae: 30.239472, mean_q: -39.538332, mean_eps: 0.904619\n",
      "  127468/1200000: episode: 512, duration: 1.284s, episode steps: 177, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: 2.737352, mae: 30.316123, mean_q: -39.714433, mean_eps: 0.904466\n",
      "  127643/1200000: episode: 513, duration: 1.289s, episode steps: 175, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 3.854280, mae: 30.234151, mean_q: -39.481051, mean_eps: 0.904334\n",
      "  127924/1200000: episode: 514, duration: 2.013s, episode steps: 281, steps per second: 140, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 4.082915, mae: 30.236394, mean_q: -39.558519, mean_eps: 0.904163\n",
      "  128238/1200000: episode: 515, duration: 2.283s, episode steps: 314, steps per second: 138, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 3.478988, mae: 30.287366, mean_q: -39.599018, mean_eps: 0.903940\n",
      "  128503/1200000: episode: 516, duration: 1.905s, episode steps: 265, steps per second: 139, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 4.481706, mae: 30.266607, mean_q: -39.481716, mean_eps: 0.903722\n",
      "  128688/1200000: episode: 517, duration: 1.361s, episode steps: 185, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 3.739807, mae: 30.081635, mean_q: -39.347324, mean_eps: 0.903554\n",
      "  128999/1200000: episode: 518, duration: 2.250s, episode steps: 311, steps per second: 138, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 3.938118, mae: 30.245957, mean_q: -39.501519, mean_eps: 0.903368\n",
      "  129178/1200000: episode: 519, duration: 1.304s, episode steps: 179, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 4.228202, mae: 30.289402, mean_q: -39.410901, mean_eps: 0.903184\n",
      "  129413/1200000: episode: 520, duration: 1.700s, episode steps: 235, steps per second: 138, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 4.307176, mae: 30.143071, mean_q: -39.377832, mean_eps: 0.903029\n",
      "  129729/1200000: episode: 521, duration: 2.284s, episode steps: 316, steps per second: 138, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 4.145455, mae: 30.167878, mean_q: -39.476466, mean_eps: 0.902822\n",
      "  129961/1200000: episode: 522, duration: 1.679s, episode steps: 232, steps per second: 138, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 4.085537, mae: 30.444512, mean_q: -39.773010, mean_eps: 0.902617\n",
      "  130195/1200000: episode: 523, duration: 1.711s, episode steps: 234, steps per second: 137, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 4.173846, mae: 29.780674, mean_q: -38.960635, mean_eps: 0.902442\n",
      "  130416/1200000: episode: 524, duration: 1.615s, episode steps: 221, steps per second: 137, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 3.955356, mae: 29.810360, mean_q: -38.986113, mean_eps: 0.902271\n",
      "  130589/1200000: episode: 525, duration: 1.280s, episode steps: 173, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: 3.586107, mae: 29.886768, mean_q: -39.141093, mean_eps: 0.902123\n",
      "  130985/1200000: episode: 526, duration: 2.851s, episode steps: 396, steps per second: 139, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 3.955231, mae: 29.835244, mean_q: -38.966969, mean_eps: 0.901910\n",
      "  131164/1200000: episode: 527, duration: 1.306s, episode steps: 179, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 2.965785, mae: 29.751199, mean_q: -39.059775, mean_eps: 0.901695\n",
      "  131361/1200000: episode: 528, duration: 1.444s, episode steps: 197, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 4.091431, mae: 29.927961, mean_q: -39.149279, mean_eps: 0.901554\n",
      "  131537/1200000: episode: 529, duration: 1.278s, episode steps: 176, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 4.549526, mae: 29.542395, mean_q: -38.594761, mean_eps: 0.901414\n",
      "  131892/1200000: episode: 530, duration: 2.568s, episode steps: 355, steps per second: 138, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 3.931277, mae: 29.726184, mean_q: -38.915346, mean_eps: 0.901215\n",
      "  132131/1200000: episode: 531, duration: 1.747s, episode steps: 239, steps per second: 137, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 3.905435, mae: 29.958410, mean_q: -39.247946, mean_eps: 0.900992\n",
      "  132421/1200000: episode: 532, duration: 2.119s, episode steps: 290, steps per second: 137, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 4.590873, mae: 29.705817, mean_q: -38.807067, mean_eps: 0.900793\n",
      "  132702/1200000: episode: 533, duration: 2.051s, episode steps: 281, steps per second: 137, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 5.280891, mae: 29.776288, mean_q: -38.809850, mean_eps: 0.900579\n",
      "  132931/1200000: episode: 534, duration: 1.684s, episode steps: 229, steps per second: 136, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 4.737547, mae: 29.699805, mean_q: -38.858803, mean_eps: 0.900388\n",
      "  133117/1200000: episode: 535, duration: 1.364s, episode steps: 186, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 4.465465, mae: 29.753058, mean_q: -38.885095, mean_eps: 0.900232\n",
      "  133489/1200000: episode: 536, duration: 2.692s, episode steps: 372, steps per second: 138, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 3.397395, mae: 29.783296, mean_q: -38.956068, mean_eps: 0.900023\n",
      "  133804/1200000: episode: 537, duration: 2.297s, episode steps: 315, steps per second: 137, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 4.282915, mae: 29.618952, mean_q: -38.770275, mean_eps: 0.899765\n",
      "  134068/1200000: episode: 538, duration: 1.949s, episode steps: 264, steps per second: 135, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 3.977100, mae: 29.693491, mean_q: -38.855190, mean_eps: 0.899548\n",
      "  134332/1200000: episode: 539, duration: 1.926s, episode steps: 264, steps per second: 137, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 3.917922, mae: 29.718844, mean_q: -38.881505, mean_eps: 0.899350\n",
      "  134644/1200000: episode: 540, duration: 2.294s, episode steps: 312, steps per second: 136, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 3.570535, mae: 29.879391, mean_q: -39.144791, mean_eps: 0.899134\n",
      "  134923/1200000: episode: 541, duration: 2.029s, episode steps: 279, steps per second: 137, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 3.050076, mae: 29.836067, mean_q: -39.115753, mean_eps: 0.898913\n",
      "  135099/1200000: episode: 542, duration: 1.304s, episode steps: 176, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 3.934065, mae: 29.671380, mean_q: -38.830962, mean_eps: 0.898742\n",
      "  135390/1200000: episode: 543, duration: 2.120s, episode steps: 291, steps per second: 137, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 3.050945, mae: 29.781128, mean_q: -39.022683, mean_eps: 0.898567\n",
      "  135574/1200000: episode: 544, duration: 1.367s, episode steps: 184, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 4.538120, mae: 29.812551, mean_q: -38.948923, mean_eps: 0.898389\n",
      "  135817/1200000: episode: 545, duration: 1.789s, episode steps: 243, steps per second: 136, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 3.973928, mae: 29.853570, mean_q: -39.076154, mean_eps: 0.898229\n",
      "  136171/1200000: episode: 546, duration: 2.584s, episode steps: 354, steps per second: 137, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 3.670088, mae: 29.785996, mean_q: -38.945724, mean_eps: 0.898005\n",
      "  136499/1200000: episode: 547, duration: 2.393s, episode steps: 328, steps per second: 137, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 4.107081, mae: 29.792115, mean_q: -38.933164, mean_eps: 0.897749\n",
      "  136707/1200000: episode: 548, duration: 1.556s, episode steps: 208, steps per second: 134, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 3.382481, mae: 29.672972, mean_q: -38.795101, mean_eps: 0.897548\n",
      "  136979/1200000: episode: 549, duration: 2.026s, episode steps: 272, steps per second: 134, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 3.676557, mae: 29.755826, mean_q: -38.899402, mean_eps: 0.897368\n",
      "  137354/1200000: episode: 550, duration: 2.753s, episode steps: 375, steps per second: 136, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 3.203453, mae: 29.793819, mean_q: -39.038769, mean_eps: 0.897125\n",
      "  137581/1200000: episode: 551, duration: 1.720s, episode steps: 227, steps per second: 132, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 3.318051, mae: 29.941957, mean_q: -39.127452, mean_eps: 0.896900\n",
      "  137809/1200000: episode: 552, duration: 1.679s, episode steps: 228, steps per second: 136, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 4.230204, mae: 29.783738, mean_q: -38.995210, mean_eps: 0.896729\n",
      "  138058/1200000: episode: 553, duration: 1.760s, episode steps: 249, steps per second: 141, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 3.860700, mae: 29.752423, mean_q: -38.961634, mean_eps: 0.896550\n",
      "  138427/1200000: episode: 554, duration: 2.742s, episode steps: 369, steps per second: 135, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 4.453042, mae: 29.816046, mean_q: -38.889203, mean_eps: 0.896318\n",
      "  138632/1200000: episode: 555, duration: 1.521s, episode steps: 205, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 4.160292, mae: 29.859002, mean_q: -39.082610, mean_eps: 0.896103\n",
      "  138811/1200000: episode: 556, duration: 1.340s, episode steps: 179, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 3.582184, mae: 29.871606, mean_q: -39.175600, mean_eps: 0.895959\n",
      "  139078/1200000: episode: 557, duration: 2.013s, episode steps: 267, steps per second: 133, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 3.508912, mae: 29.839675, mean_q: -39.066027, mean_eps: 0.895792\n",
      "  139435/1200000: episode: 558, duration: 2.610s, episode steps: 357, steps per second: 137, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.361 [0.000, 3.000],  loss: 4.381196, mae: 29.796431, mean_q: -38.960604, mean_eps: 0.895558\n",
      "  139674/1200000: episode: 559, duration: 1.769s, episode steps: 239, steps per second: 135, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 3.314989, mae: 29.959102, mean_q: -39.222685, mean_eps: 0.895334\n",
      "  139862/1200000: episode: 560, duration: 1.402s, episode steps: 188, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: 3.790000, mae: 29.830170, mean_q: -39.091866, mean_eps: 0.895174\n",
      "  140077/1200000: episode: 561, duration: 1.596s, episode steps: 215, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 4.210780, mae: 29.620615, mean_q: -38.685541, mean_eps: 0.895023\n",
      "  140256/1200000: episode: 562, duration: 1.332s, episode steps: 179, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 2.962924, mae: 29.376998, mean_q: -38.497066, mean_eps: 0.894875\n",
      "  140456/1200000: episode: 563, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 3.937902, mae: 29.227581, mean_q: -38.214302, mean_eps: 0.894733\n",
      "  140690/1200000: episode: 564, duration: 1.735s, episode steps: 234, steps per second: 135, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 3.347355, mae: 29.256963, mean_q: -38.304998, mean_eps: 0.894571\n",
      "  140866/1200000: episode: 565, duration: 1.326s, episode steps: 176, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.580 [0.000, 3.000],  loss: 3.511799, mae: 29.074788, mean_q: -38.051796, mean_eps: 0.894417\n",
      "  141129/1200000: episode: 566, duration: 1.962s, episode steps: 263, steps per second: 134, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.399 [0.000, 3.000],  loss: 4.033827, mae: 28.991579, mean_q: -37.930857, mean_eps: 0.894252\n",
      "  141378/1200000: episode: 567, duration: 1.846s, episode steps: 249, steps per second: 135, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 3.501104, mae: 29.161374, mean_q: -38.129682, mean_eps: 0.894060\n",
      "  141561/1200000: episode: 568, duration: 1.382s, episode steps: 183, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 3.270316, mae: 29.225869, mean_q: -38.295579, mean_eps: 0.893898\n",
      "  141730/1200000: episode: 569, duration: 1.276s, episode steps: 169, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 4.200503, mae: 29.132998, mean_q: -38.128305, mean_eps: 0.893766\n",
      "  141963/1200000: episode: 570, duration: 1.756s, episode steps: 233, steps per second: 133, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 3.814253, mae: 29.152818, mean_q: -38.101115, mean_eps: 0.893616\n",
      "  142141/1200000: episode: 571, duration: 1.320s, episode steps: 178, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 4.137475, mae: 29.138097, mean_q: -38.175453, mean_eps: 0.893461\n",
      "  142402/1200000: episode: 572, duration: 1.954s, episode steps: 261, steps per second: 134, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 3.136207, mae: 29.223547, mean_q: -38.257636, mean_eps: 0.893297\n",
      "  142577/1200000: episode: 573, duration: 1.379s, episode steps: 175, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 3.470482, mae: 28.779984, mean_q: -37.688200, mean_eps: 0.893133\n",
      "  142963/1200000: episode: 574, duration: 2.873s, episode steps: 386, steps per second: 134, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 4.380775, mae: 29.182601, mean_q: -38.128960, mean_eps: 0.892923\n",
      "  143208/1200000: episode: 575, duration: 1.829s, episode steps: 245, steps per second: 134, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 3.080471, mae: 29.106656, mean_q: -38.152635, mean_eps: 0.892686\n",
      "  143382/1200000: episode: 576, duration: 1.307s, episode steps: 174, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 3.703745, mae: 29.174151, mean_q: -38.218145, mean_eps: 0.892529\n",
      "  143657/1200000: episode: 577, duration: 2.054s, episode steps: 275, steps per second: 134, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 3.863181, mae: 29.133826, mean_q: -38.103165, mean_eps: 0.892361\n",
      "  143982/1200000: episode: 578, duration: 2.437s, episode steps: 325, steps per second: 133, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 3.295990, mae: 29.089764, mean_q: -38.040086, mean_eps: 0.892136\n",
      "  144232/1200000: episode: 579, duration: 1.865s, episode steps: 250, steps per second: 134, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 3.889466, mae: 29.244894, mean_q: -38.226261, mean_eps: 0.891920\n",
      "  144472/1200000: episode: 580, duration: 1.786s, episode steps: 240, steps per second: 134, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 3.116425, mae: 29.090779, mean_q: -38.094441, mean_eps: 0.891736\n",
      "  144794/1200000: episode: 581, duration: 2.580s, episode steps: 322, steps per second: 125, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 4.097196, mae: 29.153459, mean_q: -38.109790, mean_eps: 0.891526\n",
      "  145082/1200000: episode: 582, duration: 2.255s, episode steps: 288, steps per second: 128, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 4.355130, mae: 29.113696, mean_q: -38.003873, mean_eps: 0.891297\n",
      "  145282/1200000: episode: 583, duration: 1.601s, episode steps: 200, steps per second: 125, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 4.056799, mae: 29.239180, mean_q: -38.265643, mean_eps: 0.891114\n",
      "  145535/1200000: episode: 584, duration: 2.030s, episode steps: 253, steps per second: 125, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 3.555778, mae: 29.118542, mean_q: -38.076592, mean_eps: 0.890944\n",
      "  145905/1200000: episode: 585, duration: 2.923s, episode steps: 370, steps per second: 127, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 3.595895, mae: 29.140582, mean_q: -38.172088, mean_eps: 0.890710\n",
      "  146166/1200000: episode: 586, duration: 2.060s, episode steps: 261, steps per second: 127, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 4.221462, mae: 29.229273, mean_q: -38.147738, mean_eps: 0.890474\n",
      "  146340/1200000: episode: 587, duration: 1.390s, episode steps: 174, steps per second: 125, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 3.265443, mae: 29.217624, mean_q: -38.257531, mean_eps: 0.890311\n",
      "  146568/1200000: episode: 588, duration: 1.835s, episode steps: 228, steps per second: 124, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 3.818072, mae: 29.236083, mean_q: -38.274233, mean_eps: 0.890160\n",
      "  146744/1200000: episode: 589, duration: 1.438s, episode steps: 176, steps per second: 122, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: 4.231103, mae: 29.142585, mean_q: -38.098405, mean_eps: 0.890008\n",
      "  147024/1200000: episode: 590, duration: 2.204s, episode steps: 280, steps per second: 127, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 3.597785, mae: 29.218123, mean_q: -38.211305, mean_eps: 0.889837\n",
      "  147390/1200000: episode: 591, duration: 2.868s, episode steps: 366, steps per second: 128, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 3.448998, mae: 29.213633, mean_q: -38.192018, mean_eps: 0.889595\n",
      "  147637/1200000: episode: 592, duration: 1.975s, episode steps: 247, steps per second: 125, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 2.965944, mae: 29.141262, mean_q: -38.197246, mean_eps: 0.889365\n",
      "  147882/1200000: episode: 593, duration: 1.952s, episode steps: 245, steps per second: 125, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 2.903405, mae: 29.177794, mean_q: -38.255449, mean_eps: 0.889181\n",
      "  148271/1200000: episode: 594, duration: 3.060s, episode steps: 389, steps per second: 127, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 3.377357, mae: 29.249202, mean_q: -38.271170, mean_eps: 0.888943\n",
      "  148551/1200000: episode: 595, duration: 2.223s, episode steps: 280, steps per second: 126, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 3.670868, mae: 29.143048, mean_q: -38.080696, mean_eps: 0.888692\n",
      "  148788/1200000: episode: 596, duration: 1.879s, episode steps: 237, steps per second: 126, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 2.850622, mae: 29.233992, mean_q: -38.314102, mean_eps: 0.888498\n",
      "  148956/1200000: episode: 597, duration: 1.352s, episode steps: 168, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 4.691192, mae: 29.288376, mean_q: -38.224880, mean_eps: 0.888346\n",
      "  149188/1200000: episode: 598, duration: 1.844s, episode steps: 232, steps per second: 126, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 3.005227, mae: 29.131780, mean_q: -38.164954, mean_eps: 0.888196\n",
      "  149546/1200000: episode: 599, duration: 2.862s, episode steps: 358, steps per second: 125, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 3.361686, mae: 29.155730, mean_q: -38.108541, mean_eps: 0.887975\n",
      "  149810/1200000: episode: 600, duration: 2.112s, episode steps: 264, steps per second: 125, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 3.725851, mae: 29.192904, mean_q: -38.149798, mean_eps: 0.887742\n",
      "  150018/1200000: episode: 601, duration: 1.662s, episode steps: 208, steps per second: 125, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 3.213583, mae: 29.165937, mean_q: -38.206653, mean_eps: 0.887565\n",
      "  150326/1200000: episode: 602, duration: 2.471s, episode steps: 308, steps per second: 125, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 3.799118, mae: 28.972795, mean_q: -37.949431, mean_eps: 0.887371\n",
      "  150568/1200000: episode: 603, duration: 1.931s, episode steps: 242, steps per second: 125, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 3.984776, mae: 29.108326, mean_q: -38.082622, mean_eps: 0.887165\n",
      "  150904/1200000: episode: 604, duration: 2.689s, episode steps: 336, steps per second: 125, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 3.915953, mae: 28.995377, mean_q: -37.947258, mean_eps: 0.886948\n",
      "  151069/1200000: episode: 605, duration: 1.326s, episode steps: 165, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 3.139807, mae: 29.119493, mean_q: -38.162347, mean_eps: 0.886761\n",
      "  151330/1200000: episode: 606, duration: 2.110s, episode steps: 261, steps per second: 124, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 3.691006, mae: 28.966609, mean_q: -37.931006, mean_eps: 0.886601\n",
      "  151736/1200000: episode: 607, duration: 3.222s, episode steps: 406, steps per second: 126, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 2.680262, mae: 29.079896, mean_q: -38.123460, mean_eps: 0.886351\n",
      "  151901/1200000: episode: 608, duration: 1.334s, episode steps: 165, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 2.783023, mae: 29.059742, mean_q: -38.089835, mean_eps: 0.886136\n",
      "  152067/1200000: episode: 609, duration: 1.337s, episode steps: 166, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 4.972431, mae: 29.061613, mean_q: -37.895380, mean_eps: 0.886012\n",
      "  152407/1200000: episode: 610, duration: 2.686s, episode steps: 340, steps per second: 127, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 2.854087, mae: 29.004796, mean_q: -37.979881, mean_eps: 0.885823\n",
      "  152717/1200000: episode: 611, duration: 2.488s, episode steps: 310, steps per second: 125, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 2.806874, mae: 29.001724, mean_q: -38.065139, mean_eps: 0.885579\n",
      "  152876/1200000: episode: 612, duration: 1.298s, episode steps: 159, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 2.490236, mae: 28.950314, mean_q: -37.968950, mean_eps: 0.885403\n",
      "  153061/1200000: episode: 613, duration: 1.500s, episode steps: 185, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 3.583726, mae: 28.895837, mean_q: -37.847327, mean_eps: 0.885274\n",
      "  153359/1200000: episode: 614, duration: 2.403s, episode steps: 298, steps per second: 124, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 3.195232, mae: 29.070790, mean_q: -38.048840, mean_eps: 0.885093\n",
      "  153538/1200000: episode: 615, duration: 1.432s, episode steps: 179, steps per second: 125, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 3.326467, mae: 28.980516, mean_q: -37.907374, mean_eps: 0.884914\n",
      "  153791/1200000: episode: 616, duration: 2.024s, episode steps: 253, steps per second: 125, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 3.858321, mae: 29.008788, mean_q: -37.871916, mean_eps: 0.884752\n",
      "  154065/1200000: episode: 617, duration: 2.231s, episode steps: 274, steps per second: 123, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 3.871405, mae: 28.996212, mean_q: -37.913893, mean_eps: 0.884554\n",
      "  154242/1200000: episode: 618, duration: 1.443s, episode steps: 177, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 2.700792, mae: 28.952007, mean_q: -37.919539, mean_eps: 0.884385\n",
      "  154601/1200000: episode: 619, duration: 2.787s, episode steps: 359, steps per second: 129, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 3.657936, mae: 29.012267, mean_q: -38.005434, mean_eps: 0.884184\n",
      "  154954/1200000: episode: 620, duration: 2.838s, episode steps: 353, steps per second: 124, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 3.329918, mae: 28.892508, mean_q: -37.788782, mean_eps: 0.883917\n",
      "  155136/1200000: episode: 621, duration: 1.472s, episode steps: 182, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 3.433435, mae: 28.988732, mean_q: -37.997133, mean_eps: 0.883717\n",
      "  155405/1200000: episode: 622, duration: 2.173s, episode steps: 269, steps per second: 124, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 3.708988, mae: 28.963652, mean_q: -37.847005, mean_eps: 0.883547\n",
      "  155583/1200000: episode: 623, duration: 1.450s, episode steps: 178, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 2.640906, mae: 29.001735, mean_q: -37.987880, mean_eps: 0.883380\n",
      "  155823/1200000: episode: 624, duration: 1.941s, episode steps: 240, steps per second: 124, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 3.063328, mae: 29.032727, mean_q: -38.085946, mean_eps: 0.883223\n",
      "  155999/1200000: episode: 625, duration: 1.425s, episode steps: 176, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 2.220402, mae: 29.130642, mean_q: -38.270138, mean_eps: 0.883067\n",
      "  156333/1200000: episode: 626, duration: 2.677s, episode steps: 334, steps per second: 125, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 3.248739, mae: 28.894019, mean_q: -37.814694, mean_eps: 0.882876\n",
      "  156522/1200000: episode: 627, duration: 1.548s, episode steps: 189, steps per second: 122, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 2.927195, mae: 29.073690, mean_q: -38.124577, mean_eps: 0.882680\n",
      "  156693/1200000: episode: 628, duration: 2.006s, episode steps: 171, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 2.330949, mae: 29.190516, mean_q: -38.326067, mean_eps: 0.882545\n",
      "  156867/1200000: episode: 629, duration: 3.039s, episode steps: 174, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 3.752846, mae: 28.781732, mean_q: -37.588401, mean_eps: 0.882415\n",
      "  157086/1200000: episode: 630, duration: 3.505s, episode steps: 219, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 3.373534, mae: 28.998446, mean_q: -37.933794, mean_eps: 0.882268\n",
      "  157377/1200000: episode: 631, duration: 4.189s, episode steps: 291, steps per second:  69, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 3.061825, mae: 28.899514, mean_q: -37.927874, mean_eps: 0.882077\n",
      "  157664/1200000: episode: 632, duration: 2.461s, episode steps: 287, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 3.610218, mae: 28.913636, mean_q: -37.803676, mean_eps: 0.881860\n",
      "  158063/1200000: episode: 633, duration: 3.503s, episode steps: 399, steps per second: 114, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 3.505990, mae: 29.031157, mean_q: -37.995164, mean_eps: 0.881603\n",
      "  158298/1200000: episode: 634, duration: 2.282s, episode steps: 235, steps per second: 103, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 3.075396, mae: 29.157874, mean_q: -38.218824, mean_eps: 0.881365\n",
      "  158479/1200000: episode: 635, duration: 1.618s, episode steps: 181, steps per second: 112, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 3.767361, mae: 28.963245, mean_q: -37.936782, mean_eps: 0.881209\n",
      "  158689/1200000: episode: 636, duration: 1.790s, episode steps: 210, steps per second: 117, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 3.615357, mae: 29.093360, mean_q: -38.034632, mean_eps: 0.881062\n",
      "  158932/1200000: episode: 637, duration: 2.048s, episode steps: 243, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 2.704190, mae: 29.009505, mean_q: -38.040081, mean_eps: 0.880892\n",
      "  159196/1200000: episode: 638, duration: 2.161s, episode steps: 264, steps per second: 122, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 3.197608, mae: 29.054030, mean_q: -38.026726, mean_eps: 0.880702\n",
      "  159560/1200000: episode: 639, duration: 2.838s, episode steps: 364, steps per second: 128, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 3.848368, mae: 28.919474, mean_q: -37.828488, mean_eps: 0.880467\n",
      "  159844/1200000: episode: 640, duration: 2.203s, episode steps: 284, steps per second: 129, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 3.077759, mae: 29.001517, mean_q: -38.017435, mean_eps: 0.880224\n",
      "  160026/1200000: episode: 641, duration: 1.484s, episode steps: 182, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.670 [0.000, 3.000],  loss: 2.639290, mae: 28.767220, mean_q: -37.792305, mean_eps: 0.880049\n",
      "  160303/1200000: episode: 642, duration: 2.194s, episode steps: 277, steps per second: 126, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 2.896782, mae: 27.672900, mean_q: -36.275246, mean_eps: 0.879877\n",
      "  160624/1200000: episode: 643, duration: 2.605s, episode steps: 321, steps per second: 123, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 2.668449, mae: 27.813131, mean_q: -36.482287, mean_eps: 0.879653\n",
      "  160804/1200000: episode: 644, duration: 1.483s, episode steps: 180, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.694 [0.000, 3.000],  loss: 4.046901, mae: 27.889297, mean_q: -36.444836, mean_eps: 0.879465\n",
      "  161113/1200000: episode: 645, duration: 2.448s, episode steps: 309, steps per second: 126, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 2.790973, mae: 27.645364, mean_q: -36.275885, mean_eps: 0.879282\n",
      "  161288/1200000: episode: 646, duration: 1.418s, episode steps: 175, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 2.694024, mae: 27.854068, mean_q: -36.582820, mean_eps: 0.879100\n",
      "  161553/1200000: episode: 647, duration: 2.152s, episode steps: 265, steps per second: 123, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 3.308520, mae: 27.764760, mean_q: -36.356060, mean_eps: 0.878935\n",
      "  161927/1200000: episode: 648, duration: 3.033s, episode steps: 374, steps per second: 123, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 2.817039, mae: 27.674284, mean_q: -36.307592, mean_eps: 0.878695\n",
      "  162114/1200000: episode: 649, duration: 1.531s, episode steps: 187, steps per second: 122, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: 3.300248, mae: 27.635026, mean_q: -36.228898, mean_eps: 0.878485\n",
      "  162509/1200000: episode: 650, duration: 3.205s, episode steps: 395, steps per second: 123, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 2.974921, mae: 27.788777, mean_q: -36.400395, mean_eps: 0.878267\n",
      "  162798/1200000: episode: 651, duration: 2.341s, episode steps: 289, steps per second: 123, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 2.559724, mae: 27.751678, mean_q: -36.373709, mean_eps: 0.878010\n",
      "  162962/1200000: episode: 652, duration: 1.333s, episode steps: 164, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 2.645299, mae: 27.852649, mean_q: -36.516056, mean_eps: 0.877840\n",
      "  163172/1200000: episode: 653, duration: 1.730s, episode steps: 210, steps per second: 121, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 3.123628, mae: 27.824094, mean_q: -36.406138, mean_eps: 0.877700\n",
      "  163576/1200000: episode: 654, duration: 3.272s, episode steps: 404, steps per second: 123, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 3.636907, mae: 27.843277, mean_q: -36.388986, mean_eps: 0.877470\n",
      "  163820/1200000: episode: 655, duration: 1.970s, episode steps: 244, steps per second: 124, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 2.826706, mae: 27.794586, mean_q: -36.462882, mean_eps: 0.877227\n",
      "  164064/1200000: episode: 656, duration: 1.995s, episode steps: 244, steps per second: 122, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 3.037310, mae: 27.846189, mean_q: -36.446682, mean_eps: 0.877044\n",
      "  164257/1200000: episode: 657, duration: 1.603s, episode steps: 193, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 3.406254, mae: 27.747574, mean_q: -36.230000, mean_eps: 0.876880\n",
      "  164465/1200000: episode: 658, duration: 1.705s, episode steps: 208, steps per second: 122, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 3.672202, mae: 27.734969, mean_q: -36.229384, mean_eps: 0.876730\n",
      "  164841/1200000: episode: 659, duration: 3.056s, episode steps: 376, steps per second: 123, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 3.231236, mae: 27.750861, mean_q: -36.368061, mean_eps: 0.876511\n",
      "  165015/1200000: episode: 660, duration: 1.543s, episode steps: 174, steps per second: 113, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: 2.628636, mae: 27.668897, mean_q: -36.292347, mean_eps: 0.876304\n",
      "  165256/1200000: episode: 661, duration: 1.960s, episode steps: 241, steps per second: 123, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 3.296355, mae: 27.832422, mean_q: -36.417954, mean_eps: 0.876149\n",
      "  165530/1200000: episode: 662, duration: 2.276s, episode steps: 274, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 2.639524, mae: 27.698920, mean_q: -36.319035, mean_eps: 0.875956\n",
      "  165700/1200000: episode: 663, duration: 1.406s, episode steps: 170, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 3.173934, mae: 27.613555, mean_q: -36.098747, mean_eps: 0.875789\n",
      "  165863/1200000: episode: 664, duration: 1.371s, episode steps: 163, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 4.096591, mae: 27.679125, mean_q: -36.170362, mean_eps: 0.875664\n",
      "  166075/1200000: episode: 665, duration: 1.741s, episode steps: 212, steps per second: 122, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 3.655438, mae: 27.671240, mean_q: -36.223522, mean_eps: 0.875524\n",
      "  166248/1200000: episode: 666, duration: 1.437s, episode steps: 173, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 2.815087, mae: 27.720810, mean_q: -36.285972, mean_eps: 0.875379\n",
      "  166542/1200000: episode: 667, duration: 2.409s, episode steps: 294, steps per second: 122, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 2.337204, mae: 27.774244, mean_q: -36.452027, mean_eps: 0.875204\n",
      "  166809/1200000: episode: 668, duration: 2.197s, episode steps: 267, steps per second: 122, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 2.550269, mae: 27.612937, mean_q: -36.181459, mean_eps: 0.874994\n",
      "  167159/1200000: episode: 669, duration: 2.834s, episode steps: 350, steps per second: 124, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 2.937609, mae: 27.747112, mean_q: -36.367891, mean_eps: 0.874762\n",
      "  167430/1200000: episode: 670, duration: 2.255s, episode steps: 271, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 2.723353, mae: 27.828843, mean_q: -36.480636, mean_eps: 0.874530\n",
      "  167777/1200000: episode: 671, duration: 2.842s, episode steps: 347, steps per second: 122, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 3.350704, mae: 27.715518, mean_q: -36.272573, mean_eps: 0.874298\n",
      "  167955/1200000: episode: 672, duration: 1.469s, episode steps: 178, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 3.078148, mae: 27.715538, mean_q: -36.227613, mean_eps: 0.874101\n",
      "  168312/1200000: episode: 673, duration: 2.928s, episode steps: 357, steps per second: 122, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 2.838585, mae: 27.897868, mean_q: -36.539797, mean_eps: 0.873900\n",
      "  168499/1200000: episode: 674, duration: 1.536s, episode steps: 187, steps per second: 122, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 2.777192, mae: 27.663617, mean_q: -36.279347, mean_eps: 0.873696\n",
      "  168677/1200000: episode: 675, duration: 1.536s, episode steps: 178, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 3.374868, mae: 27.807061, mean_q: -36.524667, mean_eps: 0.873559\n",
      "  168961/1200000: episode: 676, duration: 2.325s, episode steps: 284, steps per second: 122, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 2.575357, mae: 27.782647, mean_q: -36.333133, mean_eps: 0.873386\n",
      "  169243/1200000: episode: 677, duration: 2.332s, episode steps: 282, steps per second: 121, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 2.575968, mae: 27.739760, mean_q: -36.296990, mean_eps: 0.873174\n",
      "  169411/1200000: episode: 678, duration: 1.409s, episode steps: 168, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 3.179541, mae: 27.677296, mean_q: -36.259724, mean_eps: 0.873005\n",
      "  169585/1200000: episode: 679, duration: 1.448s, episode steps: 174, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 3.315796, mae: 27.806721, mean_q: -36.376839, mean_eps: 0.872877\n",
      "  169759/1200000: episode: 680, duration: 1.463s, episode steps: 174, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 2.727551, mae: 27.792407, mean_q: -36.430643, mean_eps: 0.872746\n",
      "  169953/1200000: episode: 681, duration: 1.613s, episode steps: 194, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 3.441800, mae: 27.592988, mean_q: -36.112976, mean_eps: 0.872608\n",
      "  170223/1200000: episode: 682, duration: 2.230s, episode steps: 270, steps per second: 121, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 3.099344, mae: 27.220426, mean_q: -35.688549, mean_eps: 0.872434\n",
      "  170397/1200000: episode: 683, duration: 1.444s, episode steps: 174, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 2.659151, mae: 26.971995, mean_q: -35.391016, mean_eps: 0.872268\n",
      "  170561/1200000: episode: 684, duration: 1.395s, episode steps: 164, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 2.648547, mae: 26.983118, mean_q: -35.397398, mean_eps: 0.872141\n",
      "  170824/1200000: episode: 685, duration: 2.176s, episode steps: 263, steps per second: 121, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 2.396224, mae: 26.834701, mean_q: -35.172200, mean_eps: 0.871981\n",
      "  170997/1200000: episode: 686, duration: 1.460s, episode steps: 173, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.751 [0.000, 3.000],  loss: 3.507197, mae: 26.885698, mean_q: -35.177183, mean_eps: 0.871818\n",
      "  171272/1200000: episode: 687, duration: 2.263s, episode steps: 275, steps per second: 122, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 2.484651, mae: 26.915937, mean_q: -35.259320, mean_eps: 0.871650\n",
      "  171620/1200000: episode: 688, duration: 2.860s, episode steps: 348, steps per second: 122, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 3.175786, mae: 26.933047, mean_q: -35.239242, mean_eps: 0.871416\n",
      "  171978/1200000: episode: 689, duration: 2.949s, episode steps: 358, steps per second: 121, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 2.532869, mae: 26.983331, mean_q: -35.378385, mean_eps: 0.871151\n",
      "  172151/1200000: episode: 690, duration: 1.449s, episode steps: 173, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 2.369925, mae: 27.001497, mean_q: -35.405703, mean_eps: 0.870952\n",
      "  172440/1200000: episode: 691, duration: 2.424s, episode steps: 289, steps per second: 119, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 3.025251, mae: 26.850083, mean_q: -35.191839, mean_eps: 0.870779\n",
      "  172621/1200000: episode: 692, duration: 1.457s, episode steps: 181, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 3.352095, mae: 26.883678, mean_q: -35.146139, mean_eps: 0.870602\n",
      "  172864/1200000: episode: 693, duration: 2.043s, episode steps: 243, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 2.681604, mae: 26.876873, mean_q: -35.250716, mean_eps: 0.870443\n",
      "  173122/1200000: episode: 694, duration: 2.150s, episode steps: 258, steps per second: 120, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 3.997124, mae: 27.032592, mean_q: -35.351279, mean_eps: 0.870256\n",
      "  173367/1200000: episode: 695, duration: 2.051s, episode steps: 245, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 2.992962, mae: 26.937830, mean_q: -35.344561, mean_eps: 0.870067\n",
      "  173543/1200000: episode: 696, duration: 1.487s, episode steps: 176, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 2.620199, mae: 26.852475, mean_q: -35.160856, mean_eps: 0.869909\n",
      "  173718/1200000: episode: 697, duration: 1.479s, episode steps: 175, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 3.018085, mae: 27.000076, mean_q: -35.431299, mean_eps: 0.869777\n",
      "  173997/1200000: episode: 698, duration: 2.319s, episode steps: 279, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 2.539034, mae: 26.868713, mean_q: -35.279389, mean_eps: 0.869607\n",
      "  174236/1200000: episode: 699, duration: 1.988s, episode steps: 239, steps per second: 120, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 3.874538, mae: 26.899717, mean_q: -35.172618, mean_eps: 0.869413\n",
      "  174508/1200000: episode: 700, duration: 2.271s, episode steps: 272, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 2.881905, mae: 26.838461, mean_q: -35.130248, mean_eps: 0.869221\n",
      "  174787/1200000: episode: 701, duration: 2.330s, episode steps: 279, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 2.610377, mae: 26.937380, mean_q: -35.352357, mean_eps: 0.869015\n",
      "  174967/1200000: episode: 702, duration: 1.501s, episode steps: 180, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 2.748007, mae: 26.875272, mean_q: -35.138305, mean_eps: 0.868843\n",
      "  175146/1200000: episode: 703, duration: 1.545s, episode steps: 179, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 3.549545, mae: 26.995297, mean_q: -35.332956, mean_eps: 0.868708\n",
      "  175353/1200000: episode: 704, duration: 1.776s, episode steps: 207, steps per second: 117, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 2.633602, mae: 26.812380, mean_q: -35.144509, mean_eps: 0.868563\n",
      "  175626/1200000: episode: 705, duration: 2.313s, episode steps: 273, steps per second: 118, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 2.811979, mae: 26.887202, mean_q: -35.212293, mean_eps: 0.868383\n",
      "  175876/1200000: episode: 706, duration: 2.068s, episode steps: 250, steps per second: 121, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 3.055055, mae: 26.805255, mean_q: -35.109308, mean_eps: 0.868187\n",
      "  176146/1200000: episode: 707, duration: 2.340s, episode steps: 270, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 2.892985, mae: 26.839546, mean_q: -35.155304, mean_eps: 0.867992\n",
      "  176318/1200000: episode: 708, duration: 1.447s, episode steps: 172, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 2.423150, mae: 26.947033, mean_q: -35.347795, mean_eps: 0.867826\n",
      "  176490/1200000: episode: 709, duration: 1.466s, episode steps: 172, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 3.510886, mae: 27.056228, mean_q: -35.289559, mean_eps: 0.867697\n",
      "  176695/1200000: episode: 710, duration: 1.731s, episode steps: 205, steps per second: 118, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 2.645913, mae: 26.998963, mean_q: -35.321385, mean_eps: 0.867556\n",
      "  176989/1200000: episode: 711, duration: 2.500s, episode steps: 294, steps per second: 118, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 3.044589, mae: 26.808685, mean_q: -35.060847, mean_eps: 0.867369\n",
      "  177200/1200000: episode: 712, duration: 1.799s, episode steps: 211, steps per second: 117, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 2.545678, mae: 26.721364, mean_q: -35.018883, mean_eps: 0.867179\n",
      "  177505/1200000: episode: 713, duration: 2.555s, episode steps: 305, steps per second: 119, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 3.008549, mae: 26.912188, mean_q: -35.245708, mean_eps: 0.866986\n",
      "  177693/1200000: episode: 714, duration: 1.593s, episode steps: 188, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.388 [0.000, 3.000],  loss: 2.390878, mae: 26.897588, mean_q: -35.254843, mean_eps: 0.866801\n",
      "  177919/1200000: episode: 715, duration: 1.903s, episode steps: 226, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 2.855253, mae: 26.997233, mean_q: -35.414619, mean_eps: 0.866646\n",
      "  178257/1200000: episode: 716, duration: 2.839s, episode steps: 338, steps per second: 119, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 2.632981, mae: 26.989197, mean_q: -35.393604, mean_eps: 0.866434\n",
      "  178497/1200000: episode: 717, duration: 2.018s, episode steps: 240, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 2.824494, mae: 26.795588, mean_q: -35.074334, mean_eps: 0.866218\n",
      "  178951/1200000: episode: 718, duration: 3.772s, episode steps: 454, steps per second: 120, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 2.694112, mae: 27.012419, mean_q: -35.396748, mean_eps: 0.865957\n",
      "  179136/1200000: episode: 719, duration: 1.562s, episode steps: 185, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 2.696581, mae: 26.872175, mean_q: -35.231621, mean_eps: 0.865718\n",
      "  179311/1200000: episode: 720, duration: 1.500s, episode steps: 175, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 1.923421, mae: 26.827783, mean_q: -35.245840, mean_eps: 0.865583\n",
      "  179482/1200000: episode: 721, duration: 1.463s, episode steps: 171, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 2.331371, mae: 26.856440, mean_q: -35.264884, mean_eps: 0.865453\n",
      "  179727/1200000: episode: 722, duration: 2.063s, episode steps: 245, steps per second: 119, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 3.056256, mae: 26.811630, mean_q: -35.088466, mean_eps: 0.865297\n",
      "  180051/1200000: episode: 723, duration: 2.734s, episode steps: 324, steps per second: 119, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 2.343759, mae: 26.585905, mean_q: -34.864863, mean_eps: 0.865084\n",
      "  180228/1200000: episode: 724, duration: 1.492s, episode steps: 177, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 2.586119, mae: 26.004487, mean_q: -34.087846, mean_eps: 0.864896\n",
      "  180411/1200000: episode: 725, duration: 1.528s, episode steps: 183, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 2.309134, mae: 25.988289, mean_q: -34.068231, mean_eps: 0.864761\n",
      "  180643/1200000: episode: 726, duration: 1.974s, episode steps: 232, steps per second: 118, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 2.167285, mae: 26.080723, mean_q: -34.204304, mean_eps: 0.864605\n",
      "  180861/1200000: episode: 727, duration: 1.837s, episode steps: 218, steps per second: 119, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 3.426542, mae: 26.072577, mean_q: -34.048438, mean_eps: 0.864436\n",
      "  181121/1200000: episode: 728, duration: 2.244s, episode steps: 260, steps per second: 116, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 2.750047, mae: 26.203509, mean_q: -34.365873, mean_eps: 0.864257\n",
      "  181520/1200000: episode: 729, duration: 3.267s, episode steps: 399, steps per second: 122, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 2.973164, mae: 26.046840, mean_q: -34.074617, mean_eps: 0.864010\n",
      "  181834/1200000: episode: 730, duration: 2.660s, episode steps: 314, steps per second: 118, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 2.124928, mae: 25.966979, mean_q: -34.100523, mean_eps: 0.863743\n",
      "  181999/1200000: episode: 731, duration: 1.432s, episode steps: 165, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 2.468361, mae: 26.063782, mean_q: -34.145708, mean_eps: 0.863563\n",
      "  182182/1200000: episode: 732, duration: 1.554s, episode steps: 183, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 2.805353, mae: 26.060100, mean_q: -34.091890, mean_eps: 0.863433\n",
      "  182475/1200000: episode: 733, duration: 2.504s, episode steps: 293, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 2.871065, mae: 26.052490, mean_q: -34.133628, mean_eps: 0.863254\n",
      "  182862/1200000: episode: 734, duration: 3.264s, episode steps: 387, steps per second: 119, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 2.445932, mae: 26.080937, mean_q: -34.187263, mean_eps: 0.862999\n",
      "  183185/1200000: episode: 735, duration: 2.719s, episode steps: 323, steps per second: 119, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 2.491315, mae: 26.194564, mean_q: -34.289836, mean_eps: 0.862733\n",
      "  183422/1200000: episode: 736, duration: 2.065s, episode steps: 237, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 2.204942, mae: 26.085635, mean_q: -34.196793, mean_eps: 0.862523\n",
      "  183591/1200000: episode: 737, duration: 1.442s, episode steps: 169, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 2.096832, mae: 26.251995, mean_q: -34.451353, mean_eps: 0.862370\n",
      "  183768/1200000: episode: 738, duration: 1.510s, episode steps: 177, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.356 [0.000, 3.000],  loss: 2.380155, mae: 26.097159, mean_q: -34.260440, mean_eps: 0.862241\n",
      "  184115/1200000: episode: 739, duration: 2.927s, episode steps: 347, steps per second: 119, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 2.626850, mae: 26.136407, mean_q: -34.221283, mean_eps: 0.862044\n",
      "  184349/1200000: episode: 740, duration: 2.075s, episode steps: 234, steps per second: 113, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 2.980929, mae: 26.085160, mean_q: -34.155478, mean_eps: 0.861826\n",
      "  184521/1200000: episode: 741, duration: 1.462s, episode steps: 172, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.523 [0.000, 3.000],  loss: 2.675024, mae: 26.124675, mean_q: -34.171503, mean_eps: 0.861674\n",
      "  184763/1200000: episode: 742, duration: 2.060s, episode steps: 242, steps per second: 117, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 2.234311, mae: 26.210062, mean_q: -34.429753, mean_eps: 0.861519\n",
      "  184948/1200000: episode: 743, duration: 1.581s, episode steps: 185, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 2.699994, mae: 25.861364, mean_q: -33.875538, mean_eps: 0.861359\n",
      "  185220/1200000: episode: 744, duration: 2.320s, episode steps: 272, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 2.706687, mae: 26.054519, mean_q: -34.146467, mean_eps: 0.861187\n",
      "  185483/1200000: episode: 745, duration: 2.251s, episode steps: 263, steps per second: 117, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 2.228118, mae: 26.026519, mean_q: -34.174554, mean_eps: 0.860987\n",
      "  185697/1200000: episode: 746, duration: 1.822s, episode steps: 214, steps per second: 117, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 3.183711, mae: 26.032232, mean_q: -34.083509, mean_eps: 0.860808\n",
      "  185874/1200000: episode: 747, duration: 1.537s, episode steps: 177, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 2.826313, mae: 26.181646, mean_q: -34.307027, mean_eps: 0.860661\n",
      "  186150/1200000: episode: 748, duration: 2.361s, episode steps: 276, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 2.692249, mae: 26.031263, mean_q: -34.136484, mean_eps: 0.860491\n",
      "  186413/1200000: episode: 749, duration: 2.249s, episode steps: 263, steps per second: 117, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 2.868344, mae: 26.052050, mean_q: -34.091027, mean_eps: 0.860289\n",
      "  186727/1200000: episode: 750, duration: 2.661s, episode steps: 314, steps per second: 118, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 2.572867, mae: 26.055328, mean_q: -34.132798, mean_eps: 0.860073\n",
      "  187009/1200000: episode: 751, duration: 2.425s, episode steps: 282, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 2.930147, mae: 26.016999, mean_q: -34.000225, mean_eps: 0.859849\n",
      "  187238/1200000: episode: 752, duration: 1.941s, episode steps: 229, steps per second: 118, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 3.151859, mae: 26.098635, mean_q: -34.176574, mean_eps: 0.859658\n",
      "  187403/1200000: episode: 753, duration: 1.443s, episode steps: 165, steps per second: 114, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 2.204192, mae: 26.146723, mean_q: -34.332243, mean_eps: 0.859510\n",
      "  187581/1200000: episode: 754, duration: 1.513s, episode steps: 178, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 2.036024, mae: 26.086430, mean_q: -34.173087, mean_eps: 0.859381\n",
      "  187818/1200000: episode: 755, duration: 2.040s, episode steps: 237, steps per second: 116, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 2.699259, mae: 26.089573, mean_q: -34.163928, mean_eps: 0.859226\n",
      "  188051/1200000: episode: 756, duration: 2.137s, episode steps: 233, steps per second: 109, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 2.242623, mae: 26.018574, mean_q: -34.103366, mean_eps: 0.859050\n",
      "  188352/1200000: episode: 757, duration: 2.578s, episode steps: 301, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 2.449689, mae: 26.037439, mean_q: -34.167158, mean_eps: 0.858849\n",
      "  188548/1200000: episode: 758, duration: 1.660s, episode steps: 196, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 2.676880, mae: 26.124759, mean_q: -34.229255, mean_eps: 0.858663\n",
      "  188878/1200000: episode: 759, duration: 2.840s, episode steps: 330, steps per second: 116, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 2.893892, mae: 26.004562, mean_q: -33.996721, mean_eps: 0.858466\n",
      "  189166/1200000: episode: 760, duration: 2.512s, episode steps: 288, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 2.457482, mae: 26.188407, mean_q: -34.364496, mean_eps: 0.858234\n",
      "  189345/1200000: episode: 761, duration: 1.572s, episode steps: 179, steps per second: 114, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.430 [0.000, 3.000],  loss: 2.502790, mae: 26.188756, mean_q: -34.325237, mean_eps: 0.858059\n",
      "  189570/1200000: episode: 762, duration: 1.993s, episode steps: 225, steps per second: 113, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 2.275154, mae: 26.073677, mean_q: -34.225535, mean_eps: 0.857907\n",
      "  189750/1200000: episode: 763, duration: 1.564s, episode steps: 180, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 2.612946, mae: 26.089779, mean_q: -34.156091, mean_eps: 0.857755\n",
      "  189921/1200000: episode: 764, duration: 1.463s, episode steps: 171, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 2.635050, mae: 26.037340, mean_q: -34.113435, mean_eps: 0.857624\n",
      "  190098/1200000: episode: 765, duration: 1.515s, episode steps: 177, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 2.658171, mae: 25.744269, mean_q: -33.703771, mean_eps: 0.857493\n",
      "  190504/1200000: episode: 766, duration: 3.561s, episode steps: 406, steps per second: 114, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 2.338573, mae: 25.653291, mean_q: -33.612410, mean_eps: 0.857275\n",
      "  190697/1200000: episode: 767, duration: 1.774s, episode steps: 193, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 2.674803, mae: 25.593127, mean_q: -33.535267, mean_eps: 0.857050\n",
      "  191090/1200000: episode: 768, duration: 3.342s, episode steps: 393, steps per second: 118, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 2.864946, mae: 25.608643, mean_q: -33.525779, mean_eps: 0.856830\n",
      "  191429/1200000: episode: 769, duration: 3.001s, episode steps: 339, steps per second: 113, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 2.340763, mae: 25.619873, mean_q: -33.595771, mean_eps: 0.856556\n",
      "  191729/1200000: episode: 770, duration: 2.599s, episode steps: 300, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 2.394992, mae: 25.676239, mean_q: -33.663045, mean_eps: 0.856316\n",
      "  191896/1200000: episode: 771, duration: 1.440s, episode steps: 167, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 3.046025, mae: 25.717602, mean_q: -33.645586, mean_eps: 0.856141\n",
      "  192087/1200000: episode: 772, duration: 1.619s, episode steps: 191, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 2.607940, mae: 25.698316, mean_q: -33.684337, mean_eps: 0.856007\n",
      "  192269/1200000: episode: 773, duration: 1.548s, episode steps: 182, steps per second: 118, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.495 [0.000, 3.000],  loss: 2.030790, mae: 25.531426, mean_q: -33.508788, mean_eps: 0.855867\n",
      "  192655/1200000: episode: 774, duration: 3.337s, episode steps: 386, steps per second: 116, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 2.874366, mae: 25.571275, mean_q: -33.494296, mean_eps: 0.855654\n",
      "  192932/1200000: episode: 775, duration: 2.371s, episode steps: 277, steps per second: 117, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 1.961694, mae: 25.540188, mean_q: -33.499564, mean_eps: 0.855405\n",
      "  193203/1200000: episode: 776, duration: 2.332s, episode steps: 271, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 2.055529, mae: 25.505220, mean_q: -33.491227, mean_eps: 0.855200\n",
      "  193477/1200000: episode: 777, duration: 2.353s, episode steps: 274, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 2.305618, mae: 25.641023, mean_q: -33.597888, mean_eps: 0.854995\n",
      "  193760/1200000: episode: 778, duration: 2.359s, episode steps: 283, steps per second: 120, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 2.861924, mae: 25.595692, mean_q: -33.552055, mean_eps: 0.854787\n",
      "  193974/1200000: episode: 779, duration: 1.867s, episode steps: 214, steps per second: 115, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 1.973418, mae: 25.739360, mean_q: -33.722668, mean_eps: 0.854600\n",
      "  194144/1200000: episode: 780, duration: 1.458s, episode steps: 170, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 2.004316, mae: 25.653809, mean_q: -33.663408, mean_eps: 0.854456\n",
      "  194512/1200000: episode: 781, duration: 3.321s, episode steps: 368, steps per second: 111, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 2.151828, mae: 25.704261, mean_q: -33.721120, mean_eps: 0.854254\n",
      "  194744/1200000: episode: 782, duration: 2.020s, episode steps: 232, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 2.118928, mae: 25.815387, mean_q: -33.779216, mean_eps: 0.854029\n",
      "  194949/1200000: episode: 783, duration: 1.786s, episode steps: 205, steps per second: 115, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 3.269640, mae: 25.640800, mean_q: -33.534455, mean_eps: 0.853866\n",
      "  195131/1200000: episode: 784, duration: 1.570s, episode steps: 182, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 1.716511, mae: 25.506697, mean_q: -33.515713, mean_eps: 0.853720\n",
      "  195352/1200000: episode: 785, duration: 1.902s, episode steps: 221, steps per second: 116, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 2.625694, mae: 25.478516, mean_q: -33.362817, mean_eps: 0.853569\n",
      "  195852/1200000: episode: 786, duration: 4.239s, episode steps: 500, steps per second: 118, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 2.779245, mae: 25.657159, mean_q: -33.609640, mean_eps: 0.853299\n",
      "  196025/1200000: episode: 787, duration: 1.511s, episode steps: 173, steps per second: 114, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 2.728844, mae: 25.610811, mean_q: -33.580840, mean_eps: 0.853047\n",
      "  196198/1200000: episode: 788, duration: 1.510s, episode steps: 173, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 1.602335, mae: 25.824771, mean_q: -33.975691, mean_eps: 0.852917\n",
      "  196587/1200000: episode: 789, duration: 3.369s, episode steps: 389, steps per second: 115, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 2.388636, mae: 25.596934, mean_q: -33.564674, mean_eps: 0.852706\n",
      "  196932/1200000: episode: 790, duration: 2.973s, episode steps: 345, steps per second: 116, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 2.004149, mae: 25.649139, mean_q: -33.652549, mean_eps: 0.852431\n",
      "  197097/1200000: episode: 791, duration: 1.446s, episode steps: 165, steps per second: 114, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 1.687436, mae: 25.696559, mean_q: -33.742148, mean_eps: 0.852240\n",
      "  197339/1200000: episode: 792, duration: 2.091s, episode steps: 242, steps per second: 116, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 2.477742, mae: 25.718220, mean_q: -33.670391, mean_eps: 0.852087\n",
      "  197573/1200000: episode: 793, duration: 2.029s, episode steps: 234, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 2.139899, mae: 25.565603, mean_q: -33.528756, mean_eps: 0.851908\n",
      "  197829/1200000: episode: 794, duration: 2.214s, episode steps: 256, steps per second: 116, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 2.378983, mae: 25.583834, mean_q: -33.501112, mean_eps: 0.851725\n",
      "  198001/1200000: episode: 795, duration: 1.502s, episode steps: 172, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.430 [0.000, 3.000],  loss: 2.976424, mae: 25.495216, mean_q: -33.351362, mean_eps: 0.851564\n",
      "  198285/1200000: episode: 796, duration: 2.440s, episode steps: 284, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 2.632599, mae: 25.636110, mean_q: -33.630371, mean_eps: 0.851393\n",
      "  198581/1200000: episode: 797, duration: 2.564s, episode steps: 296, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 2.293258, mae: 25.684149, mean_q: -33.679943, mean_eps: 0.851176\n",
      "  198889/1200000: episode: 798, duration: 2.662s, episode steps: 308, steps per second: 116, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 2.059095, mae: 25.685340, mean_q: -33.691848, mean_eps: 0.850949\n",
      "  199266/1200000: episode: 799, duration: 3.240s, episode steps: 377, steps per second: 116, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 2.484542, mae: 25.592458, mean_q: -33.578473, mean_eps: 0.850692\n",
      "  199494/1200000: episode: 800, duration: 1.982s, episode steps: 228, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 2.297758, mae: 25.569402, mean_q: -33.484888, mean_eps: 0.850465\n",
      "  199671/1200000: episode: 801, duration: 1.537s, episode steps: 177, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.718 [0.000, 3.000],  loss: 2.435899, mae: 25.551266, mean_q: -33.479426, mean_eps: 0.850313\n",
      "  199881/1200000: episode: 802, duration: 1.815s, episode steps: 210, steps per second: 116, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 2.326081, mae: 25.591076, mean_q: -33.560233, mean_eps: 0.850168\n",
      "  200109/1200000: episode: 803, duration: 1.990s, episode steps: 228, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 2.406605, mae: 25.291934, mean_q: -33.137676, mean_eps: 0.850004\n",
      "  200321/1200000: episode: 804, duration: 1.810s, episode steps: 212, steps per second: 117, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 2.387193, mae: 25.051344, mean_q: -32.832308, mean_eps: 0.849839\n",
      "  200634/1200000: episode: 805, duration: 2.701s, episode steps: 313, steps per second: 116, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 2.436144, mae: 25.100886, mean_q: -32.907257, mean_eps: 0.849642\n",
      "  201041/1200000: episode: 806, duration: 3.528s, episode steps: 407, steps per second: 115, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 2.396943, mae: 25.067783, mean_q: -32.844816, mean_eps: 0.849372\n",
      "  201231/1200000: episode: 807, duration: 1.662s, episode steps: 190, steps per second: 114, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 2.587619, mae: 25.163461, mean_q: -32.924109, mean_eps: 0.849148\n",
      "  201528/1200000: episode: 808, duration: 2.557s, episode steps: 297, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 2.336291, mae: 25.177099, mean_q: -33.010485, mean_eps: 0.848966\n",
      "  201780/1200000: episode: 809, duration: 2.185s, episode steps: 252, steps per second: 115, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 2.667680, mae: 25.088583, mean_q: -32.859929, mean_eps: 0.848760\n",
      "  202170/1200000: episode: 810, duration: 3.369s, episode steps: 390, steps per second: 116, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 2.086804, mae: 25.160935, mean_q: -32.988762, mean_eps: 0.848519\n",
      "  202496/1200000: episode: 811, duration: 2.829s, episode steps: 326, steps per second: 115, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 1.917045, mae: 25.099550, mean_q: -32.966436, mean_eps: 0.848251\n",
      "  202800/1200000: episode: 812, duration: 2.615s, episode steps: 304, steps per second: 116, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 2.332892, mae: 25.206849, mean_q: -33.001371, mean_eps: 0.848014\n",
      "  202971/1200000: episode: 813, duration: 1.483s, episode steps: 171, steps per second: 115, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 2.427217, mae: 25.197876, mean_q: -33.030032, mean_eps: 0.847836\n",
      "  203243/1200000: episode: 814, duration: 2.389s, episode steps: 272, steps per second: 114, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 2.325347, mae: 25.235666, mean_q: -33.074294, mean_eps: 0.847670\n",
      "  203589/1200000: episode: 815, duration: 2.999s, episode steps: 346, steps per second: 115, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 1.839364, mae: 25.178346, mean_q: -33.041058, mean_eps: 0.847438\n",
      "  203866/1200000: episode: 816, duration: 2.410s, episode steps: 277, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 2.501564, mae: 25.190989, mean_q: -33.010639, mean_eps: 0.847205\n",
      "  204271/1200000: episode: 817, duration: 3.497s, episode steps: 405, steps per second: 116, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 2.371936, mae: 25.108148, mean_q: -32.899616, mean_eps: 0.846949\n",
      "  204637/1200000: episode: 818, duration: 3.165s, episode steps: 366, steps per second: 116, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 2.074248, mae: 25.182296, mean_q: -33.005312, mean_eps: 0.846660\n",
      "  204890/1200000: episode: 819, duration: 2.188s, episode steps: 253, steps per second: 116, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 2.244000, mae: 25.181064, mean_q: -33.010310, mean_eps: 0.846428\n",
      "  205124/1200000: episode: 820, duration: 2.051s, episode steps: 234, steps per second: 114, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 2.181611, mae: 25.164680, mean_q: -32.971162, mean_eps: 0.846245\n",
      "  205360/1200000: episode: 821, duration: 2.069s, episode steps: 236, steps per second: 114, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 2.621697, mae: 25.177121, mean_q: -32.925228, mean_eps: 0.846069\n",
      "  205636/1200000: episode: 822, duration: 2.410s, episode steps: 276, steps per second: 115, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 2.179880, mae: 25.142669, mean_q: -32.979013, mean_eps: 0.845877\n",
      "  205905/1200000: episode: 823, duration: 2.355s, episode steps: 269, steps per second: 114, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 1.891647, mae: 25.211973, mean_q: -33.117567, mean_eps: 0.845673\n",
      "  206182/1200000: episode: 824, duration: 2.459s, episode steps: 277, steps per second: 113, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 1.906249, mae: 25.157926, mean_q: -33.006372, mean_eps: 0.845468\n",
      "  206388/1200000: episode: 825, duration: 1.821s, episode steps: 206, steps per second: 113, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 2.063950, mae: 25.337837, mean_q: -33.209452, mean_eps: 0.845287\n",
      "  206657/1200000: episode: 826, duration: 2.353s, episode steps: 269, steps per second: 114, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 2.458186, mae: 25.301281, mean_q: -33.143744, mean_eps: 0.845108\n",
      "  206931/1200000: episode: 827, duration: 2.410s, episode steps: 274, steps per second: 114, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 2.182497, mae: 25.021015, mean_q: -32.841979, mean_eps: 0.844905\n",
      "  207209/1200000: episode: 828, duration: 2.432s, episode steps: 278, steps per second: 114, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 2.618279, mae: 25.250533, mean_q: -33.100306, mean_eps: 0.844698\n",
      "  207377/1200000: episode: 829, duration: 1.500s, episode steps: 168, steps per second: 112, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 2.230536, mae: 25.097791, mean_q: -32.924972, mean_eps: 0.844531\n",
      "  207717/1200000: episode: 830, duration: 2.968s, episode steps: 340, steps per second: 115, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 2.539062, mae: 25.212859, mean_q: -33.038156, mean_eps: 0.844340\n",
      "  207880/1200000: episode: 831, duration: 1.495s, episode steps: 163, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 2.712676, mae: 25.306035, mean_q: -33.098486, mean_eps: 0.844152\n",
      "  208157/1200000: episode: 832, duration: 2.483s, episode steps: 277, steps per second: 112, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 1.644641, mae: 25.194007, mean_q: -33.071433, mean_eps: 0.843987\n",
      "  208454/1200000: episode: 833, duration: 2.666s, episode steps: 297, steps per second: 111, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 2.352227, mae: 25.215915, mean_q: -33.015499, mean_eps: 0.843771\n",
      "  208693/1200000: episode: 834, duration: 2.139s, episode steps: 239, steps per second: 112, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 2.193596, mae: 25.054242, mean_q: -32.915852, mean_eps: 0.843570\n",
      "  208963/1200000: episode: 835, duration: 2.417s, episode steps: 270, steps per second: 112, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 2.655072, mae: 25.240865, mean_q: -33.019292, mean_eps: 0.843379\n",
      "  209381/1200000: episode: 836, duration: 3.674s, episode steps: 418, steps per second: 114, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 2.511127, mae: 25.230906, mean_q: -33.036956, mean_eps: 0.843121\n",
      "  209775/1200000: episode: 837, duration: 3.545s, episode steps: 394, steps per second: 111, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 2.345220, mae: 25.254826, mean_q: -33.074307, mean_eps: 0.842817\n",
      "  210087/1200000: episode: 838, duration: 2.810s, episode steps: 312, steps per second: 111, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 1.904801, mae: 25.079642, mean_q: -32.931231, mean_eps: 0.842552\n",
      "  210297/1200000: episode: 839, duration: 1.915s, episode steps: 210, steps per second: 110, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 1.840597, mae: 24.808910, mean_q: -32.560945, mean_eps: 0.842356\n",
      "  210707/1200000: episode: 840, duration: 3.680s, episode steps: 410, steps per second: 111, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 2.459893, mae: 24.843193, mean_q: -32.546218, mean_eps: 0.842124\n",
      "  210950/1200000: episode: 841, duration: 2.167s, episode steps: 243, steps per second: 112, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 2.635006, mae: 24.750333, mean_q: -32.421986, mean_eps: 0.841879\n",
      "  211124/1200000: episode: 842, duration: 1.565s, episode steps: 174, steps per second: 111, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 2.269471, mae: 24.876093, mean_q: -32.595662, mean_eps: 0.841723\n",
      "  211324/1200000: episode: 843, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 1.930168, mae: 24.763163, mean_q: -32.468365, mean_eps: 0.841582\n",
      "  211499/1200000: episode: 844, duration: 1.582s, episode steps: 175, steps per second: 111, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 2.093952, mae: 24.811425, mean_q: -32.520829, mean_eps: 0.841442\n",
      "  211728/1200000: episode: 845, duration: 2.059s, episode steps: 229, steps per second: 111, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 1.803619, mae: 24.901209, mean_q: -32.669330, mean_eps: 0.841290\n",
      "  211902/1200000: episode: 846, duration: 1.589s, episode steps: 174, steps per second: 110, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 2.031248, mae: 24.696865, mean_q: -32.347742, mean_eps: 0.841139\n",
      "  212214/1200000: episode: 847, duration: 2.794s, episode steps: 312, steps per second: 112, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 1.984815, mae: 24.889833, mean_q: -32.653058, mean_eps: 0.840957\n",
      "  212508/1200000: episode: 848, duration: 2.643s, episode steps: 294, steps per second: 111, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 2.044052, mae: 24.797621, mean_q: -32.499161, mean_eps: 0.840730\n",
      "  212802/1200000: episode: 849, duration: 2.641s, episode steps: 294, steps per second: 111, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 2.352048, mae: 24.785745, mean_q: -32.492040, mean_eps: 0.840509\n",
      "  213018/1200000: episode: 850, duration: 1.973s, episode steps: 216, steps per second: 109, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 2.442349, mae: 24.812880, mean_q: -32.538479, mean_eps: 0.840318\n",
      "  213457/1200000: episode: 851, duration: 3.911s, episode steps: 439, steps per second: 112, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 1.916424, mae: 24.834551, mean_q: -32.602750, mean_eps: 0.840072\n",
      "  213620/1200000: episode: 852, duration: 1.489s, episode steps: 163, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 2.270313, mae: 25.018783, mean_q: -32.816823, mean_eps: 0.839847\n",
      "  214091/1200000: episode: 853, duration: 4.215s, episode steps: 471, steps per second: 112, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 2.257989, mae: 24.915981, mean_q: -32.649777, mean_eps: 0.839609\n",
      "  214565/1200000: episode: 854, duration: 4.260s, episode steps: 474, steps per second: 111, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 2.251162, mae: 24.724069, mean_q: -32.390360, mean_eps: 0.839254\n",
      "  214882/1200000: episode: 855, duration: 2.885s, episode steps: 317, steps per second: 110, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 2.009134, mae: 24.716347, mean_q: -32.423121, mean_eps: 0.838958\n",
      "  215154/1200000: episode: 856, duration: 2.476s, episode steps: 272, steps per second: 110, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 2.122003, mae: 24.859374, mean_q: -32.590535, mean_eps: 0.838737\n",
      "  215336/1200000: episode: 857, duration: 1.671s, episode steps: 182, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 1.966227, mae: 24.936493, mean_q: -32.738906, mean_eps: 0.838567\n",
      "  215516/1200000: episode: 858, duration: 1.632s, episode steps: 180, steps per second: 110, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 2.184637, mae: 24.758210, mean_q: -32.443689, mean_eps: 0.838431\n",
      "  215687/1200000: episode: 859, duration: 1.576s, episode steps: 171, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 2.023884, mae: 24.921488, mean_q: -32.715754, mean_eps: 0.838299\n",
      "  215934/1200000: episode: 860, duration: 2.210s, episode steps: 247, steps per second: 112, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 2.276718, mae: 24.823793, mean_q: -32.554980, mean_eps: 0.838142\n",
      "  216261/1200000: episode: 861, duration: 2.987s, episode steps: 327, steps per second: 109, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 2.283517, mae: 24.717020, mean_q: -32.346417, mean_eps: 0.837927\n",
      "  216600/1200000: episode: 862, duration: 3.053s, episode steps: 339, steps per second: 111, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 1.988382, mae: 24.754467, mean_q: -32.526830, mean_eps: 0.837677\n",
      "  216777/1200000: episode: 863, duration: 1.648s, episode steps: 177, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 2.460793, mae: 24.991445, mean_q: -32.700840, mean_eps: 0.837484\n",
      "  216950/1200000: episode: 864, duration: 1.586s, episode steps: 173, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 2.769783, mae: 24.805166, mean_q: -32.488309, mean_eps: 0.837353\n",
      "  217138/1200000: episode: 865, duration: 1.730s, episode steps: 188, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 1.980638, mae: 24.910208, mean_q: -32.628582, mean_eps: 0.837217\n",
      "  217415/1200000: episode: 866, duration: 2.543s, episode steps: 277, steps per second: 109, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 2.003680, mae: 24.792211, mean_q: -32.529134, mean_eps: 0.837043\n",
      "  217698/1200000: episode: 867, duration: 2.705s, episode steps: 283, steps per second: 105, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 1.801162, mae: 24.803704, mean_q: -32.572551, mean_eps: 0.836833\n",
      "  217983/1200000: episode: 868, duration: 2.583s, episode steps: 285, steps per second: 110, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 2.225270, mae: 24.745248, mean_q: -32.412468, mean_eps: 0.836620\n",
      "  218261/1200000: episode: 869, duration: 2.566s, episode steps: 278, steps per second: 108, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 2.031601, mae: 24.834206, mean_q: -32.565612, mean_eps: 0.836409\n",
      "  218450/1200000: episode: 870, duration: 1.701s, episode steps: 189, steps per second: 111, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 2.073347, mae: 24.816629, mean_q: -32.508860, mean_eps: 0.836234\n",
      "  218637/1200000: episode: 871, duration: 1.725s, episode steps: 187, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 2.428529, mae: 24.879155, mean_q: -32.559804, mean_eps: 0.836093\n",
      "  219135/1200000: episode: 872, duration: 4.478s, episode steps: 498, steps per second: 111, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 2.255445, mae: 24.820376, mean_q: -32.522154, mean_eps: 0.835836\n",
      "  219367/1200000: episode: 873, duration: 2.135s, episode steps: 232, steps per second: 109, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 2.494507, mae: 24.737572, mean_q: -32.414572, mean_eps: 0.835562\n",
      "  219532/1200000: episode: 874, duration: 1.525s, episode steps: 165, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.479 [0.000, 3.000],  loss: 2.207140, mae: 24.923586, mean_q: -32.650725, mean_eps: 0.835413\n",
      "  219693/1200000: episode: 875, duration: 1.487s, episode steps: 161, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 2.171812, mae: 24.888324, mean_q: -32.632158, mean_eps: 0.835291\n",
      "  219981/1200000: episode: 876, duration: 2.618s, episode steps: 288, steps per second: 110, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 2.049997, mae: 24.829864, mean_q: -32.619312, mean_eps: 0.835123\n",
      "  220288/1200000: episode: 877, duration: 2.819s, episode steps: 307, steps per second: 109, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 1.848012, mae: 24.343649, mean_q: -31.995100, mean_eps: 0.834900\n",
      "  220659/1200000: episode: 878, duration: 3.381s, episode steps: 371, steps per second: 110, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 2.366571, mae: 24.213745, mean_q: -31.740362, mean_eps: 0.834645\n",
      "  220902/1200000: episode: 879, duration: 2.287s, episode steps: 243, steps per second: 106, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 2.313595, mae: 24.359911, mean_q: -31.971296, mean_eps: 0.834415\n",
      "  221183/1200000: episode: 880, duration: 2.626s, episode steps: 281, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 2.070137, mae: 24.346907, mean_q: -31.958489, mean_eps: 0.834218\n",
      "  221452/1200000: episode: 881, duration: 2.482s, episode steps: 269, steps per second: 108, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 2.163518, mae: 24.314968, mean_q: -31.897908, mean_eps: 0.834012\n",
      "  221630/1200000: episode: 882, duration: 1.673s, episode steps: 178, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 2.166249, mae: 24.243648, mean_q: -31.780289, mean_eps: 0.833845\n",
      "  221908/1200000: episode: 883, duration: 2.548s, episode steps: 278, steps per second: 109, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 2.162639, mae: 24.297301, mean_q: -31.872189, mean_eps: 0.833674\n",
      "  222081/1200000: episode: 884, duration: 1.600s, episode steps: 173, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 2.621901, mae: 24.302724, mean_q: -31.831174, mean_eps: 0.833504\n",
      "  222365/1200000: episode: 885, duration: 2.603s, episode steps: 284, steps per second: 109, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 2.135988, mae: 24.311665, mean_q: -31.885744, mean_eps: 0.833333\n",
      "  222602/1200000: episode: 886, duration: 2.171s, episode steps: 237, steps per second: 109, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 2.279606, mae: 24.321593, mean_q: -31.857983, mean_eps: 0.833138\n",
      "  222790/1200000: episode: 887, duration: 1.746s, episode steps: 188, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 1.863600, mae: 24.319874, mean_q: -31.998028, mean_eps: 0.832978\n",
      "  222964/1200000: episode: 888, duration: 1.616s, episode steps: 174, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 1.985155, mae: 24.443978, mean_q: -32.082195, mean_eps: 0.832843\n",
      "  223145/1200000: episode: 889, duration: 1.707s, episode steps: 181, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 2.316126, mae: 24.270540, mean_q: -31.781179, mean_eps: 0.832710\n",
      "  223356/1200000: episode: 890, duration: 1.951s, episode steps: 211, steps per second: 108, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: 1.679209, mae: 24.352416, mean_q: -31.992373, mean_eps: 0.832563\n",
      "  223571/1200000: episode: 891, duration: 1.992s, episode steps: 215, steps per second: 108, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.498 [0.000, 3.000],  loss: 2.243558, mae: 24.302933, mean_q: -31.884301, mean_eps: 0.832403\n",
      "  223764/1200000: episode: 892, duration: 1.799s, episode steps: 193, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 2.026156, mae: 24.234888, mean_q: -31.816149, mean_eps: 0.832250\n",
      "  224002/1200000: episode: 893, duration: 2.222s, episode steps: 238, steps per second: 107, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 1.923654, mae: 24.414067, mean_q: -32.031530, mean_eps: 0.832088\n",
      "  224357/1200000: episode: 894, duration: 3.391s, episode steps: 355, steps per second: 105, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 2.267138, mae: 24.262475, mean_q: -31.810416, mean_eps: 0.831866\n",
      "  224656/1200000: episode: 895, duration: 2.757s, episode steps: 299, steps per second: 108, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 2.074745, mae: 24.406124, mean_q: -31.995926, mean_eps: 0.831620\n",
      "  224917/1200000: episode: 896, duration: 2.411s, episode steps: 261, steps per second: 108, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 1.761893, mae: 24.345173, mean_q: -31.977790, mean_eps: 0.831411\n",
      "  225235/1200000: episode: 897, duration: 2.932s, episode steps: 318, steps per second: 108, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 2.314644, mae: 24.315615, mean_q: -31.893878, mean_eps: 0.831193\n",
      "  225474/1200000: episode: 898, duration: 2.224s, episode steps: 239, steps per second: 107, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 1.788607, mae: 24.352218, mean_q: -31.917682, mean_eps: 0.830985\n",
      "  225648/1200000: episode: 899, duration: 1.624s, episode steps: 174, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 2.442034, mae: 24.327719, mean_q: -31.878627, mean_eps: 0.830830\n",
      "  225913/1200000: episode: 900, duration: 2.444s, episode steps: 265, steps per second: 108, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 1.944345, mae: 24.308928, mean_q: -31.881154, mean_eps: 0.830665\n",
      "  226086/1200000: episode: 901, duration: 1.633s, episode steps: 173, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: 2.893554, mae: 24.452743, mean_q: -32.044904, mean_eps: 0.830501\n",
      "  226436/1200000: episode: 902, duration: 3.243s, episode steps: 350, steps per second: 108, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 1.958463, mae: 24.288667, mean_q: -31.871953, mean_eps: 0.830305\n",
      "  226667/1200000: episode: 903, duration: 2.152s, episode steps: 231, steps per second: 107, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 2.169450, mae: 24.208620, mean_q: -31.750522, mean_eps: 0.830087\n",
      "  226922/1200000: episode: 904, duration: 2.356s, episode steps: 255, steps per second: 108, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 1.761307, mae: 24.245226, mean_q: -31.865887, mean_eps: 0.829905\n",
      "  227103/1200000: episode: 905, duration: 1.667s, episode steps: 181, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 1.663789, mae: 24.413028, mean_q: -32.086410, mean_eps: 0.829741\n",
      "  227553/1200000: episode: 906, duration: 4.141s, episode steps: 450, steps per second: 109, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 2.348265, mae: 24.380797, mean_q: -31.946922, mean_eps: 0.829504\n",
      "  227746/1200000: episode: 907, duration: 1.841s, episode steps: 193, steps per second: 105, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 1.863374, mae: 24.328809, mean_q: -31.890157, mean_eps: 0.829263\n",
      "  228088/1200000: episode: 908, duration: 3.179s, episode steps: 342, steps per second: 108, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 1.581835, mae: 24.382193, mean_q: -32.011516, mean_eps: 0.829063\n",
      "  228264/1200000: episode: 909, duration: 1.640s, episode steps: 176, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 2.212133, mae: 24.250884, mean_q: -31.770567, mean_eps: 0.828868\n",
      "  228471/1200000: episode: 910, duration: 1.948s, episode steps: 207, steps per second: 106, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 1.904266, mae: 24.387153, mean_q: -31.996300, mean_eps: 0.828725\n",
      "  228857/1200000: episode: 911, duration: 3.545s, episode steps: 386, steps per second: 109, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 2.345226, mae: 24.377965, mean_q: -31.960646, mean_eps: 0.828502\n",
      "  229031/1200000: episode: 912, duration: 1.642s, episode steps: 174, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 1.609270, mae: 24.349634, mean_q: -31.936611, mean_eps: 0.828292\n",
      "  229343/1200000: episode: 913, duration: 2.901s, episode steps: 312, steps per second: 108, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 1.959186, mae: 24.345230, mean_q: -31.943891, mean_eps: 0.828110\n",
      "  229617/1200000: episode: 914, duration: 2.554s, episode steps: 274, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 1.664572, mae: 24.397111, mean_q: -32.059461, mean_eps: 0.827890\n",
      "  230077/1200000: episode: 915, duration: 4.248s, episode steps: 460, steps per second: 108, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 2.015044, mae: 24.187510, mean_q: -31.696945, mean_eps: 0.827615\n",
      "  230288/1200000: episode: 916, duration: 1.991s, episode steps: 211, steps per second: 106, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 1.633794, mae: 23.208361, mean_q: -30.433309, mean_eps: 0.827363\n",
      "  230677/1200000: episode: 917, duration: 3.618s, episode steps: 389, steps per second: 108, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 1.849959, mae: 23.148441, mean_q: -30.341621, mean_eps: 0.827138\n",
      "  230906/1200000: episode: 918, duration: 2.151s, episode steps: 229, steps per second: 106, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 1.808219, mae: 23.289669, mean_q: -30.570974, mean_eps: 0.826907\n",
      "  231087/1200000: episode: 919, duration: 1.702s, episode steps: 181, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 1.962088, mae: 23.287982, mean_q: -30.523716, mean_eps: 0.826753\n",
      "  231361/1200000: episode: 920, duration: 2.562s, episode steps: 274, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.361 [0.000, 3.000],  loss: 1.761517, mae: 23.161997, mean_q: -30.375166, mean_eps: 0.826582\n",
      "  231610/1200000: episode: 921, duration: 2.309s, episode steps: 249, steps per second: 108, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 1.859126, mae: 23.225047, mean_q: -30.446764, mean_eps: 0.826386\n",
      "  231785/1200000: episode: 922, duration: 1.634s, episode steps: 175, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 1.804358, mae: 23.267580, mean_q: -30.494630, mean_eps: 0.826227\n",
      "  232067/1200000: episode: 923, duration: 2.635s, episode steps: 282, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 1.697581, mae: 23.238800, mean_q: -30.468229, mean_eps: 0.826056\n",
      "  232273/1200000: episode: 924, duration: 1.948s, episode steps: 206, steps per second: 106, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 2.035909, mae: 23.163873, mean_q: -30.371307, mean_eps: 0.825873\n",
      "  232596/1200000: episode: 925, duration: 3.022s, episode steps: 323, steps per second: 107, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 1.717940, mae: 23.180055, mean_q: -30.425155, mean_eps: 0.825674\n",
      "  232874/1200000: episode: 926, duration: 2.594s, episode steps: 278, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 1.872255, mae: 23.242812, mean_q: -30.467786, mean_eps: 0.825449\n",
      "  233163/1200000: episode: 927, duration: 2.692s, episode steps: 289, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 2.083839, mae: 23.205653, mean_q: -30.425900, mean_eps: 0.825237\n",
      "  233463/1200000: episode: 928, duration: 2.808s, episode steps: 300, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 2.029442, mae: 23.274530, mean_q: -30.487148, mean_eps: 0.825016\n",
      "  233703/1200000: episode: 929, duration: 2.241s, episode steps: 240, steps per second: 107, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 2.205321, mae: 23.323237, mean_q: -30.567758, mean_eps: 0.824813\n",
      "  234040/1200000: episode: 930, duration: 3.170s, episode steps: 337, steps per second: 106, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 1.709342, mae: 23.251651, mean_q: -30.503150, mean_eps: 0.824597\n",
      "  234401/1200000: episode: 931, duration: 3.457s, episode steps: 361, steps per second: 104, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 2.323753, mae: 23.352433, mean_q: -30.575314, mean_eps: 0.824335\n",
      "  234669/1200000: episode: 932, duration: 2.531s, episode steps: 268, steps per second: 106, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 2.351047, mae: 23.326676, mean_q: -30.560725, mean_eps: 0.824099\n",
      "  234843/1200000: episode: 933, duration: 1.666s, episode steps: 174, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 2.129350, mae: 23.240708, mean_q: -30.448564, mean_eps: 0.823933\n",
      "  235071/1200000: episode: 934, duration: 2.157s, episode steps: 228, steps per second: 106, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 1.980401, mae: 23.286505, mean_q: -30.528976, mean_eps: 0.823783\n",
      "  235385/1200000: episode: 935, duration: 2.944s, episode steps: 314, steps per second: 107, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.647149, mae: 23.236337, mean_q: -30.494273, mean_eps: 0.823579\n",
      "  235602/1200000: episode: 936, duration: 2.068s, episode steps: 217, steps per second: 105, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 1.643743, mae: 23.311454, mean_q: -30.613742, mean_eps: 0.823380\n",
      "  235873/1200000: episode: 937, duration: 2.545s, episode steps: 271, steps per second: 106, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 1.862732, mae: 23.289464, mean_q: -30.528375, mean_eps: 0.823197\n",
      "  236202/1200000: episode: 938, duration: 3.102s, episode steps: 329, steps per second: 106, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 1.801175, mae: 23.267753, mean_q: -30.514203, mean_eps: 0.822972\n",
      "  236479/1200000: episode: 939, duration: 2.623s, episode steps: 277, steps per second: 106, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 2.045640, mae: 23.115227, mean_q: -30.249775, mean_eps: 0.822745\n",
      "  236758/1200000: episode: 940, duration: 2.615s, episode steps: 279, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 1.862077, mae: 23.237137, mean_q: -30.491790, mean_eps: 0.822537\n",
      "  237030/1200000: episode: 941, duration: 2.595s, episode steps: 272, steps per second: 105, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 1.783793, mae: 23.265981, mean_q: -30.498774, mean_eps: 0.822330\n",
      "  237207/1200000: episode: 942, duration: 1.670s, episode steps: 177, steps per second: 106, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 2.106819, mae: 23.226686, mean_q: -30.407273, mean_eps: 0.822161\n",
      "  237443/1200000: episode: 943, duration: 2.245s, episode steps: 236, steps per second: 105, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 1.981282, mae: 23.254554, mean_q: -30.407558, mean_eps: 0.822007\n",
      "  237712/1200000: episode: 944, duration: 2.557s, episode steps: 269, steps per second: 105, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 2.048845, mae: 23.183233, mean_q: -30.418388, mean_eps: 0.821817\n",
      "  237994/1200000: episode: 945, duration: 2.636s, episode steps: 282, steps per second: 107, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 1.406808, mae: 23.175582, mean_q: -30.463859, mean_eps: 0.821611\n",
      "  238270/1200000: episode: 946, duration: 2.642s, episode steps: 276, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 1.809013, mae: 23.237253, mean_q: -30.461339, mean_eps: 0.821401\n",
      "  238668/1200000: episode: 947, duration: 3.793s, episode steps: 398, steps per second: 105, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 1.814182, mae: 23.406206, mean_q: -30.688090, mean_eps: 0.821149\n",
      "  238842/1200000: episode: 948, duration: 1.678s, episode steps: 174, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 2.062946, mae: 23.090383, mean_q: -30.211513, mean_eps: 0.820934\n",
      "  239008/1200000: episode: 949, duration: 1.595s, episode steps: 166, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 2.095040, mae: 23.199290, mean_q: -30.415451, mean_eps: 0.820807\n",
      "  239284/1200000: episode: 950, duration: 2.638s, episode steps: 276, steps per second: 105, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 1.953317, mae: 23.326277, mean_q: -30.540348, mean_eps: 0.820641\n",
      "  239556/1200000: episode: 951, duration: 2.603s, episode steps: 272, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 1.755774, mae: 23.268903, mean_q: -30.559083, mean_eps: 0.820435\n",
      "  239827/1200000: episode: 952, duration: 2.606s, episode steps: 271, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 1.827482, mae: 23.293098, mean_q: -30.563955, mean_eps: 0.820232\n",
      "  240149/1200000: episode: 953, duration: 3.056s, episode steps: 322, steps per second: 105, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 2.145704, mae: 23.014329, mean_q: -30.187047, mean_eps: 0.820009\n",
      "  240316/1200000: episode: 954, duration: 1.608s, episode steps: 167, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 2.148519, mae: 22.734002, mean_q: -29.762672, mean_eps: 0.819826\n",
      "  240644/1200000: episode: 955, duration: 3.174s, episode steps: 328, steps per second: 103, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 1.824748, mae: 22.726292, mean_q: -29.776456, mean_eps: 0.819640\n",
      "  240871/1200000: episode: 956, duration: 2.185s, episode steps: 227, steps per second: 104, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 1.376421, mae: 22.761918, mean_q: -29.930744, mean_eps: 0.819432\n",
      "  241262/1200000: episode: 957, duration: 3.702s, episode steps: 391, steps per second: 106, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 1.889058, mae: 22.660733, mean_q: -29.659077, mean_eps: 0.819200\n",
      "  241525/1200000: episode: 958, duration: 2.552s, episode steps: 263, steps per second: 103, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 1.764581, mae: 22.746712, mean_q: -29.850050, mean_eps: 0.818955\n",
      "  241982/1200000: episode: 959, duration: 4.345s, episode steps: 457, steps per second: 105, episode reward:  8.000, mean reward:  0.018 [ 0.000,  4.000], mean action: 1.547 [0.000, 3.000],  loss: 1.752058, mae: 22.733363, mean_q: -29.836097, mean_eps: 0.818685\n",
      "  242163/1200000: episode: 960, duration: 1.758s, episode steps: 181, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 1.774430, mae: 22.637841, mean_q: -29.760044, mean_eps: 0.818446\n",
      "  242446/1200000: episode: 961, duration: 2.716s, episode steps: 283, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 1.420672, mae: 22.654473, mean_q: -29.743435, mean_eps: 0.818272\n",
      "  242739/1200000: episode: 962, duration: 2.797s, episode steps: 293, steps per second: 105, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 1.662668, mae: 22.662874, mean_q: -29.760177, mean_eps: 0.818056\n",
      "  243118/1200000: episode: 963, duration: 3.615s, episode steps: 379, steps per second: 105, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 1.796309, mae: 22.732175, mean_q: -29.784640, mean_eps: 0.817804\n",
      "  243286/1200000: episode: 964, duration: 1.646s, episode steps: 168, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 2.162773, mae: 22.650102, mean_q: -29.637644, mean_eps: 0.817599\n",
      "  243636/1200000: episode: 965, duration: 3.339s, episode steps: 350, steps per second: 105, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 1.848545, mae: 22.668968, mean_q: -29.754290, mean_eps: 0.817405\n",
      "  243913/1200000: episode: 966, duration: 2.657s, episode steps: 277, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 1.697314, mae: 22.788752, mean_q: -29.872141, mean_eps: 0.817169\n",
      "  244275/1200000: episode: 967, duration: 3.453s, episode steps: 362, steps per second: 105, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 1.641020, mae: 22.725046, mean_q: -29.815172, mean_eps: 0.816930\n",
      "  244528/1200000: episode: 968, duration: 2.446s, episode steps: 253, steps per second: 103, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 1.756165, mae: 22.856470, mean_q: -29.995169, mean_eps: 0.816699\n",
      "  244710/1200000: episode: 969, duration: 1.785s, episode steps: 182, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 1.704044, mae: 22.540863, mean_q: -29.576600, mean_eps: 0.816536\n",
      "  244982/1200000: episode: 970, duration: 2.614s, episode steps: 272, steps per second: 104, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 1.575302, mae: 22.761432, mean_q: -29.892563, mean_eps: 0.816366\n",
      "  245152/1200000: episode: 971, duration: 1.644s, episode steps: 170, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 1.694619, mae: 22.706336, mean_q: -29.820572, mean_eps: 0.816200\n",
      "  245402/1200000: episode: 972, duration: 2.418s, episode steps: 250, steps per second: 103, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 1.691097, mae: 22.640882, mean_q: -29.706486, mean_eps: 0.816043\n",
      "  245847/1200000: episode: 973, duration: 4.267s, episode steps: 445, steps per second: 104, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 1.659747, mae: 22.703679, mean_q: -29.775984, mean_eps: 0.815782\n",
      "  246108/1200000: episode: 974, duration: 2.522s, episode steps: 261, steps per second: 103, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 1.635075, mae: 22.617361, mean_q: -29.652730, mean_eps: 0.815517\n",
      "  246493/1200000: episode: 975, duration: 3.706s, episode steps: 385, steps per second: 104, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 1.727301, mae: 22.697953, mean_q: -29.763695, mean_eps: 0.815275\n",
      "  246660/1200000: episode: 976, duration: 1.619s, episode steps: 167, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 1.876356, mae: 22.709580, mean_q: -29.779457, mean_eps: 0.815068\n",
      "  247017/1200000: episode: 977, duration: 3.536s, episode steps: 357, steps per second: 101, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 1.705525, mae: 22.765317, mean_q: -29.892226, mean_eps: 0.814871\n",
      "  247296/1200000: episode: 978, duration: 2.710s, episode steps: 279, steps per second: 103, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 1.432831, mae: 22.693144, mean_q: -29.770490, mean_eps: 0.814633\n",
      "  247468/1200000: episode: 979, duration: 1.687s, episode steps: 172, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 1.863244, mae: 22.601711, mean_q: -29.663885, mean_eps: 0.814464\n",
      "  247817/1200000: episode: 980, duration: 3.358s, episode steps: 349, steps per second: 104, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 1.567393, mae: 22.752739, mean_q: -29.867552, mean_eps: 0.814269\n",
      "  248117/1200000: episode: 981, duration: 2.985s, episode steps: 300, steps per second: 101, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 1.652585, mae: 22.652783, mean_q: -29.707320, mean_eps: 0.814025\n",
      "  248529/1200000: episode: 982, duration: 4.060s, episode steps: 412, steps per second: 101, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 1.751705, mae: 22.744829, mean_q: -29.847619, mean_eps: 0.813758\n",
      "  248752/1200000: episode: 983, duration: 2.161s, episode steps: 223, steps per second: 103, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 1.975664, mae: 22.690209, mean_q: -29.751949, mean_eps: 0.813520\n",
      "  249134/1200000: episode: 984, duration: 3.701s, episode steps: 382, steps per second: 103, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 1.804314, mae: 22.786137, mean_q: -29.863029, mean_eps: 0.813293\n",
      "  249409/1200000: episode: 985, duration: 2.688s, episode steps: 275, steps per second: 102, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 1.816510, mae: 22.595127, mean_q: -29.624222, mean_eps: 0.813047\n",
      "  249642/1200000: episode: 986, duration: 2.308s, episode steps: 233, steps per second: 101, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 1.869853, mae: 22.669117, mean_q: -29.705510, mean_eps: 0.812856\n",
      "  249934/1200000: episode: 987, duration: 2.849s, episode steps: 292, steps per second: 102, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 1.621926, mae: 22.601616, mean_q: -29.640696, mean_eps: 0.812659\n",
      "  250220/1200000: episode: 988, duration: 2.771s, episode steps: 286, steps per second: 103, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 1.627267, mae: 22.305307, mean_q: -29.301055, mean_eps: 0.812443\n",
      "  250401/1200000: episode: 989, duration: 1.780s, episode steps: 181, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.751 [0.000, 3.000],  loss: 1.850793, mae: 22.155815, mean_q: -29.088228, mean_eps: 0.812268\n",
      "  250575/1200000: episode: 990, duration: 1.757s, episode steps: 174, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 2.168941, mae: 22.214443, mean_q: -29.088723, mean_eps: 0.812134\n",
      "  250886/1200000: episode: 991, duration: 3.033s, episode steps: 311, steps per second: 103, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 1.521604, mae: 22.238181, mean_q: -29.171346, mean_eps: 0.811952\n",
      "  251131/1200000: episode: 992, duration: 2.424s, episode steps: 245, steps per second: 101, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 1.857034, mae: 22.169822, mean_q: -29.097334, mean_eps: 0.811744\n",
      "  251313/1200000: episode: 993, duration: 1.796s, episode steps: 182, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 1.850865, mae: 22.327469, mean_q: -29.253320, mean_eps: 0.811584\n",
      "  251644/1200000: episode: 994, duration: 3.252s, episode steps: 331, steps per second: 102, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 1.532784, mae: 22.262632, mean_q: -29.239922, mean_eps: 0.811391\n",
      "  251821/1200000: episode: 995, duration: 1.722s, episode steps: 177, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 1.702971, mae: 22.258323, mean_q: -29.234355, mean_eps: 0.811201\n",
      "  251994/1200000: episode: 996, duration: 1.721s, episode steps: 173, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 1.954246, mae: 22.275373, mean_q: -29.179209, mean_eps: 0.811070\n",
      "  252301/1200000: episode: 997, duration: 3.000s, episode steps: 307, steps per second: 102, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 1.456619, mae: 22.252488, mean_q: -29.218661, mean_eps: 0.810890\n",
      "  252697/1200000: episode: 998, duration: 3.887s, episode steps: 396, steps per second: 102, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 1.744553, mae: 22.273579, mean_q: -29.224381, mean_eps: 0.810626\n",
      "  253116/1200000: episode: 999, duration: 4.140s, episode steps: 419, steps per second: 101, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 1.506968, mae: 22.266498, mean_q: -29.205642, mean_eps: 0.810320\n",
      "  253388/1200000: episode: 1000, duration: 2.674s, episode steps: 272, steps per second: 102, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 1.515060, mae: 22.320355, mean_q: -29.278880, mean_eps: 0.810061\n",
      "  253720/1200000: episode: 1001, duration: 3.264s, episode steps: 332, steps per second: 102, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 1.456067, mae: 22.257072, mean_q: -29.221419, mean_eps: 0.809835\n",
      "  253891/1200000: episode: 1002, duration: 1.716s, episode steps: 171, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 1.611761, mae: 22.454200, mean_q: -29.409918, mean_eps: 0.809646\n",
      "  254280/1200000: episode: 1003, duration: 3.797s, episode steps: 389, steps per second: 102, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 1.716587, mae: 22.263431, mean_q: -29.176930, mean_eps: 0.809436\n",
      "  254716/1200000: episode: 1004, duration: 4.227s, episode steps: 436, steps per second: 103, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 1.801618, mae: 22.309265, mean_q: -29.234961, mean_eps: 0.809127\n",
      "  254929/1200000: episode: 1005, duration: 2.098s, episode steps: 213, steps per second: 102, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 1.562139, mae: 22.276403, mean_q: -29.201782, mean_eps: 0.808884\n",
      "  255205/1200000: episode: 1006, duration: 2.739s, episode steps: 276, steps per second: 101, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 1.544925, mae: 22.182015, mean_q: -29.068348, mean_eps: 0.808700\n",
      "  255468/1200000: episode: 1007, duration: 2.597s, episode steps: 263, steps per second: 101, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 1.535360, mae: 22.228036, mean_q: -29.184652, mean_eps: 0.808498\n",
      "  255783/1200000: episode: 1008, duration: 3.096s, episode steps: 315, steps per second: 102, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 1.581121, mae: 22.301942, mean_q: -29.293958, mean_eps: 0.808281\n",
      "  256182/1200000: episode: 1009, duration: 3.931s, episode steps: 399, steps per second: 101, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 1.747430, mae: 22.262868, mean_q: -29.217741, mean_eps: 0.808014\n",
      "  256353/1200000: episode: 1010, duration: 1.726s, episode steps: 171, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: 1.904067, mae: 22.098871, mean_q: -28.937432, mean_eps: 0.807800\n",
      "  256525/1200000: episode: 1011, duration: 1.720s, episode steps: 172, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 1.598283, mae: 22.449119, mean_q: -29.445683, mean_eps: 0.807671\n",
      "  256819/1200000: episode: 1012, duration: 2.922s, episode steps: 294, steps per second: 101, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 1.651182, mae: 22.332333, mean_q: -29.259170, mean_eps: 0.807496\n",
      "  257106/1200000: episode: 1013, duration: 2.817s, episode steps: 287, steps per second: 102, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 1.567520, mae: 22.314269, mean_q: -29.262580, mean_eps: 0.807279\n",
      "  257299/1200000: episode: 1014, duration: 1.914s, episode steps: 193, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.694 [0.000, 3.000],  loss: 1.555115, mae: 22.210466, mean_q: -29.152455, mean_eps: 0.807099\n",
      "  257549/1200000: episode: 1015, duration: 2.522s, episode steps: 250, steps per second:  99, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 1.471234, mae: 22.276437, mean_q: -29.239781, mean_eps: 0.806932\n",
      "  257939/1200000: episode: 1016, duration: 3.848s, episode steps: 390, steps per second: 101, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 1.565186, mae: 22.274508, mean_q: -29.211935, mean_eps: 0.806692\n",
      "  258172/1200000: episode: 1017, duration: 2.326s, episode steps: 233, steps per second: 100, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 1.371251, mae: 22.199894, mean_q: -29.178935, mean_eps: 0.806459\n",
      "  258517/1200000: episode: 1018, duration: 3.389s, episode steps: 345, steps per second: 102, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 1.570327, mae: 22.277432, mean_q: -29.227039, mean_eps: 0.806242\n",
      "  258800/1200000: episode: 1019, duration: 2.794s, episode steps: 283, steps per second: 101, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 1.409439, mae: 22.203837, mean_q: -29.135494, mean_eps: 0.806006\n",
      "  259046/1200000: episode: 1020, duration: 2.504s, episode steps: 246, steps per second:  98, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 1.605695, mae: 22.262815, mean_q: -29.235729, mean_eps: 0.805808\n",
      "  259385/1200000: episode: 1021, duration: 3.387s, episode steps: 339, steps per second: 100, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 1.553437, mae: 22.276268, mean_q: -29.231072, mean_eps: 0.805589\n",
      "  259873/1200000: episode: 1022, duration: 4.808s, episode steps: 488, steps per second: 101, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 1.599993, mae: 22.311033, mean_q: -29.281134, mean_eps: 0.805279\n",
      "  260330/1200000: episode: 1023, duration: 4.501s, episode steps: 457, steps per second: 102, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 1.456314, mae: 22.159657, mean_q: -29.101429, mean_eps: 0.804924\n",
      "  260612/1200000: episode: 1024, duration: 2.796s, episode steps: 282, steps per second: 101, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 1.508294, mae: 22.250871, mean_q: -29.228915, mean_eps: 0.804647\n",
      "  260802/1200000: episode: 1025, duration: 1.926s, episode steps: 190, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 1.617014, mae: 22.158933, mean_q: -29.082911, mean_eps: 0.804470\n",
      "  261060/1200000: episode: 1026, duration: 2.626s, episode steps: 258, steps per second:  98, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 1.670308, mae: 22.155003, mean_q: -29.059889, mean_eps: 0.804302\n",
      "  261484/1200000: episode: 1027, duration: 4.259s, episode steps: 424, steps per second: 100, episode reward:  8.000, mean reward:  0.019 [ 0.000,  4.000], mean action: 1.502 [0.000, 3.000],  loss: 1.609421, mae: 22.177692, mean_q: -29.088754, mean_eps: 0.804046\n",
      "  261653/1200000: episode: 1028, duration: 1.710s, episode steps: 169, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.391 [0.000, 3.000],  loss: 1.551398, mae: 22.260984, mean_q: -29.238773, mean_eps: 0.803824\n",
      "  262131/1200000: episode: 1029, duration: 4.721s, episode steps: 478, steps per second: 101, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 1.643323, mae: 22.101557, mean_q: -28.975499, mean_eps: 0.803581\n",
      "  262476/1200000: episode: 1030, duration: 3.451s, episode steps: 345, steps per second: 100, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 1.830964, mae: 22.181092, mean_q: -29.045373, mean_eps: 0.803273\n",
      "  262719/1200000: episode: 1031, duration: 2.458s, episode steps: 243, steps per second:  99, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 1.351422, mae: 22.136782, mean_q: -29.052683, mean_eps: 0.803052\n",
      "  263076/1200000: episode: 1032, duration: 3.552s, episode steps: 357, steps per second: 100, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 1.723147, mae: 22.221999, mean_q: -29.146571, mean_eps: 0.802827\n",
      "  263332/1200000: episode: 1033, duration: 2.578s, episode steps: 256, steps per second:  99, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 1.449838, mae: 22.064503, mean_q: -28.966882, mean_eps: 0.802597\n",
      "  263580/1200000: episode: 1034, duration: 2.492s, episode steps: 248, steps per second: 100, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 1.473089, mae: 22.197159, mean_q: -29.144412, mean_eps: 0.802408\n",
      "  263952/1200000: episode: 1035, duration: 3.724s, episode steps: 372, steps per second: 100, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 1.713061, mae: 22.243237, mean_q: -29.165770, mean_eps: 0.802176\n",
      "  264124/1200000: episode: 1036, duration: 1.915s, episode steps: 172, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 1.491186, mae: 22.083226, mean_q: -28.943220, mean_eps: 0.801972\n",
      "  264293/1200000: episode: 1037, duration: 1.719s, episode steps: 169, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 2.048213, mae: 22.183045, mean_q: -29.008640, mean_eps: 0.801844\n",
      "  264482/1200000: episode: 1038, duration: 1.929s, episode steps: 189, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 1.899203, mae: 22.142213, mean_q: -29.008211, mean_eps: 0.801710\n",
      "  264859/1200000: episode: 1039, duration: 3.689s, episode steps: 377, steps per second: 102, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 1.278481, mae: 22.180294, mean_q: -29.135850, mean_eps: 0.801497\n",
      "  265142/1200000: episode: 1040, duration: 2.869s, episode steps: 283, steps per second:  99, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 1.392669, mae: 22.232535, mean_q: -29.202154, mean_eps: 0.801250\n",
      "  265327/1200000: episode: 1041, duration: 1.881s, episode steps: 185, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 1.691660, mae: 22.169567, mean_q: -29.098787, mean_eps: 0.801074\n",
      "  265516/1200000: episode: 1042, duration: 1.950s, episode steps: 189, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 1.853468, mae: 22.036923, mean_q: -28.876969, mean_eps: 0.800934\n",
      "  265752/1200000: episode: 1043, duration: 2.401s, episode steps: 236, steps per second:  98, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 1.737335, mae: 22.178866, mean_q: -29.095653, mean_eps: 0.800775\n",
      "  265974/1200000: episode: 1044, duration: 2.250s, episode steps: 222, steps per second:  99, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 1.697032, mae: 22.262099, mean_q: -29.190556, mean_eps: 0.800603\n",
      "  266249/1200000: episode: 1045, duration: 2.786s, episode steps: 275, steps per second:  99, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 1.940178, mae: 22.113048, mean_q: -28.964966, mean_eps: 0.800417\n",
      "  266537/1200000: episode: 1046, duration: 2.882s, episode steps: 288, steps per second: 100, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 1.694884, mae: 22.203841, mean_q: -29.138725, mean_eps: 0.800206\n",
      "  266709/1200000: episode: 1047, duration: 1.754s, episode steps: 172, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 1.842700, mae: 22.193364, mean_q: -29.081915, mean_eps: 0.800033\n",
      "  267042/1200000: episode: 1048, duration: 3.432s, episode steps: 333, steps per second:  97, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 1.465951, mae: 22.194067, mean_q: -29.146499, mean_eps: 0.799844\n",
      "  267216/1200000: episode: 1049, duration: 1.783s, episode steps: 174, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.770 [0.000, 3.000],  loss: 1.841350, mae: 22.223875, mean_q: -29.148561, mean_eps: 0.799654\n",
      "  267496/1200000: episode: 1050, duration: 2.833s, episode steps: 280, steps per second:  99, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 1.285681, mae: 22.112785, mean_q: -29.058836, mean_eps: 0.799483\n",
      "  267674/1200000: episode: 1051, duration: 1.810s, episode steps: 178, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 1.502827, mae: 21.924669, mean_q: -28.782261, mean_eps: 0.799312\n",
      "  267966/1200000: episode: 1052, duration: 2.962s, episode steps: 292, steps per second:  99, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 1.806945, mae: 22.167328, mean_q: -29.034406, mean_eps: 0.799135\n",
      "  268452/1200000: episode: 1053, duration: 4.871s, episode steps: 486, steps per second: 100, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 1.808052, mae: 22.122613, mean_q: -29.000856, mean_eps: 0.798844\n",
      "  268783/1200000: episode: 1054, duration: 3.350s, episode steps: 331, steps per second:  99, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 1.549934, mae: 22.213363, mean_q: -29.165887, mean_eps: 0.798537\n",
      "  268964/1200000: episode: 1055, duration: 1.879s, episode steps: 181, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 1.612341, mae: 22.245529, mean_q: -29.190972, mean_eps: 0.798345\n",
      "  269178/1200000: episode: 1056, duration: 2.233s, episode steps: 214, steps per second:  96, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 1.819755, mae: 22.230108, mean_q: -29.131269, mean_eps: 0.798197\n",
      "  269527/1200000: episode: 1057, duration: 3.550s, episode steps: 349, steps per second:  98, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 1.636958, mae: 22.186056, mean_q: -29.109811, mean_eps: 0.797986\n",
      "  269836/1200000: episode: 1058, duration: 3.178s, episode steps: 309, steps per second:  97, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 1.527737, mae: 22.199709, mean_q: -29.105925, mean_eps: 0.797739\n",
      "  270078/1200000: episode: 1059, duration: 2.576s, episode steps: 242, steps per second:  94, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 1.600837, mae: 22.304021, mean_q: -29.279895, mean_eps: 0.797533\n",
      "  270354/1200000: episode: 1060, duration: 2.819s, episode steps: 276, steps per second:  98, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 1.214359, mae: 22.453848, mean_q: -29.529419, mean_eps: 0.797338\n",
      "  270562/1200000: episode: 1061, duration: 2.140s, episode steps: 208, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 1.659021, mae: 22.503583, mean_q: -29.537030, mean_eps: 0.797157\n",
      "  270845/1200000: episode: 1062, duration: 2.913s, episode steps: 283, steps per second:  97, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 1.482123, mae: 22.401624, mean_q: -29.405485, mean_eps: 0.796973\n",
      "  271203/1200000: episode: 1063, duration: 3.742s, episode steps: 358, steps per second:  96, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 1.511402, mae: 22.438203, mean_q: -29.477212, mean_eps: 0.796732\n",
      "  271475/1200000: episode: 1064, duration: 2.785s, episode steps: 272, steps per second:  98, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 1.368587, mae: 22.513276, mean_q: -29.562401, mean_eps: 0.796496\n",
      "  271654/1200000: episode: 1065, duration: 1.864s, episode steps: 179, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 1.376656, mae: 22.396495, mean_q: -29.448821, mean_eps: 0.796327\n",
      "  271982/1200000: episode: 1066, duration: 3.332s, episode steps: 328, steps per second:  98, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 1.845720, mae: 22.366024, mean_q: -29.335350, mean_eps: 0.796137\n",
      "  272157/1200000: episode: 1067, duration: 1.855s, episode steps: 175, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: 2.093961, mae: 22.348882, mean_q: -29.246369, mean_eps: 0.795948\n",
      "  272394/1200000: episode: 1068, duration: 2.475s, episode steps: 237, steps per second:  96, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 1.227001, mae: 22.379413, mean_q: -29.419000, mean_eps: 0.795794\n",
      "  272573/1200000: episode: 1069, duration: 1.893s, episode steps: 179, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 1.341034, mae: 22.395725, mean_q: -29.435892, mean_eps: 0.795638\n",
      "  272912/1200000: episode: 1070, duration: 3.475s, episode steps: 339, steps per second:  98, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 1.583591, mae: 22.387766, mean_q: -29.396511, mean_eps: 0.795443\n",
      "  273238/1200000: episode: 1071, duration: 3.328s, episode steps: 326, steps per second:  98, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 1.373143, mae: 22.433105, mean_q: -29.479086, mean_eps: 0.795194\n",
      "  273447/1200000: episode: 1072, duration: 2.179s, episode steps: 209, steps per second:  96, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 1.805549, mae: 22.448196, mean_q: -29.433817, mean_eps: 0.794993\n",
      "  273624/1200000: episode: 1073, duration: 1.827s, episode steps: 177, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.288 [0.000, 3.000],  loss: 1.299525, mae: 22.290591, mean_q: -29.312268, mean_eps: 0.794849\n",
      "  273800/1200000: episode: 1074, duration: 1.850s, episode steps: 176, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 1.780085, mae: 22.288030, mean_q: -29.240159, mean_eps: 0.794716\n",
      "  273983/1200000: episode: 1075, duration: 1.880s, episode steps: 183, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 1.576232, mae: 22.362245, mean_q: -29.354660, mean_eps: 0.794582\n",
      "  274220/1200000: episode: 1076, duration: 2.443s, episode steps: 237, steps per second:  97, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 1.632591, mae: 22.443887, mean_q: -29.444685, mean_eps: 0.794424\n",
      "  274497/1200000: episode: 1077, duration: 2.881s, episode steps: 277, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 1.506701, mae: 22.466661, mean_q: -29.517204, mean_eps: 0.794231\n",
      "  274808/1200000: episode: 1078, duration: 3.180s, episode steps: 311, steps per second:  98, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 1.617939, mae: 22.333649, mean_q: -29.293373, mean_eps: 0.794011\n",
      "  274981/1200000: episode: 1079, duration: 1.795s, episode steps: 173, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 1.598670, mae: 22.241275, mean_q: -29.226082, mean_eps: 0.793830\n",
      "  275161/1200000: episode: 1080, duration: 1.870s, episode steps: 180, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 1.599492, mae: 22.344674, mean_q: -29.358505, mean_eps: 0.793697\n",
      "  275432/1200000: episode: 1081, duration: 2.792s, episode steps: 271, steps per second:  97, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 1.698376, mae: 22.421024, mean_q: -29.392219, mean_eps: 0.793528\n",
      "  275617/1200000: episode: 1082, duration: 1.950s, episode steps: 185, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 1.655122, mae: 22.457980, mean_q: -29.473242, mean_eps: 0.793357\n",
      "  275791/1200000: episode: 1083, duration: 1.826s, episode steps: 174, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 1.464455, mae: 22.430222, mean_q: -29.455475, mean_eps: 0.793222\n",
      "  275959/1200000: episode: 1084, duration: 1.767s, episode steps: 168, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 1.417932, mae: 22.374522, mean_q: -29.368471, mean_eps: 0.793094\n",
      "  276298/1200000: episode: 1085, duration: 3.466s, episode steps: 339, steps per second:  98, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 1.817674, mae: 22.324642, mean_q: -29.288712, mean_eps: 0.792904\n",
      "  276609/1200000: episode: 1086, duration: 3.207s, episode steps: 311, steps per second:  97, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 1.613388, mae: 22.278517, mean_q: -29.220014, mean_eps: 0.792660\n",
      "  276786/1200000: episode: 1087, duration: 1.858s, episode steps: 177, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 1.617289, mae: 22.204210, mean_q: -29.167803, mean_eps: 0.792477\n",
      "  276972/1200000: episode: 1088, duration: 1.968s, episode steps: 186, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 1.228688, mae: 22.300343, mean_q: -29.290606, mean_eps: 0.792341\n",
      "  277192/1200000: episode: 1089, duration: 2.312s, episode steps: 220, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 1.405488, mae: 22.338977, mean_q: -29.307860, mean_eps: 0.792189\n",
      "  277387/1200000: episode: 1090, duration: 2.032s, episode steps: 195, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.410 [0.000, 3.000],  loss: 1.764529, mae: 22.491738, mean_q: -29.518850, mean_eps: 0.792033\n",
      "  277657/1200000: episode: 1091, duration: 2.819s, episode steps: 270, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 1.616364, mae: 22.379745, mean_q: -29.401847, mean_eps: 0.791859\n",
      "  277891/1200000: episode: 1092, duration: 2.447s, episode steps: 234, steps per second:  96, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 1.541252, mae: 22.422336, mean_q: -29.418919, mean_eps: 0.791670\n",
      "  278192/1200000: episode: 1093, duration: 3.113s, episode steps: 301, steps per second:  97, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 1.809351, mae: 22.403942, mean_q: -29.371145, mean_eps: 0.791469\n",
      "  278364/1200000: episode: 1094, duration: 1.790s, episode steps: 172, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 1.524963, mae: 22.403671, mean_q: -29.415209, mean_eps: 0.791292\n",
      "  278593/1200000: episode: 1095, duration: 2.377s, episode steps: 229, steps per second:  96, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 1.654406, mae: 22.453976, mean_q: -29.461783, mean_eps: 0.791141\n",
      "  278758/1200000: episode: 1096, duration: 1.739s, episode steps: 165, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 1.626434, mae: 22.371047, mean_q: -29.343360, mean_eps: 0.790994\n",
      "  278997/1200000: episode: 1097, duration: 2.534s, episode steps: 239, steps per second:  94, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 1.664315, mae: 22.382489, mean_q: -29.388134, mean_eps: 0.790842\n",
      "  279180/1200000: episode: 1098, duration: 1.928s, episode steps: 183, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 1.650246, mae: 22.270094, mean_q: -29.227660, mean_eps: 0.790684\n",
      "  279427/1200000: episode: 1099, duration: 2.587s, episode steps: 247, steps per second:  95, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 1.619967, mae: 22.286203, mean_q: -29.269017, mean_eps: 0.790523\n",
      "  279864/1200000: episode: 1100, duration: 4.504s, episode steps: 437, steps per second:  97, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 1.526123, mae: 22.339570, mean_q: -29.339019, mean_eps: 0.790266\n",
      "  280353/1200000: episode: 1101, duration: 5.058s, episode steps: 489, steps per second:  97, episode reward:  8.000, mean reward:  0.016 [ 0.000,  4.000], mean action: 1.519 [0.000, 3.000],  loss: 1.319807, mae: 21.896575, mean_q: -28.770188, mean_eps: 0.789919\n",
      "  280530/1200000: episode: 1102, duration: 1.875s, episode steps: 177, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 1.781134, mae: 21.787818, mean_q: -28.569830, mean_eps: 0.789669\n",
      "  280715/1200000: episode: 1103, duration: 1.952s, episode steps: 185, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 1.327478, mae: 21.674207, mean_q: -28.469279, mean_eps: 0.789533\n",
      "  281034/1200000: episode: 1104, duration: 3.302s, episode steps: 319, steps per second:  97, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 1.501417, mae: 21.595606, mean_q: -28.347548, mean_eps: 0.789344\n",
      "  281316/1200000: episode: 1105, duration: 2.952s, episode steps: 282, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 1.680459, mae: 21.636188, mean_q: -28.416122, mean_eps: 0.789119\n",
      "  281594/1200000: episode: 1106, duration: 2.899s, episode steps: 278, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 1.444897, mae: 21.857017, mean_q: -28.704766, mean_eps: 0.788909\n",
      "  281825/1200000: episode: 1107, duration: 2.475s, episode steps: 231, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 1.659260, mae: 21.658490, mean_q: -28.405401, mean_eps: 0.788718\n",
      "  282067/1200000: episode: 1108, duration: 2.511s, episode steps: 242, steps per second:  96, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 1.439399, mae: 21.798791, mean_q: -28.607072, mean_eps: 0.788541\n",
      "  282315/1200000: episode: 1109, duration: 2.587s, episode steps: 248, steps per second:  96, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 1.664860, mae: 21.741912, mean_q: -28.507064, mean_eps: 0.788357\n",
      "  282496/1200000: episode: 1110, duration: 1.897s, episode steps: 181, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 1.599839, mae: 21.742322, mean_q: -28.584469, mean_eps: 0.788196\n",
      "  282777/1200000: episode: 1111, duration: 2.988s, episode steps: 281, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 1.998002, mae: 21.674484, mean_q: -28.412752, mean_eps: 0.788023\n",
      "  282948/1200000: episode: 1112, duration: 1.828s, episode steps: 171, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 1.336015, mae: 21.864998, mean_q: -28.696380, mean_eps: 0.787853\n",
      "  283326/1200000: episode: 1113, duration: 3.933s, episode steps: 378, steps per second:  96, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.393231, mae: 21.744721, mean_q: -28.560169, mean_eps: 0.787648\n",
      "  283607/1200000: episode: 1114, duration: 2.996s, episode steps: 281, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 1.717728, mae: 21.807810, mean_q: -28.599733, mean_eps: 0.787401\n",
      "  283944/1200000: episode: 1115, duration: 3.527s, episode steps: 337, steps per second:  96, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 1.593781, mae: 21.699375, mean_q: -28.489237, mean_eps: 0.787169\n",
      "  284234/1200000: episode: 1116, duration: 3.020s, episode steps: 290, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 1.479991, mae: 21.598238, mean_q: -28.352983, mean_eps: 0.786934\n",
      "  284404/1200000: episode: 1117, duration: 1.822s, episode steps: 170, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 1.457420, mae: 21.828345, mean_q: -28.690209, mean_eps: 0.786761\n",
      "  284645/1200000: episode: 1118, duration: 2.582s, episode steps: 241, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 1.418689, mae: 21.694944, mean_q: -28.501314, mean_eps: 0.786607\n",
      "  284811/1200000: episode: 1119, duration: 1.775s, episode steps: 166, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 1.422128, mae: 21.797082, mean_q: -28.609999, mean_eps: 0.786454\n",
      "  285086/1200000: episode: 1120, duration: 2.932s, episode steps: 275, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 1.442324, mae: 21.589206, mean_q: -28.341338, mean_eps: 0.786289\n",
      "  285357/1200000: episode: 1121, duration: 2.833s, episode steps: 271, steps per second:  96, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 1.762351, mae: 21.779197, mean_q: -28.531351, mean_eps: 0.786084\n",
      "  285633/1200000: episode: 1122, duration: 2.902s, episode steps: 276, steps per second:  95, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 1.613639, mae: 21.754746, mean_q: -28.569425, mean_eps: 0.785879\n",
      "  285818/1200000: episode: 1123, duration: 1.939s, episode steps: 185, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 1.385655, mae: 21.754184, mean_q: -28.575404, mean_eps: 0.785706\n",
      "  286155/1200000: episode: 1124, duration: 3.577s, episode steps: 337, steps per second:  94, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 1.424441, mae: 21.682954, mean_q: -28.491326, mean_eps: 0.785511\n",
      "  286360/1200000: episode: 1125, duration: 2.169s, episode steps: 205, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 1.598251, mae: 21.703418, mean_q: -28.483548, mean_eps: 0.785307\n",
      "  286621/1200000: episode: 1126, duration: 2.749s, episode steps: 261, steps per second:  95, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 1.558210, mae: 21.709921, mean_q: -28.500545, mean_eps: 0.785133\n",
      "  286896/1200000: episode: 1127, duration: 2.891s, episode steps: 275, steps per second:  95, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 1.589927, mae: 21.687661, mean_q: -28.477017, mean_eps: 0.784931\n",
      "  287087/1200000: episode: 1128, duration: 2.058s, episode steps: 191, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 1.651751, mae: 21.732426, mean_q: -28.498845, mean_eps: 0.784757\n",
      "  287254/1200000: episode: 1129, duration: 1.795s, episode steps: 167, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 1.403060, mae: 21.540747, mean_q: -28.233792, mean_eps: 0.784623\n",
      "  287446/1200000: episode: 1130, duration: 2.021s, episode steps: 192, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 1.709227, mae: 21.779599, mean_q: -28.541677, mean_eps: 0.784488\n",
      "  287662/1200000: episode: 1131, duration: 2.300s, episode steps: 216, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.627507, mae: 21.592973, mean_q: -28.295656, mean_eps: 0.784335\n",
      "  287872/1200000: episode: 1132, duration: 2.230s, episode steps: 210, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 1.447381, mae: 21.769150, mean_q: -28.592359, mean_eps: 0.784175\n",
      "  288205/1200000: episode: 1133, duration: 3.559s, episode steps: 333, steps per second:  94, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 1.249314, mae: 21.675063, mean_q: -28.475607, mean_eps: 0.783972\n",
      "  288475/1200000: episode: 1134, duration: 2.906s, episode steps: 270, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 1.572198, mae: 21.733128, mean_q: -28.533269, mean_eps: 0.783745\n",
      "  288650/1200000: episode: 1135, duration: 1.899s, episode steps: 175, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 1.799662, mae: 21.763838, mean_q: -28.543153, mean_eps: 0.783579\n",
      "  288829/1200000: episode: 1136, duration: 1.909s, episode steps: 179, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: 1.564196, mae: 21.785957, mean_q: -28.594821, mean_eps: 0.783446\n",
      "  289063/1200000: episode: 1137, duration: 2.513s, episode steps: 234, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.321 [0.000, 3.000],  loss: 1.630652, mae: 21.794697, mean_q: -28.600469, mean_eps: 0.783291\n",
      "  289237/1200000: episode: 1138, duration: 1.876s, episode steps: 174, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 1.673984, mae: 21.804577, mean_q: -28.541451, mean_eps: 0.783138\n",
      "  289476/1200000: episode: 1139, duration: 2.629s, episode steps: 239, steps per second:  91, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 1.553749, mae: 21.712632, mean_q: -28.489551, mean_eps: 0.782983\n",
      "  289696/1200000: episode: 1140, duration: 2.344s, episode steps: 220, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 1.525115, mae: 21.689729, mean_q: -28.473233, mean_eps: 0.782811\n",
      "  289876/1200000: episode: 1141, duration: 1.944s, episode steps: 180, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 1.721355, mae: 21.741300, mean_q: -28.529987, mean_eps: 0.782661\n",
      "  290076/1200000: episode: 1142, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 1.464422, mae: 21.362488, mean_q: -28.035938, mean_eps: 0.782518\n",
      "  290442/1200000: episode: 1143, duration: 3.885s, episode steps: 366, steps per second:  94, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 1.185299, mae: 21.323462, mean_q: -28.065660, mean_eps: 0.782306\n",
      "  290625/1200000: episode: 1144, duration: 1.971s, episode steps: 183, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 1.124565, mae: 21.300456, mean_q: -28.010969, mean_eps: 0.782100\n",
      "  290862/1200000: episode: 1145, duration: 2.509s, episode steps: 237, steps per second:  94, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 1.392145, mae: 21.282270, mean_q: -27.941055, mean_eps: 0.781943\n",
      "  291066/1200000: episode: 1146, duration: 2.163s, episode steps: 204, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.510 [0.000, 3.000],  loss: 1.424888, mae: 21.243903, mean_q: -27.872624, mean_eps: 0.781777\n",
      "  291294/1200000: episode: 1147, duration: 2.461s, episode steps: 228, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 1.343027, mae: 21.287053, mean_q: -27.954137, mean_eps: 0.781615\n",
      "  291576/1200000: episode: 1148, duration: 2.988s, episode steps: 282, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 1.382705, mae: 21.278143, mean_q: -27.944133, mean_eps: 0.781424\n",
      "  291811/1200000: episode: 1149, duration: 2.518s, episode steps: 235, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 1.477095, mae: 21.146041, mean_q: -27.786627, mean_eps: 0.781230\n",
      "  292012/1200000: episode: 1150, duration: 2.142s, episode steps: 201, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 1.727488, mae: 21.196286, mean_q: -27.797651, mean_eps: 0.781067\n",
      "  292186/1200000: episode: 1151, duration: 1.875s, episode steps: 174, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 1.170892, mae: 21.125924, mean_q: -27.782053, mean_eps: 0.780926\n",
      "  292486/1200000: episode: 1152, duration: 3.207s, episode steps: 300, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 1.470165, mae: 21.251444, mean_q: -27.924000, mean_eps: 0.780748\n",
      "  292708/1200000: episode: 1153, duration: 2.379s, episode steps: 222, steps per second:  93, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 1.340876, mae: 21.271483, mean_q: -27.945045, mean_eps: 0.780553\n",
      "  292991/1200000: episode: 1154, duration: 3.002s, episode steps: 283, steps per second:  94, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 1.354810, mae: 21.314884, mean_q: -27.967960, mean_eps: 0.780363\n",
      "  293247/1200000: episode: 1155, duration: 2.714s, episode steps: 256, steps per second:  94, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 1.278260, mae: 21.247053, mean_q: -27.887632, mean_eps: 0.780161\n",
      "  293427/1200000: episode: 1156, duration: 1.938s, episode steps: 180, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 1.568961, mae: 21.095625, mean_q: -27.652291, mean_eps: 0.779998\n",
      "  293640/1200000: episode: 1157, duration: 2.309s, episode steps: 213, steps per second:  92, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.441895, mae: 21.244922, mean_q: -27.847757, mean_eps: 0.779850\n",
      "  293923/1200000: episode: 1158, duration: 3.057s, episode steps: 283, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 1.469016, mae: 21.245010, mean_q: -27.891769, mean_eps: 0.779664\n",
      "  294153/1200000: episode: 1159, duration: 2.476s, episode steps: 230, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 1.435509, mae: 21.272121, mean_q: -27.921487, mean_eps: 0.779472\n",
      "  294401/1200000: episode: 1160, duration: 2.652s, episode steps: 248, steps per second:  93, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 1.404010, mae: 21.334573, mean_q: -28.017723, mean_eps: 0.779293\n",
      "  294577/1200000: episode: 1161, duration: 1.911s, episode steps: 176, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 1.427584, mae: 21.128117, mean_q: -27.762032, mean_eps: 0.779134\n",
      "  294845/1200000: episode: 1162, duration: 2.897s, episode steps: 268, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 1.380505, mae: 21.160561, mean_q: -27.772554, mean_eps: 0.778967\n",
      "  295181/1200000: episode: 1163, duration: 3.587s, episode steps: 336, steps per second:  94, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 1.345185, mae: 21.328457, mean_q: -28.006332, mean_eps: 0.778741\n",
      "  295366/1200000: episode: 1164, duration: 2.001s, episode steps: 185, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 1.328913, mae: 21.167613, mean_q: -27.803764, mean_eps: 0.778545\n",
      "  295601/1200000: episode: 1165, duration: 2.548s, episode steps: 235, steps per second:  92, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 1.263374, mae: 21.134066, mean_q: -27.795457, mean_eps: 0.778388\n",
      "  295771/1200000: episode: 1166, duration: 1.852s, episode steps: 170, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 1.445418, mae: 21.068987, mean_q: -27.658239, mean_eps: 0.778236\n",
      "  295941/1200000: episode: 1167, duration: 1.888s, episode steps: 170, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 1.378846, mae: 21.192258, mean_q: -27.851347, mean_eps: 0.778108\n",
      "  296112/1200000: episode: 1168, duration: 1.901s, episode steps: 171, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 1.501790, mae: 21.153489, mean_q: -27.745407, mean_eps: 0.777981\n",
      "  296286/1200000: episode: 1169, duration: 1.876s, episode steps: 174, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.523 [0.000, 3.000],  loss: 1.338115, mae: 21.339234, mean_q: -27.980186, mean_eps: 0.777851\n",
      "  296458/1200000: episode: 1170, duration: 1.870s, episode steps: 172, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 1.414626, mae: 21.226179, mean_q: -27.854126, mean_eps: 0.777721\n",
      "  296635/1200000: episode: 1171, duration: 1.964s, episode steps: 177, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 1.485100, mae: 21.316610, mean_q: -27.992395, mean_eps: 0.777590\n",
      "  296874/1200000: episode: 1172, duration: 2.584s, episode steps: 239, steps per second:  92, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: 1.427945, mae: 21.296891, mean_q: -27.951977, mean_eps: 0.777435\n",
      "  297149/1200000: episode: 1173, duration: 2.968s, episode steps: 275, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 1.458155, mae: 21.165289, mean_q: -27.783823, mean_eps: 0.777242\n",
      "  297444/1200000: episode: 1174, duration: 3.179s, episode steps: 295, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 1.392483, mae: 21.121579, mean_q: -27.706646, mean_eps: 0.777028\n",
      "  297672/1200000: episode: 1175, duration: 2.524s, episode steps: 228, steps per second:  90, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 1.287559, mae: 21.199061, mean_q: -27.873142, mean_eps: 0.776832\n",
      "  298129/1200000: episode: 1176, duration: 4.977s, episode steps: 457, steps per second:  92, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 1.301991, mae: 21.186321, mean_q: -27.814873, mean_eps: 0.776575\n",
      "  298304/1200000: episode: 1177, duration: 1.893s, episode steps: 175, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 1.284569, mae: 21.314039, mean_q: -27.985012, mean_eps: 0.776338\n",
      "  298474/1200000: episode: 1178, duration: 1.846s, episode steps: 170, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 1.483233, mae: 21.029671, mean_q: -27.586952, mean_eps: 0.776209\n",
      "  298660/1200000: episode: 1179, duration: 2.048s, episode steps: 186, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 1.692705, mae: 21.178830, mean_q: -27.787884, mean_eps: 0.776075\n",
      "  298967/1200000: episode: 1180, duration: 3.307s, episode steps: 307, steps per second:  93, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 1.383749, mae: 21.232479, mean_q: -27.895396, mean_eps: 0.775890\n",
      "  299427/1200000: episode: 1181, duration: 5.000s, episode steps: 460, steps per second:  92, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 1.402767, mae: 21.231739, mean_q: -27.866521, mean_eps: 0.775603\n",
      "  299662/1200000: episode: 1182, duration: 2.585s, episode steps: 235, steps per second:  91, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 1.516747, mae: 21.233724, mean_q: -27.896416, mean_eps: 0.775342\n",
      "  300056/1200000: episode: 1183, duration: 4.275s, episode steps: 394, steps per second:  92, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 1.366300, mae: 21.203873, mean_q: -27.872491, mean_eps: 0.775106\n",
      "  300232/1200000: episode: 1184, duration: 1.959s, episode steps: 176, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 1.330557, mae: 20.985448, mean_q: -27.550997, mean_eps: 0.774892\n",
      "  300400/1200000: episode: 1185, duration: 1.826s, episode steps: 168, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 1.246853, mae: 20.852119, mean_q: -27.390692, mean_eps: 0.774763\n",
      "  300747/1200000: episode: 1186, duration: 3.821s, episode steps: 347, steps per second:  91, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 1.483190, mae: 21.049518, mean_q: -27.635377, mean_eps: 0.774570\n",
      "  301119/1200000: episode: 1187, duration: 4.042s, episode steps: 372, steps per second:  92, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 1.328836, mae: 21.048732, mean_q: -27.634943, mean_eps: 0.774301\n",
      "  301411/1200000: episode: 1188, duration: 3.160s, episode steps: 292, steps per second:  92, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 1.513513, mae: 21.136395, mean_q: -27.744213, mean_eps: 0.774052\n",
      "  301730/1200000: episode: 1189, duration: 3.464s, episode steps: 319, steps per second:  92, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 1.579868, mae: 21.030140, mean_q: -27.586351, mean_eps: 0.773822\n",
      "  301897/1200000: episode: 1190, duration: 1.839s, episode steps: 167, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 1.006400, mae: 20.901206, mean_q: -27.460701, mean_eps: 0.773640\n",
      "  302137/1200000: episode: 1191, duration: 2.677s, episode steps: 240, steps per second:  90, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 1.021486, mae: 21.026369, mean_q: -27.614633, mean_eps: 0.773488\n",
      "  302321/1200000: episode: 1192, duration: 2.017s, episode steps: 184, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 1.511380, mae: 21.063720, mean_q: -27.647261, mean_eps: 0.773329\n",
      "  302494/1200000: episode: 1193, duration: 1.899s, episode steps: 173, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 1.201658, mae: 20.972837, mean_q: -27.576258, mean_eps: 0.773195\n",
      "  302768/1200000: episode: 1194, duration: 3.055s, episode steps: 274, steps per second:  90, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 1.257868, mae: 20.952997, mean_q: -27.505224, mean_eps: 0.773027\n",
      "  302994/1200000: episode: 1195, duration: 2.492s, episode steps: 226, steps per second:  91, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 1.433725, mae: 21.081471, mean_q: -27.687032, mean_eps: 0.772840\n",
      "  303226/1200000: episode: 1196, duration: 2.559s, episode steps: 232, steps per second:  91, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 1.357245, mae: 21.062766, mean_q: -27.676924, mean_eps: 0.772668\n",
      "  303649/1200000: episode: 1197, duration: 4.623s, episode steps: 423, steps per second:  91, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 1.109439, mae: 20.918800, mean_q: -27.510750, mean_eps: 0.772422\n",
      "  303831/1200000: episode: 1198, duration: 2.013s, episode steps: 182, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 1.365748, mae: 20.901567, mean_q: -27.451602, mean_eps: 0.772195\n",
      "  304103/1200000: episode: 1199, duration: 3.029s, episode steps: 272, steps per second:  90, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 1.529650, mae: 20.968784, mean_q: -27.521501, mean_eps: 0.772025\n",
      "  304275/1200000: episode: 1200, duration: 1.885s, episode steps: 172, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 1.470507, mae: 21.075388, mean_q: -27.637496, mean_eps: 0.771859\n",
      "  304456/1200000: episode: 1201, duration: 2.034s, episode steps: 181, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 1.476258, mae: 20.959475, mean_q: -27.495737, mean_eps: 0.771726\n",
      "  304831/1200000: episode: 1202, duration: 4.114s, episode steps: 375, steps per second:  91, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 1.118305, mae: 21.046219, mean_q: -27.665582, mean_eps: 0.771518\n",
      "  305004/1200000: episode: 1203, duration: 1.987s, episode steps: 173, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 1.243735, mae: 21.055342, mean_q: -27.680762, mean_eps: 0.771312\n",
      "  305242/1200000: episode: 1204, duration: 2.663s, episode steps: 238, steps per second:  89, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 1.400952, mae: 21.134338, mean_q: -27.736630, mean_eps: 0.771158\n",
      "  305463/1200000: episode: 1205, duration: 2.444s, episode steps: 221, steps per second:  90, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 1.307646, mae: 20.993163, mean_q: -27.587787, mean_eps: 0.770986\n",
      "  305649/1200000: episode: 1206, duration: 2.081s, episode steps: 186, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 1.218551, mae: 21.010782, mean_q: -27.582798, mean_eps: 0.770833\n",
      "  306022/1200000: episode: 1207, duration: 4.190s, episode steps: 373, steps per second:  89, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 1.336304, mae: 20.936974, mean_q: -27.468990, mean_eps: 0.770624\n",
      "  306255/1200000: episode: 1208, duration: 2.655s, episode steps: 233, steps per second:  88, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 1.426359, mae: 20.947097, mean_q: -27.486892, mean_eps: 0.770397\n",
      "  306486/1200000: episode: 1209, duration: 2.517s, episode steps: 231, steps per second:  92, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 1.304448, mae: 20.907591, mean_q: -27.433478, mean_eps: 0.770223\n",
      "  306718/1200000: episode: 1210, duration: 2.593s, episode steps: 232, steps per second:  89, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 1.172883, mae: 21.034387, mean_q: -27.632235, mean_eps: 0.770049\n",
      "  307084/1200000: episode: 1211, duration: 3.997s, episode steps: 366, steps per second:  92, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 1.307557, mae: 21.004734, mean_q: -27.555390, mean_eps: 0.769825\n",
      "  307423/1200000: episode: 1212, duration: 3.710s, episode steps: 339, steps per second:  91, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 1.501033, mae: 21.002317, mean_q: -27.547246, mean_eps: 0.769560\n",
      "  307599/1200000: episode: 1213, duration: 1.975s, episode steps: 176, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.540 [0.000, 3.000],  loss: 1.278529, mae: 21.035396, mean_q: -27.614806, mean_eps: 0.769367\n",
      "  307788/1200000: episode: 1214, duration: 2.095s, episode steps: 189, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 1.217639, mae: 20.925431, mean_q: -27.493483, mean_eps: 0.769230\n",
      "  307959/1200000: episode: 1215, duration: 1.932s, episode steps: 171, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 1.332196, mae: 20.988943, mean_q: -27.588439, mean_eps: 0.769095\n",
      "  308303/1200000: episode: 1216, duration: 3.913s, episode steps: 344, steps per second:  88, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.231569, mae: 20.924197, mean_q: -27.497559, mean_eps: 0.768902\n",
      "  308641/1200000: episode: 1217, duration: 3.807s, episode steps: 338, steps per second:  89, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 1.139677, mae: 20.937562, mean_q: -27.519831, mean_eps: 0.768646\n",
      "  308811/1200000: episode: 1218, duration: 2.003s, episode steps: 170, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 1.121604, mae: 20.966260, mean_q: -27.568790, mean_eps: 0.768456\n",
      "  308993/1200000: episode: 1219, duration: 2.058s, episode steps: 182, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.324 [0.000, 3.000],  loss: 1.190606, mae: 20.843566, mean_q: -27.400523, mean_eps: 0.768324\n",
      "  309377/1200000: episode: 1220, duration: 4.219s, episode steps: 384, steps per second:  91, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 1.406625, mae: 20.966713, mean_q: -27.539662, mean_eps: 0.768112\n",
      "  309720/1200000: episode: 1221, duration: 3.935s, episode steps: 343, steps per second:  87, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 1.226681, mae: 20.947771, mean_q: -27.512991, mean_eps: 0.767839\n",
      "  310061/1200000: episode: 1222, duration: 3.778s, episode steps: 341, steps per second:  90, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 1.244964, mae: 20.828557, mean_q: -27.354647, mean_eps: 0.767582\n",
      "  310342/1200000: episode: 1223, duration: 3.131s, episode steps: 281, steps per second:  90, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 1.172641, mae: 20.337957, mean_q: -26.698447, mean_eps: 0.767349\n",
      "  310526/1200000: episode: 1224, duration: 2.119s, episode steps: 184, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 1.165581, mae: 20.306865, mean_q: -26.671681, mean_eps: 0.767175\n",
      "  310791/1200000: episode: 1225, duration: 2.995s, episode steps: 265, steps per second:  88, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.310853, mae: 20.341894, mean_q: -26.707580, mean_eps: 0.767007\n",
      "  310972/1200000: episode: 1226, duration: 2.037s, episode steps: 181, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 1.227453, mae: 20.454573, mean_q: -26.851030, mean_eps: 0.766839\n",
      "  311247/1200000: episode: 1227, duration: 3.086s, episode steps: 275, steps per second:  89, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 1.162363, mae: 20.425321, mean_q: -26.825067, mean_eps: 0.766668\n",
      "  311523/1200000: episode: 1228, duration: 3.166s, episode steps: 276, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 1.022816, mae: 20.340942, mean_q: -26.717071, mean_eps: 0.766462\n",
      "  311695/1200000: episode: 1229, duration: 1.917s, episode steps: 172, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 1.220596, mae: 20.464182, mean_q: -26.848090, mean_eps: 0.766294\n",
      "  312092/1200000: episode: 1230, duration: 4.453s, episode steps: 397, steps per second:  89, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 1.192255, mae: 20.449252, mean_q: -26.844214, mean_eps: 0.766080\n",
      "  312316/1200000: episode: 1231, duration: 2.519s, episode steps: 224, steps per second:  89, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 1.097182, mae: 20.387855, mean_q: -26.785310, mean_eps: 0.765847\n",
      "  312561/1200000: episode: 1232, duration: 2.802s, episode steps: 245, steps per second:  87, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 1.425269, mae: 20.464740, mean_q: -26.831472, mean_eps: 0.765671\n",
      "  312803/1200000: episode: 1233, duration: 2.734s, episode steps: 242, steps per second:  89, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 1.102739, mae: 20.403617, mean_q: -26.813150, mean_eps: 0.765489\n",
      "  313175/1200000: episode: 1234, duration: 4.170s, episode steps: 372, steps per second:  89, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 1.440481, mae: 20.459972, mean_q: -26.828818, mean_eps: 0.765259\n",
      "  313562/1200000: episode: 1235, duration: 4.403s, episode steps: 387, steps per second:  88, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 1.326751, mae: 20.425767, mean_q: -26.822088, mean_eps: 0.764974\n",
      "  313742/1200000: episode: 1236, duration: 1.982s, episode steps: 180, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 1.381186, mae: 20.406071, mean_q: -26.786280, mean_eps: 0.764761\n",
      "  313908/1200000: episode: 1237, duration: 1.909s, episode steps: 166, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 1.316612, mae: 20.364326, mean_q: -26.724817, mean_eps: 0.764632\n",
      "  314075/1200000: episode: 1238, duration: 1.905s, episode steps: 167, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.903275, mae: 20.355998, mean_q: -26.786464, mean_eps: 0.764507\n",
      "  314303/1200000: episode: 1239, duration: 2.800s, episode steps: 228, steps per second:  81, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 1.287802, mae: 20.509232, mean_q: -26.907872, mean_eps: 0.764359\n",
      "  314544/1200000: episode: 1240, duration: 2.725s, episode steps: 241, steps per second:  88, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 1.209472, mae: 20.350248, mean_q: -26.720204, mean_eps: 0.764183\n",
      "  314741/1200000: episode: 1241, duration: 2.234s, episode steps: 197, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 1.259331, mae: 20.326054, mean_q: -26.665302, mean_eps: 0.764019\n",
      "  315032/1200000: episode: 1242, duration: 3.311s, episode steps: 291, steps per second:  88, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 1.221525, mae: 20.436812, mean_q: -26.824053, mean_eps: 0.763836\n",
      "  315290/1200000: episode: 1243, duration: 2.923s, episode steps: 258, steps per second:  88, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 1.387574, mae: 20.409842, mean_q: -26.768459, mean_eps: 0.763630\n",
      "  315571/1200000: episode: 1244, duration: 3.172s, episode steps: 281, steps per second:  89, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 1.346443, mae: 20.376886, mean_q: -26.711428, mean_eps: 0.763428\n",
      "  315803/1200000: episode: 1245, duration: 2.648s, episode steps: 232, steps per second:  88, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 1.094697, mae: 20.399555, mean_q: -26.832388, mean_eps: 0.763235\n",
      "  316155/1200000: episode: 1246, duration: 3.940s, episode steps: 352, steps per second:  89, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.354301, mae: 20.571036, mean_q: -27.006759, mean_eps: 0.763016\n",
      "  316341/1200000: episode: 1247, duration: 2.110s, episode steps: 186, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 1.144330, mae: 20.467827, mean_q: -26.859301, mean_eps: 0.762814\n",
      "  316582/1200000: episode: 1248, duration: 2.765s, episode steps: 241, steps per second:  87, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 1.435480, mae: 20.492693, mean_q: -26.843400, mean_eps: 0.762654\n",
      "  316854/1200000: episode: 1249, duration: 3.050s, episode steps: 272, steps per second:  89, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 1.545279, mae: 20.489387, mean_q: -26.861179, mean_eps: 0.762462\n",
      "  317099/1200000: episode: 1250, duration: 2.839s, episode steps: 245, steps per second:  86, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 1.285603, mae: 20.357598, mean_q: -26.729618, mean_eps: 0.762268\n",
      "  317442/1200000: episode: 1251, duration: 3.843s, episode steps: 343, steps per second:  89, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 1.088339, mae: 20.477155, mean_q: -26.909283, mean_eps: 0.762047\n",
      "  317846/1200000: episode: 1252, duration: 4.565s, episode steps: 404, steps per second:  88, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 1.287307, mae: 20.428360, mean_q: -26.827728, mean_eps: 0.761767\n",
      "  318086/1200000: episode: 1253, duration: 2.743s, episode steps: 240, steps per second:  88, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 1.189241, mae: 20.392539, mean_q: -26.799211, mean_eps: 0.761526\n",
      "  318306/1200000: episode: 1254, duration: 2.503s, episode steps: 220, steps per second:  88, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 1.328287, mae: 20.442275, mean_q: -26.871586, mean_eps: 0.761353\n",
      "  318474/1200000: episode: 1255, duration: 1.930s, episode steps: 168, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 1.568883, mae: 20.460274, mean_q: -26.814333, mean_eps: 0.761208\n",
      "  318645/1200000: episode: 1256, duration: 2.014s, episode steps: 171, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 1.249537, mae: 20.419647, mean_q: -26.798148, mean_eps: 0.761081\n",
      "  318808/1200000: episode: 1257, duration: 1.867s, episode steps: 163, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 1.161347, mae: 20.366639, mean_q: -26.744826, mean_eps: 0.760955\n",
      "  319148/1200000: episode: 1258, duration: 3.874s, episode steps: 340, steps per second:  88, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 1.402287, mae: 20.462431, mean_q: -26.845434, mean_eps: 0.760767\n",
      "  319327/1200000: episode: 1259, duration: 2.037s, episode steps: 179, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 1.396831, mae: 20.392379, mean_q: -26.755508, mean_eps: 0.760572\n",
      "  319631/1200000: episode: 1260, duration: 3.490s, episode steps: 304, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 1.220497, mae: 20.409307, mean_q: -26.785154, mean_eps: 0.760391\n",
      "  319839/1200000: episode: 1261, duration: 2.373s, episode steps: 208, steps per second:  88, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 1.295231, mae: 20.422966, mean_q: -26.812110, mean_eps: 0.760199\n",
      "  320061/1200000: episode: 1262, duration: 2.566s, episode steps: 222, steps per second:  87, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 1.548918, mae: 20.376101, mean_q: -26.743616, mean_eps: 0.760038\n",
      "  320360/1200000: episode: 1263, duration: 3.393s, episode steps: 299, steps per second:  88, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 1.192940, mae: 20.476437, mean_q: -26.900588, mean_eps: 0.759842\n",
      "  320532/1200000: episode: 1264, duration: 1.966s, episode steps: 172, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.314 [0.000, 3.000],  loss: 1.266742, mae: 20.405218, mean_q: -26.819001, mean_eps: 0.759666\n",
      "  320814/1200000: episode: 1265, duration: 3.235s, episode steps: 282, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 1.207584, mae: 20.402017, mean_q: -26.782458, mean_eps: 0.759496\n",
      "  320987/1200000: episode: 1266, duration: 2.064s, episode steps: 173, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 1.108897, mae: 20.425937, mean_q: -26.822768, mean_eps: 0.759325\n",
      "  321161/1200000: episode: 1267, duration: 2.017s, episode steps: 174, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 1.200232, mae: 20.404272, mean_q: -26.782450, mean_eps: 0.759195\n",
      "  321373/1200000: episode: 1268, duration: 2.443s, episode steps: 212, steps per second:  87, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 1.400673, mae: 20.409494, mean_q: -26.747649, mean_eps: 0.759050\n",
      "  321709/1200000: episode: 1269, duration: 3.834s, episode steps: 336, steps per second:  88, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.160937, mae: 20.351755, mean_q: -26.708684, mean_eps: 0.758845\n",
      "  322012/1200000: episode: 1270, duration: 3.451s, episode steps: 303, steps per second:  88, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 1.320566, mae: 20.535333, mean_q: -26.943317, mean_eps: 0.758605\n",
      "  322302/1200000: episode: 1271, duration: 3.370s, episode steps: 290, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 1.407291, mae: 20.303472, mean_q: -26.649736, mean_eps: 0.758383\n",
      "  322538/1200000: episode: 1272, duration: 2.722s, episode steps: 236, steps per second:  87, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.285701, mae: 20.465775, mean_q: -26.849764, mean_eps: 0.758185\n",
      "  322865/1200000: episode: 1273, duration: 3.744s, episode steps: 327, steps per second:  87, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 1.181051, mae: 20.421772, mean_q: -26.814355, mean_eps: 0.757974\n",
      "  323096/1200000: episode: 1274, duration: 2.670s, episode steps: 231, steps per second:  87, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 1.371621, mae: 20.493828, mean_q: -26.886300, mean_eps: 0.757765\n",
      "  323278/1200000: episode: 1275, duration: 2.108s, episode steps: 182, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: 1.262418, mae: 20.417160, mean_q: -26.838675, mean_eps: 0.757610\n",
      "  323582/1200000: episode: 1276, duration: 3.519s, episode steps: 304, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 1.478384, mae: 20.450006, mean_q: -26.819194, mean_eps: 0.757428\n",
      "  323788/1200000: episode: 1277, duration: 2.371s, episode steps: 206, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 1.295163, mae: 20.450777, mean_q: -26.852244, mean_eps: 0.757237\n",
      "  324055/1200000: episode: 1278, duration: 3.053s, episode steps: 267, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 1.206304, mae: 20.477009, mean_q: -26.898709, mean_eps: 0.757059\n",
      "  324394/1200000: episode: 1279, duration: 3.907s, episode steps: 339, steps per second:  87, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 1.019633, mae: 20.528088, mean_q: -26.970852, mean_eps: 0.756832\n",
      "  324640/1200000: episode: 1280, duration: 2.843s, episode steps: 246, steps per second:  87, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 1.006834, mae: 20.462120, mean_q: -26.887766, mean_eps: 0.756613\n",
      "  324814/1200000: episode: 1281, duration: 2.016s, episode steps: 174, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 1.150835, mae: 20.510904, mean_q: -26.959415, mean_eps: 0.756455\n",
      "  324989/1200000: episode: 1282, duration: 2.037s, episode steps: 175, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 1.130503, mae: 20.493961, mean_q: -26.926837, mean_eps: 0.756324\n",
      "  325279/1200000: episode: 1283, duration: 3.312s, episode steps: 290, steps per second:  88, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 1.019634, mae: 20.374363, mean_q: -26.738986, mean_eps: 0.756150\n",
      "  325451/1200000: episode: 1284, duration: 1.977s, episode steps: 172, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 1.121064, mae: 20.415233, mean_q: -26.824421, mean_eps: 0.755977\n",
      "  325778/1200000: episode: 1285, duration: 3.792s, episode steps: 327, steps per second:  86, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 1.150270, mae: 20.343817, mean_q: -26.710981, mean_eps: 0.755790\n",
      "  325992/1200000: episode: 1286, duration: 2.505s, episode steps: 214, steps per second:  85, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 1.185515, mae: 20.364285, mean_q: -26.724341, mean_eps: 0.755587\n",
      "  326286/1200000: episode: 1287, duration: 3.461s, episode steps: 294, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 1.121352, mae: 20.433416, mean_q: -26.835360, mean_eps: 0.755396\n",
      "  326475/1200000: episode: 1288, duration: 2.240s, episode steps: 189, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 1.474460, mae: 20.435216, mean_q: -26.817771, mean_eps: 0.755215\n",
      "  326758/1200000: episode: 1289, duration: 3.265s, episode steps: 283, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 1.471119, mae: 20.534884, mean_q: -26.913856, mean_eps: 0.755038\n",
      "  327043/1200000: episode: 1290, duration: 3.305s, episode steps: 285, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 1.228782, mae: 20.367831, mean_q: -26.734119, mean_eps: 0.754825\n",
      "  327208/1200000: episode: 1291, duration: 1.964s, episode steps: 165, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 1.306655, mae: 20.486780, mean_q: -26.930142, mean_eps: 0.754656\n",
      "  327387/1200000: episode: 1292, duration: 2.137s, episode steps: 179, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 1.286382, mae: 20.340824, mean_q: -26.717466, mean_eps: 0.754527\n",
      "  327574/1200000: episode: 1293, duration: 2.198s, episode steps: 187, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 1.228781, mae: 20.291677, mean_q: -26.635516, mean_eps: 0.754390\n",
      "  327801/1200000: episode: 1294, duration: 2.694s, episode steps: 227, steps per second:  84, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 1.120460, mae: 20.436778, mean_q: -26.826022, mean_eps: 0.754235\n",
      "  327998/1200000: episode: 1295, duration: 2.296s, episode steps: 197, steps per second:  86, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 1.285571, mae: 20.450409, mean_q: -26.824131, mean_eps: 0.754076\n",
      "  328221/1200000: episode: 1296, duration: 2.590s, episode steps: 223, steps per second:  86, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 1.359930, mae: 20.411870, mean_q: -26.792233, mean_eps: 0.753918\n",
      "  328432/1200000: episode: 1297, duration: 2.433s, episode steps: 211, steps per second:  87, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 1.115982, mae: 20.429808, mean_q: -26.802505, mean_eps: 0.753755\n",
      "  328623/1200000: episode: 1298, duration: 2.257s, episode steps: 191, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 1.363701, mae: 20.510669, mean_q: -26.933691, mean_eps: 0.753605\n",
      "  328920/1200000: episode: 1299, duration: 3.413s, episode steps: 297, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 1.291732, mae: 20.438961, mean_q: -26.825824, mean_eps: 0.753422\n",
      "  329199/1200000: episode: 1300, duration: 3.221s, episode steps: 279, steps per second:  87, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 1.219217, mae: 20.396811, mean_q: -26.779216, mean_eps: 0.753206\n",
      "  329550/1200000: episode: 1301, duration: 4.023s, episode steps: 351, steps per second:  87, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 1.303378, mae: 20.447590, mean_q: -26.841710, mean_eps: 0.752970\n",
      "  329838/1200000: episode: 1302, duration: 3.369s, episode steps: 288, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 1.226926, mae: 20.509135, mean_q: -26.915393, mean_eps: 0.752730\n",
      "  330143/1200000: episode: 1303, duration: 3.541s, episode steps: 305, steps per second:  86, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 1.233199, mae: 20.136061, mean_q: -26.421880, mean_eps: 0.752507\n",
      "  330374/1200000: episode: 1304, duration: 2.715s, episode steps: 231, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 1.320491, mae: 19.901438, mean_q: -26.103383, mean_eps: 0.752306\n",
      "  330618/1200000: episode: 1305, duration: 2.816s, episode steps: 244, steps per second:  87, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 1.124049, mae: 19.865950, mean_q: -26.098902, mean_eps: 0.752128\n",
      "  330891/1200000: episode: 1306, duration: 3.176s, episode steps: 273, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 1.287950, mae: 19.914731, mean_q: -26.126940, mean_eps: 0.751935\n",
      "  331169/1200000: episode: 1307, duration: 3.287s, episode steps: 278, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 1.218211, mae: 19.830823, mean_q: -26.024788, mean_eps: 0.751728\n",
      "  331352/1200000: episode: 1308, duration: 2.149s, episode steps: 183, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.968907, mae: 19.733434, mean_q: -25.947924, mean_eps: 0.751555\n",
      "  331632/1200000: episode: 1309, duration: 3.319s, episode steps: 280, steps per second:  84, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 1.425325, mae: 19.756423, mean_q: -25.922837, mean_eps: 0.751381\n",
      "  331853/1200000: episode: 1310, duration: 2.608s, episode steps: 221, steps per second:  85, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 1.147249, mae: 19.739742, mean_q: -25.921364, mean_eps: 0.751193\n",
      "  332046/1200000: episode: 1311, duration: 2.362s, episode steps: 193, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 1.374433, mae: 19.786981, mean_q: -25.963936, mean_eps: 0.751038\n",
      "  332326/1200000: episode: 1312, duration: 3.267s, episode steps: 280, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 1.118460, mae: 19.941611, mean_q: -26.188793, mean_eps: 0.750861\n",
      "  332585/1200000: episode: 1313, duration: 3.039s, episode steps: 259, steps per second:  85, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 1.283267, mae: 19.900411, mean_q: -26.122861, mean_eps: 0.750659\n",
      "  332767/1200000: episode: 1314, duration: 2.136s, episode steps: 182, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 1.172266, mae: 19.910620, mean_q: -26.145113, mean_eps: 0.750493\n",
      "  333058/1200000: episode: 1315, duration: 3.394s, episode steps: 291, steps per second:  86, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 1.145595, mae: 19.805236, mean_q: -26.003273, mean_eps: 0.750316\n",
      "  333295/1200000: episode: 1316, duration: 2.788s, episode steps: 237, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 1.173174, mae: 19.822145, mean_q: -26.041342, mean_eps: 0.750118\n",
      "  333499/1200000: episode: 1317, duration: 2.408s, episode steps: 204, steps per second:  85, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.328 [0.000, 3.000],  loss: 1.130663, mae: 19.913538, mean_q: -26.149094, mean_eps: 0.749953\n",
      "  333682/1200000: episode: 1318, duration: 2.144s, episode steps: 183, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.377 [0.000, 3.000],  loss: 0.932182, mae: 19.737909, mean_q: -25.936748, mean_eps: 0.749807\n",
      "  334018/1200000: episode: 1319, duration: 3.911s, episode steps: 336, steps per second:  86, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 1.232338, mae: 19.803776, mean_q: -25.985041, mean_eps: 0.749613\n",
      "  334197/1200000: episode: 1320, duration: 2.173s, episode steps: 179, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 1.220151, mae: 19.914266, mean_q: -26.112380, mean_eps: 0.749420\n",
      "  334549/1200000: episode: 1321, duration: 4.071s, episode steps: 352, steps per second:  86, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 1.152272, mae: 19.843778, mean_q: -26.056296, mean_eps: 0.749221\n",
      "  334837/1200000: episode: 1322, duration: 3.377s, episode steps: 288, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 1.133109, mae: 19.775492, mean_q: -25.987983, mean_eps: 0.748981\n",
      "  335100/1200000: episode: 1323, duration: 3.100s, episode steps: 263, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 1.382588, mae: 19.763341, mean_q: -25.941818, mean_eps: 0.748774\n",
      "  335381/1200000: episode: 1324, duration: 3.313s, episode steps: 281, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 1.321925, mae: 19.860830, mean_q: -26.041742, mean_eps: 0.748570\n",
      "  335617/1200000: episode: 1325, duration: 2.772s, episode steps: 236, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 1.329698, mae: 19.749989, mean_q: -25.909968, mean_eps: 0.748376\n",
      "  335787/1200000: episode: 1326, duration: 1.999s, episode steps: 170, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.624 [0.000, 3.000],  loss: 1.149670, mae: 19.771060, mean_q: -25.982365, mean_eps: 0.748224\n",
      "  335968/1200000: episode: 1327, duration: 2.143s, episode steps: 181, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 1.355138, mae: 19.862995, mean_q: -26.055111, mean_eps: 0.748092\n",
      "  336320/1200000: episode: 1328, duration: 4.151s, episode steps: 352, steps per second:  85, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 1.175672, mae: 19.828395, mean_q: -26.027756, mean_eps: 0.747892\n",
      "  336550/1200000: episode: 1329, duration: 2.798s, episode steps: 230, steps per second:  82, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 1.225021, mae: 19.774734, mean_q: -25.966628, mean_eps: 0.747674\n",
      "  336792/1200000: episode: 1330, duration: 2.823s, episode steps: 242, steps per second:  86, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 1.318110, mae: 19.941398, mean_q: -26.152368, mean_eps: 0.747497\n",
      "  337035/1200000: episode: 1331, duration: 2.881s, episode steps: 243, steps per second:  84, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 1.149889, mae: 19.763990, mean_q: -25.912624, mean_eps: 0.747315\n",
      "  337341/1200000: episode: 1332, duration: 3.593s, episode steps: 306, steps per second:  85, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 1.376927, mae: 19.751653, mean_q: -25.919751, mean_eps: 0.747109\n",
      "  337521/1200000: episode: 1333, duration: 2.173s, episode steps: 180, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 1.042446, mae: 19.883722, mean_q: -26.111361, mean_eps: 0.746927\n",
      "  337692/1200000: episode: 1334, duration: 2.030s, episode steps: 171, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 1.118871, mae: 19.836418, mean_q: -25.997273, mean_eps: 0.746795\n",
      "  338150/1200000: episode: 1335, duration: 5.374s, episode steps: 458, steps per second:  85, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 1.296293, mae: 19.821672, mean_q: -26.005867, mean_eps: 0.746560\n",
      "  338373/1200000: episode: 1336, duration: 2.607s, episode steps: 223, steps per second:  86, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 1.120796, mae: 19.920046, mean_q: -26.159179, mean_eps: 0.746304\n",
      "  338651/1200000: episode: 1337, duration: 3.318s, episode steps: 278, steps per second:  84, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 1.312805, mae: 19.785817, mean_q: -25.943501, mean_eps: 0.746116\n",
      "  338821/1200000: episode: 1338, duration: 2.026s, episode steps: 170, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.641 [0.000, 3.000],  loss: 1.128856, mae: 19.730761, mean_q: -25.940674, mean_eps: 0.745948\n",
      "  339037/1200000: episode: 1339, duration: 2.551s, episode steps: 216, steps per second:  85, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 1.353090, mae: 19.908988, mean_q: -26.098876, mean_eps: 0.745804\n",
      "  339378/1200000: episode: 1340, duration: 4.174s, episode steps: 341, steps per second:  82, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 1.169346, mae: 19.760877, mean_q: -25.923509, mean_eps: 0.745595\n",
      "  339746/1200000: episode: 1341, duration: 4.302s, episode steps: 368, steps per second:  86, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 1.269934, mae: 19.805579, mean_q: -25.981111, mean_eps: 0.745329\n",
      "  340097/1200000: episode: 1342, duration: 4.074s, episode steps: 351, steps per second:  86, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 1.094971, mae: 19.694834, mean_q: -25.870095, mean_eps: 0.745059\n",
      "  340463/1200000: episode: 1343, duration: 4.268s, episode steps: 366, steps per second:  86, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 1.242435, mae: 19.270313, mean_q: -25.269914, mean_eps: 0.744790\n",
      "  340841/1200000: episode: 1344, duration: 4.465s, episode steps: 378, steps per second:  85, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 1.068956, mae: 19.234725, mean_q: -25.241002, mean_eps: 0.744511\n",
      "  341018/1200000: episode: 1345, duration: 2.141s, episode steps: 177, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.951043, mae: 19.195503, mean_q: -25.238579, mean_eps: 0.744303\n",
      "  341202/1200000: episode: 1346, duration: 2.184s, episode steps: 184, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.739 [0.000, 3.000],  loss: 1.021170, mae: 19.199051, mean_q: -25.202810, mean_eps: 0.744168\n",
      "  341377/1200000: episode: 1347, duration: 2.095s, episode steps: 175, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.691 [0.000, 3.000],  loss: 0.925122, mae: 19.290831, mean_q: -25.350609, mean_eps: 0.744033\n",
      "  341754/1200000: episode: 1348, duration: 4.520s, episode steps: 377, steps per second:  83, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 1.083297, mae: 19.181473, mean_q: -25.182470, mean_eps: 0.743826\n",
      "  341932/1200000: episode: 1349, duration: 2.144s, episode steps: 178, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.792 [0.000, 3.000],  loss: 0.906193, mae: 19.399719, mean_q: -25.480684, mean_eps: 0.743618\n",
      "  342099/1200000: episode: 1350, duration: 1.994s, episode steps: 167, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 1.077933, mae: 19.160649, mean_q: -25.183549, mean_eps: 0.743489\n",
      "  342431/1200000: episode: 1351, duration: 3.896s, episode steps: 332, steps per second:  85, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 1.006901, mae: 19.192042, mean_q: -25.219324, mean_eps: 0.743302\n",
      "  342601/1200000: episode: 1352, duration: 2.010s, episode steps: 170, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 1.003741, mae: 19.155433, mean_q: -25.139935, mean_eps: 0.743113\n",
      "  343176/1200000: episode: 1353, duration: 6.747s, episode steps: 575, steps per second:  85, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 1.158079, mae: 19.172346, mean_q: -25.165301, mean_eps: 0.742834\n",
      "  343341/1200000: episode: 1354, duration: 1.990s, episode steps: 165, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.995122, mae: 19.188921, mean_q: -25.196049, mean_eps: 0.742557\n",
      "  343568/1200000: episode: 1355, duration: 2.663s, episode steps: 227, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 1.250672, mae: 19.271398, mean_q: -25.267111, mean_eps: 0.742410\n",
      "  343798/1200000: episode: 1356, duration: 2.762s, episode steps: 230, steps per second:  83, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.924363, mae: 19.267732, mean_q: -25.305259, mean_eps: 0.742238\n",
      "  343965/1200000: episode: 1357, duration: 2.044s, episode steps: 167, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 1.073853, mae: 19.114808, mean_q: -25.088381, mean_eps: 0.742089\n",
      "  344270/1200000: episode: 1358, duration: 3.755s, episode steps: 305, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.133057, mae: 19.155737, mean_q: -25.140656, mean_eps: 0.741912\n",
      "  344467/1200000: episode: 1359, duration: 2.379s, episode steps: 197, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 1.053979, mae: 19.286010, mean_q: -25.319907, mean_eps: 0.741724\n",
      "  344713/1200000: episode: 1360, duration: 2.957s, episode steps: 246, steps per second:  83, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 1.161368, mae: 19.215558, mean_q: -25.203480, mean_eps: 0.741558\n",
      "  344890/1200000: episode: 1361, duration: 2.155s, episode steps: 177, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.599 [0.000, 3.000],  loss: 1.346854, mae: 19.219256, mean_q: -25.181726, mean_eps: 0.741399\n",
      "  345234/1200000: episode: 1362, duration: 4.095s, episode steps: 344, steps per second:  84, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 1.054543, mae: 19.240709, mean_q: -25.263400, mean_eps: 0.741204\n",
      "  345408/1200000: episode: 1363, duration: 2.105s, episode steps: 174, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 1.166353, mae: 19.036670, mean_q: -25.002614, mean_eps: 0.741010\n",
      "  345630/1200000: episode: 1364, duration: 2.664s, episode steps: 222, steps per second:  83, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 1.141027, mae: 19.205038, mean_q: -25.230462, mean_eps: 0.740861\n",
      "  345796/1200000: episode: 1365, duration: 2.071s, episode steps: 166, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 1.054202, mae: 19.212661, mean_q: -25.205936, mean_eps: 0.740716\n",
      "  346095/1200000: episode: 1366, duration: 3.508s, episode steps: 299, steps per second:  85, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 1.095210, mae: 19.237212, mean_q: -25.234867, mean_eps: 0.740541\n",
      "  346261/1200000: episode: 1367, duration: 2.025s, episode steps: 166, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.985959, mae: 19.213303, mean_q: -25.223093, mean_eps: 0.740367\n",
      "  346718/1200000: episode: 1368, duration: 5.493s, episode steps: 457, steps per second:  83, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 1.201193, mae: 19.128852, mean_q: -25.112379, mean_eps: 0.740133\n",
      "  346884/1200000: episode: 1369, duration: 2.040s, episode steps: 166, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.960549, mae: 19.047986, mean_q: -25.000221, mean_eps: 0.739900\n",
      "  347096/1200000: episode: 1370, duration: 2.557s, episode steps: 212, steps per second:  83, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 1.206820, mae: 19.223229, mean_q: -25.204372, mean_eps: 0.739758\n",
      "  347269/1200000: episode: 1371, duration: 2.067s, episode steps: 173, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: 0.975401, mae: 19.144360, mean_q: -25.110603, mean_eps: 0.739613\n",
      "  347665/1200000: episode: 1372, duration: 4.750s, episode steps: 396, steps per second:  83, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 1.119266, mae: 19.198427, mean_q: -25.183865, mean_eps: 0.739400\n",
      "  347830/1200000: episode: 1373, duration: 2.002s, episode steps: 165, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 1.052205, mae: 19.260738, mean_q: -25.287184, mean_eps: 0.739190\n",
      "  348088/1200000: episode: 1374, duration: 3.145s, episode steps: 258, steps per second:  82, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 1.185887, mae: 19.212112, mean_q: -25.169401, mean_eps: 0.739031\n",
      "  348430/1200000: episode: 1375, duration: 4.093s, episode steps: 342, steps per second:  84, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 1.125977, mae: 19.243108, mean_q: -25.264355, mean_eps: 0.738806\n",
      "  348711/1200000: episode: 1376, duration: 3.374s, episode steps: 281, steps per second:  83, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 1.023375, mae: 19.202041, mean_q: -25.218208, mean_eps: 0.738573\n",
      "  348874/1200000: episode: 1377, duration: 2.000s, episode steps: 163, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 1.225168, mae: 19.408927, mean_q: -25.469610, mean_eps: 0.738406\n",
      "  349041/1200000: episode: 1378, duration: 2.110s, episode steps: 167, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.521 [0.000, 3.000],  loss: 1.012786, mae: 19.163778, mean_q: -25.185263, mean_eps: 0.738282\n",
      "  349289/1200000: episode: 1379, duration: 3.018s, episode steps: 248, steps per second:  82, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 1.311190, mae: 19.248116, mean_q: -25.233265, mean_eps: 0.738127\n",
      "  349467/1200000: episode: 1380, duration: 2.177s, episode steps: 178, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.964712, mae: 19.192671, mean_q: -25.206861, mean_eps: 0.737967\n",
      "  349640/1200000: episode: 1381, duration: 2.138s, episode steps: 173, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.932552, mae: 19.201752, mean_q: -25.208160, mean_eps: 0.737835\n",
      "  349982/1200000: episode: 1382, duration: 4.189s, episode steps: 342, steps per second:  82, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 1.184460, mae: 19.277191, mean_q: -25.273400, mean_eps: 0.737642\n",
      "  350151/1200000: episode: 1383, duration: 2.074s, episode steps: 169, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 1.107284, mae: 18.973679, mean_q: -24.892715, mean_eps: 0.737451\n",
      "  350372/1200000: episode: 1384, duration: 2.698s, episode steps: 221, steps per second:  82, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 1.114364, mae: 18.872828, mean_q: -24.771831, mean_eps: 0.737304\n",
      "  350626/1200000: episode: 1385, duration: 3.080s, episode steps: 254, steps per second:  82, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 1.145973, mae: 18.971258, mean_q: -24.872557, mean_eps: 0.737126\n",
      "  350969/1200000: episode: 1386, duration: 4.192s, episode steps: 343, steps per second:  82, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.998629, mae: 19.011235, mean_q: -24.969174, mean_eps: 0.736902\n",
      "  351142/1200000: episode: 1387, duration: 2.288s, episode steps: 173, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 1.360788, mae: 18.936600, mean_q: -24.819871, mean_eps: 0.736709\n",
      "  351445/1200000: episode: 1388, duration: 3.684s, episode steps: 303, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 1.142388, mae: 18.965270, mean_q: -24.889599, mean_eps: 0.736530\n",
      "  351624/1200000: episode: 1389, duration: 2.203s, episode steps: 179, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 1.224984, mae: 19.034856, mean_q: -24.970102, mean_eps: 0.736349\n",
      "  351843/1200000: episode: 1390, duration: 2.810s, episode steps: 219, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 1.019425, mae: 18.975023, mean_q: -24.923968, mean_eps: 0.736200\n",
      "  352009/1200000: episode: 1391, duration: 2.068s, episode steps: 166, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.920667, mae: 19.028286, mean_q: -25.007273, mean_eps: 0.736056\n",
      "  352175/1200000: episode: 1392, duration: 2.045s, episode steps: 166, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.998843, mae: 19.106516, mean_q: -25.086947, mean_eps: 0.735931\n",
      "  352412/1200000: episode: 1393, duration: 2.891s, episode steps: 237, steps per second:  82, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 1.159175, mae: 18.983628, mean_q: -24.906732, mean_eps: 0.735780\n",
      "  352615/1200000: episode: 1394, duration: 2.501s, episode steps: 203, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 1.148888, mae: 18.980564, mean_q: -24.883316, mean_eps: 0.735615\n",
      "  353036/1200000: episode: 1395, duration: 5.159s, episode steps: 421, steps per second:  82, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 1.005491, mae: 18.921507, mean_q: -24.837282, mean_eps: 0.735381\n",
      "  353220/1200000: episode: 1396, duration: 2.247s, episode steps: 184, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 1.212837, mae: 18.922796, mean_q: -24.812779, mean_eps: 0.735154\n",
      "  353403/1200000: episode: 1397, duration: 2.252s, episode steps: 183, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 1.281083, mae: 19.022857, mean_q: -24.943772, mean_eps: 0.735017\n",
      "  353656/1200000: episode: 1398, duration: 3.124s, episode steps: 253, steps per second:  81, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.218752, mae: 18.976998, mean_q: -24.882398, mean_eps: 0.734853\n",
      "  353832/1200000: episode: 1399, duration: 2.153s, episode steps: 176, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 1.047102, mae: 18.985849, mean_q: -24.945576, mean_eps: 0.734692\n",
      "  354047/1200000: episode: 1400, duration: 2.581s, episode steps: 215, steps per second:  83, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.993839, mae: 18.870053, mean_q: -24.787435, mean_eps: 0.734546\n",
      "  354211/1200000: episode: 1401, duration: 2.193s, episode steps: 164, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 1.093303, mae: 19.012071, mean_q: -24.954844, mean_eps: 0.734404\n",
      "  354576/1200000: episode: 1402, duration: 4.563s, episode steps: 365, steps per second:  80, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 1.112787, mae: 19.021651, mean_q: -24.939039, mean_eps: 0.734205\n",
      "  354867/1200000: episode: 1403, duration: 3.496s, episode steps: 291, steps per second:  83, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 1.191848, mae: 18.905580, mean_q: -24.828165, mean_eps: 0.733959\n",
      "  355139/1200000: episode: 1404, duration: 3.493s, episode steps: 272, steps per second:  78, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 1.169170, mae: 18.902791, mean_q: -24.777131, mean_eps: 0.733748\n",
      "  355377/1200000: episode: 1405, duration: 2.924s, episode steps: 238, steps per second:  81, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 1.109532, mae: 19.024252, mean_q: -24.974083, mean_eps: 0.733557\n",
      "  355616/1200000: episode: 1406, duration: 2.944s, episode steps: 239, steps per second:  81, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 1.328082, mae: 19.101315, mean_q: -25.048771, mean_eps: 0.733378\n",
      "  355909/1200000: episode: 1407, duration: 3.593s, episode steps: 293, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.998068, mae: 19.034892, mean_q: -25.011285, mean_eps: 0.733179\n",
      "  356086/1200000: episode: 1408, duration: 2.239s, episode steps: 177, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 1.065713, mae: 18.949273, mean_q: -24.885578, mean_eps: 0.733002\n",
      "  356265/1200000: episode: 1409, duration: 2.206s, episode steps: 179, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.894585, mae: 18.983912, mean_q: -24.938912, mean_eps: 0.732869\n",
      "  356565/1200000: episode: 1410, duration: 3.766s, episode steps: 300, steps per second:  80, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 1.122298, mae: 18.878803, mean_q: -24.806682, mean_eps: 0.732689\n",
      "  356773/1200000: episode: 1411, duration: 2.584s, episode steps: 208, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 1.348057, mae: 19.013811, mean_q: -24.921578, mean_eps: 0.732499\n",
      "  357075/1200000: episode: 1412, duration: 3.697s, episode steps: 302, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.995803, mae: 19.079194, mean_q: -25.038409, mean_eps: 0.732307\n",
      "  357240/1200000: episode: 1413, duration: 2.041s, episode steps: 165, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.964645, mae: 18.788888, mean_q: -24.669478, mean_eps: 0.732132\n",
      "  357697/1200000: episode: 1414, duration: 5.564s, episode steps: 457, steps per second:  82, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 1.269776, mae: 18.949114, mean_q: -24.848049, mean_eps: 0.731899\n",
      "  357875/1200000: episode: 1415, duration: 2.237s, episode steps: 178, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.612 [0.000, 3.000],  loss: 1.120294, mae: 18.832150, mean_q: -24.710151, mean_eps: 0.731661\n",
      "  358094/1200000: episode: 1416, duration: 2.664s, episode steps: 219, steps per second:  82, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 1.274816, mae: 18.972268, mean_q: -24.877631, mean_eps: 0.731512\n",
      "  358271/1200000: episode: 1417, duration: 2.165s, episode steps: 177, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 1.021754, mae: 18.811856, mean_q: -24.700154, mean_eps: 0.731363\n",
      "  358441/1200000: episode: 1418, duration: 2.153s, episode steps: 170, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.347 [0.000, 3.000],  loss: 1.263795, mae: 18.840225, mean_q: -24.696235, mean_eps: 0.731233\n",
      "  358797/1200000: episode: 1419, duration: 4.376s, episode steps: 356, steps per second:  81, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 1.181392, mae: 18.901586, mean_q: -24.785205, mean_eps: 0.731036\n",
      "  358978/1200000: episode: 1420, duration: 2.250s, episode steps: 181, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 1.248623, mae: 18.906781, mean_q: -24.810143, mean_eps: 0.730835\n",
      "  359169/1200000: episode: 1421, duration: 2.345s, episode steps: 191, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 1.188435, mae: 18.946453, mean_q: -24.866940, mean_eps: 0.730695\n",
      "  359415/1200000: episode: 1422, duration: 3.025s, episode steps: 246, steps per second:  81, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 1.335381, mae: 18.961568, mean_q: -24.824322, mean_eps: 0.730531\n",
      "  359729/1200000: episode: 1423, duration: 3.840s, episode steps: 314, steps per second:  82, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.909613, mae: 18.916129, mean_q: -24.846892, mean_eps: 0.730321\n",
      "  360056/1200000: episode: 1424, duration: 4.017s, episode steps: 327, steps per second:  81, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 1.199876, mae: 18.945858, mean_q: -24.852021, mean_eps: 0.730081\n",
      "  360256/1200000: episode: 1425, duration: 2.511s, episode steps: 200, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.990434, mae: 18.207757, mean_q: -23.891416, mean_eps: 0.729883\n",
      "  360474/1200000: episode: 1426, duration: 2.708s, episode steps: 218, steps per second:  81, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.825291, mae: 18.152172, mean_q: -23.877564, mean_eps: 0.729727\n",
      "  360645/1200000: episode: 1427, duration: 2.106s, episode steps: 171, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 1.288729, mae: 18.289614, mean_q: -24.001224, mean_eps: 0.729581\n",
      "  360858/1200000: episode: 1428, duration: 2.652s, episode steps: 213, steps per second:  80, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.296 [0.000, 3.000],  loss: 1.263019, mae: 18.310170, mean_q: -24.003391, mean_eps: 0.729437\n",
      "  361204/1200000: episode: 1429, duration: 4.253s, episode steps: 346, steps per second:  81, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 1.070396, mae: 18.239081, mean_q: -23.957799, mean_eps: 0.729227\n",
      "  361460/1200000: episode: 1430, duration: 3.274s, episode steps: 256, steps per second:  78, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 1.219317, mae: 18.299829, mean_q: -24.006138, mean_eps: 0.729001\n",
      "  361861/1200000: episode: 1431, duration: 4.930s, episode steps: 401, steps per second:  81, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 1.135772, mae: 18.269858, mean_q: -23.982518, mean_eps: 0.728755\n",
      "  362030/1200000: episode: 1432, duration: 2.115s, episode steps: 169, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.857229, mae: 18.276564, mean_q: -24.027631, mean_eps: 0.728541\n",
      "  362330/1200000: episode: 1433, duration: 3.680s, episode steps: 300, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 1.147109, mae: 18.173265, mean_q: -23.804795, mean_eps: 0.728365\n",
      "  362594/1200000: episode: 1434, duration: 3.276s, episode steps: 264, steps per second:  81, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 1.290675, mae: 18.211941, mean_q: -23.885995, mean_eps: 0.728154\n",
      "  362830/1200000: episode: 1435, duration: 2.940s, episode steps: 236, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 1.140090, mae: 18.206765, mean_q: -23.909717, mean_eps: 0.727966\n",
      "  363012/1200000: episode: 1436, duration: 2.297s, episode steps: 182, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 1.300240, mae: 18.309995, mean_q: -23.970230, mean_eps: 0.727810\n",
      "  363300/1200000: episode: 1437, duration: 3.580s, episode steps: 288, steps per second:  80, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 1.225339, mae: 18.231849, mean_q: -23.902289, mean_eps: 0.727633\n",
      "  363480/1200000: episode: 1438, duration: 2.266s, episode steps: 180, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 1.034260, mae: 18.316786, mean_q: -24.032659, mean_eps: 0.727458\n",
      "  363655/1200000: episode: 1439, duration: 2.222s, episode steps: 175, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.932796, mae: 18.185437, mean_q: -23.877155, mean_eps: 0.727325\n",
      "  363881/1200000: episode: 1440, duration: 2.810s, episode steps: 226, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 1.006543, mae: 18.241266, mean_q: -23.933750, mean_eps: 0.727174\n",
      "  364049/1200000: episode: 1441, duration: 2.098s, episode steps: 168, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 1.395716, mae: 18.126432, mean_q: -23.724369, mean_eps: 0.727027\n",
      "  364271/1200000: episode: 1442, duration: 2.838s, episode steps: 222, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.846947, mae: 18.290100, mean_q: -24.026808, mean_eps: 0.726880\n",
      "  364480/1200000: episode: 1443, duration: 2.608s, episode steps: 209, steps per second:  80, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.273 [0.000, 3.000],  loss: 1.013569, mae: 18.302564, mean_q: -24.038289, mean_eps: 0.726719\n",
      "  364850/1200000: episode: 1444, duration: 4.588s, episode steps: 370, steps per second:  81, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 1.022765, mae: 18.193935, mean_q: -23.881566, mean_eps: 0.726502\n",
      "  365293/1200000: episode: 1445, duration: 5.488s, episode steps: 443, steps per second:  81, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 1.058558, mae: 18.183782, mean_q: -23.880159, mean_eps: 0.726197\n",
      "  365552/1200000: episode: 1446, duration: 3.318s, episode steps: 259, steps per second:  78, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 1.028275, mae: 18.258850, mean_q: -23.969546, mean_eps: 0.725934\n",
      "  365827/1200000: episode: 1447, duration: 3.445s, episode steps: 275, steps per second:  80, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 1.128917, mae: 18.188926, mean_q: -23.888656, mean_eps: 0.725733\n",
      "  366077/1200000: episode: 1448, duration: 3.749s, episode steps: 250, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 1.069499, mae: 18.202867, mean_q: -23.879357, mean_eps: 0.725536\n",
      "  366308/1200000: episode: 1449, duration: 3.052s, episode steps: 231, steps per second:  76, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 1.156738, mae: 18.334055, mean_q: -24.019470, mean_eps: 0.725356\n",
      "  366573/1200000: episode: 1450, duration: 3.664s, episode steps: 265, steps per second:  72, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.959598, mae: 18.200261, mean_q: -23.887380, mean_eps: 0.725170\n",
      "  366774/1200000: episode: 1451, duration: 2.572s, episode steps: 201, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 1.190591, mae: 18.343895, mean_q: -24.059891, mean_eps: 0.724995\n",
      "  366941/1200000: episode: 1452, duration: 2.237s, episode steps: 167, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 1.455639, mae: 18.245006, mean_q: -23.880814, mean_eps: 0.724857\n",
      "  367112/1200000: episode: 1453, duration: 2.236s, episode steps: 171, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 1.138621, mae: 18.181858, mean_q: -23.844048, mean_eps: 0.724730\n",
      "  367294/1200000: episode: 1454, duration: 2.288s, episode steps: 182, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 1.064898, mae: 18.182065, mean_q: -23.889221, mean_eps: 0.724598\n",
      "  367457/1200000: episode: 1455, duration: 2.036s, episode steps: 163, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.313 [0.000, 3.000],  loss: 1.065658, mae: 18.227648, mean_q: -23.925159, mean_eps: 0.724469\n",
      "  367617/1200000: episode: 1456, duration: 2.062s, episode steps: 160, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 1.086693, mae: 18.285319, mean_q: -24.004331, mean_eps: 0.724348\n",
      "  367786/1200000: episode: 1457, duration: 2.123s, episode steps: 169, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 1.201756, mae: 18.237528, mean_q: -23.917715, mean_eps: 0.724224\n",
      "  367962/1200000: episode: 1458, duration: 2.206s, episode steps: 176, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 1.202131, mae: 18.152838, mean_q: -23.846066, mean_eps: 0.724095\n",
      "  368129/1200000: episode: 1459, duration: 2.082s, episode steps: 167, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 1.131716, mae: 18.232819, mean_q: -23.880003, mean_eps: 0.723966\n",
      "  368348/1200000: episode: 1460, duration: 2.901s, episode steps: 219, steps per second:  75, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 1.054221, mae: 18.350013, mean_q: -24.062668, mean_eps: 0.723822\n",
      "  368513/1200000: episode: 1461, duration: 2.320s, episode steps: 165, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.521 [0.000, 3.000],  loss: 1.161791, mae: 18.181697, mean_q: -23.839434, mean_eps: 0.723677\n",
      "  368760/1200000: episode: 1462, duration: 3.300s, episode steps: 247, steps per second:  75, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 1.100235, mae: 18.174320, mean_q: -23.829160, mean_eps: 0.723523\n",
      "  368932/1200000: episode: 1463, duration: 2.331s, episode steps: 172, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.879282, mae: 18.357239, mean_q: -24.128140, mean_eps: 0.723366\n",
      "  369137/1200000: episode: 1464, duration: 2.776s, episode steps: 205, steps per second:  74, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 1.023979, mae: 18.121145, mean_q: -23.790386, mean_eps: 0.723224\n",
      "  369447/1200000: episode: 1465, duration: 3.873s, episode steps: 310, steps per second:  80, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 1.229709, mae: 18.218289, mean_q: -23.898507, mean_eps: 0.723031\n",
      "  369621/1200000: episode: 1466, duration: 2.216s, episode steps: 174, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.702657, mae: 18.223531, mean_q: -23.995831, mean_eps: 0.722850\n",
      "  369876/1200000: episode: 1467, duration: 3.265s, episode steps: 255, steps per second:  78, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 1.032912, mae: 18.194505, mean_q: -23.895895, mean_eps: 0.722689\n",
      "  370051/1200000: episode: 1468, duration: 2.218s, episode steps: 175, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 1.129408, mae: 18.025679, mean_q: -23.638958, mean_eps: 0.722528\n",
      "  370268/1200000: episode: 1469, duration: 2.750s, episode steps: 217, steps per second:  79, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 1.328374, mae: 17.622047, mean_q: -23.074057, mean_eps: 0.722381\n",
      "  370612/1200000: episode: 1470, duration: 4.348s, episode steps: 344, steps per second:  79, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 1.050373, mae: 17.724143, mean_q: -23.261318, mean_eps: 0.722170\n",
      "  370783/1200000: episode: 1471, duration: 2.194s, episode steps: 171, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 0.890251, mae: 17.647907, mean_q: -23.167405, mean_eps: 0.721977\n",
      "  371015/1200000: episode: 1472, duration: 3.063s, episode steps: 232, steps per second:  76, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 1.005198, mae: 17.882727, mean_q: -23.475998, mean_eps: 0.721826\n",
      "  371180/1200000: episode: 1473, duration: 2.124s, episode steps: 165, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 1.078167, mae: 17.811788, mean_q: -23.365448, mean_eps: 0.721677\n",
      "  371352/1200000: episode: 1474, duration: 2.281s, episode steps: 172, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.918719, mae: 17.730332, mean_q: -23.276867, mean_eps: 0.721551\n",
      "  371587/1200000: episode: 1475, duration: 2.976s, episode steps: 235, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.391 [0.000, 3.000],  loss: 0.823274, mae: 17.705763, mean_q: -23.227192, mean_eps: 0.721398\n",
      "  371764/1200000: episode: 1476, duration: 2.233s, episode steps: 177, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 1.028105, mae: 17.721266, mean_q: -23.278251, mean_eps: 0.721244\n",
      "  372064/1200000: episode: 1477, duration: 3.696s, episode steps: 300, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 1.033536, mae: 17.710819, mean_q: -23.238343, mean_eps: 0.721065\n",
      "  372446/1200000: episode: 1478, duration: 4.861s, episode steps: 382, steps per second:  79, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.919957, mae: 17.824775, mean_q: -23.389883, mean_eps: 0.720809\n",
      "  372620/1200000: episode: 1479, duration: 2.248s, episode steps: 174, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 1.241338, mae: 17.853959, mean_q: -23.378494, mean_eps: 0.720601\n",
      "  372859/1200000: episode: 1480, duration: 2.976s, episode steps: 239, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 1.166253, mae: 17.694538, mean_q: -23.225389, mean_eps: 0.720446\n",
      "  373149/1200000: episode: 1481, duration: 3.738s, episode steps: 290, steps per second:  78, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.972643, mae: 17.851826, mean_q: -23.434433, mean_eps: 0.720247\n",
      "  373336/1200000: episode: 1482, duration: 2.404s, episode steps: 187, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.929999, mae: 17.619699, mean_q: -23.124096, mean_eps: 0.720068\n",
      "  373528/1200000: episode: 1483, duration: 2.583s, episode steps: 192, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.960887, mae: 17.708899, mean_q: -23.208476, mean_eps: 0.719926\n",
      "  373842/1200000: episode: 1484, duration: 3.972s, episode steps: 314, steps per second:  79, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.952001, mae: 17.793003, mean_q: -23.361816, mean_eps: 0.719737\n",
      "  374205/1200000: episode: 1485, duration: 4.518s, episode steps: 363, steps per second:  80, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 1.078256, mae: 17.762182, mean_q: -23.300104, mean_eps: 0.719483\n",
      "  374636/1200000: episode: 1486, duration: 5.516s, episode steps: 431, steps per second:  78, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 1.132408, mae: 17.761757, mean_q: -23.294692, mean_eps: 0.719185\n",
      "  374849/1200000: episode: 1487, duration: 2.750s, episode steps: 213, steps per second:  77, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 1.154774, mae: 17.850174, mean_q: -23.389273, mean_eps: 0.718944\n",
      "  375009/1200000: episode: 1488, duration: 2.078s, episode steps: 160, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.256 [0.000, 3.000],  loss: 1.172053, mae: 17.847296, mean_q: -23.400320, mean_eps: 0.718804\n",
      "  375267/1200000: episode: 1489, duration: 3.250s, episode steps: 258, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 1.032855, mae: 17.809757, mean_q: -23.379005, mean_eps: 0.718647\n",
      "  375648/1200000: episode: 1490, duration: 5.124s, episode steps: 381, steps per second:  74, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.778771, mae: 17.803285, mean_q: -23.397165, mean_eps: 0.718407\n",
      "  375815/1200000: episode: 1491, duration: 2.183s, episode steps: 167, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 1.078043, mae: 17.803024, mean_q: -23.353791, mean_eps: 0.718202\n",
      "  376109/1200000: episode: 1492, duration: 3.768s, episode steps: 294, steps per second:  78, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 1.039444, mae: 17.762559, mean_q: -23.298304, mean_eps: 0.718029\n",
      "  376272/1200000: episode: 1493, duration: 2.134s, episode steps: 163, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.713435, mae: 17.739418, mean_q: -23.335202, mean_eps: 0.717858\n",
      "  376443/1200000: episode: 1494, duration: 2.269s, episode steps: 171, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 1.020211, mae: 17.814964, mean_q: -23.352004, mean_eps: 0.717732\n",
      "  376624/1200000: episode: 1495, duration: 2.717s, episode steps: 181, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.641 [0.000, 3.000],  loss: 0.870161, mae: 17.690163, mean_q: -23.225665, mean_eps: 0.717600\n",
      "  376835/1200000: episode: 1496, duration: 3.061s, episode steps: 211, steps per second:  69, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.961419, mae: 17.812505, mean_q: -23.344635, mean_eps: 0.717453\n",
      "  377039/1200000: episode: 1497, duration: 2.852s, episode steps: 204, steps per second:  72, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.934465, mae: 17.779385, mean_q: -23.378226, mean_eps: 0.717298\n",
      "  377246/1200000: episode: 1498, duration: 2.852s, episode steps: 207, steps per second:  73, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.969651, mae: 17.758540, mean_q: -23.306182, mean_eps: 0.717144\n",
      "  377483/1200000: episode: 1499, duration: 3.297s, episode steps: 237, steps per second:  72, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 1.014639, mae: 17.812963, mean_q: -23.385504, mean_eps: 0.716977\n",
      "  377647/1200000: episode: 1500, duration: 2.203s, episode steps: 164, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 0.922027, mae: 17.637594, mean_q: -23.145519, mean_eps: 0.716827\n",
      "  377828/1200000: episode: 1501, duration: 2.392s, episode steps: 181, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.580 [0.000, 3.000],  loss: 1.035712, mae: 17.735652, mean_q: -23.257612, mean_eps: 0.716697\n",
      "  378065/1200000: episode: 1502, duration: 3.160s, episode steps: 237, steps per second:  75, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 1.111000, mae: 17.560353, mean_q: -23.044059, mean_eps: 0.716540\n",
      "  378340/1200000: episode: 1503, duration: 3.678s, episode steps: 275, steps per second:  75, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.878519, mae: 17.890512, mean_q: -23.493224, mean_eps: 0.716348\n",
      "  378505/1200000: episode: 1504, duration: 2.267s, episode steps: 165, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.914124, mae: 17.771794, mean_q: -23.356248, mean_eps: 0.716183\n",
      "  378790/1200000: episode: 1505, duration: 3.714s, episode steps: 285, steps per second:  77, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 1.098763, mae: 17.864973, mean_q: -23.433638, mean_eps: 0.716015\n",
      "  379014/1200000: episode: 1506, duration: 2.916s, episode steps: 224, steps per second:  77, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 1.086201, mae: 17.804617, mean_q: -23.383793, mean_eps: 0.715824\n",
      "  379282/1200000: episode: 1507, duration: 3.632s, episode steps: 268, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 1.063131, mae: 17.720108, mean_q: -23.227726, mean_eps: 0.715639\n",
      "  379458/1200000: episode: 1508, duration: 2.459s, episode steps: 176, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.523 [0.000, 3.000],  loss: 0.812747, mae: 17.649103, mean_q: -23.182830, mean_eps: 0.715473\n",
      "  379635/1200000: episode: 1509, duration: 2.370s, episode steps: 177, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.805046, mae: 17.795054, mean_q: -23.404719, mean_eps: 0.715340\n",
      "  380025/1200000: episode: 1510, duration: 5.415s, episode steps: 390, steps per second:  72, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.996083, mae: 17.722525, mean_q: -23.270233, mean_eps: 0.715128\n",
      "  380248/1200000: episode: 1511, duration: 3.784s, episode steps: 223, steps per second:  59, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.850178, mae: 17.686522, mean_q: -23.233895, mean_eps: 0.714898\n",
      "  380486/1200000: episode: 1512, duration: 3.849s, episode steps: 238, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.729577, mae: 17.635573, mean_q: -23.187391, mean_eps: 0.714725\n",
      "  380730/1200000: episode: 1513, duration: 3.071s, episode steps: 244, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.980807, mae: 17.644457, mean_q: -23.154845, mean_eps: 0.714544\n",
      "  380903/1200000: episode: 1514, duration: 2.113s, episode steps: 173, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.861892, mae: 17.577687, mean_q: -23.054821, mean_eps: 0.714388\n",
      "  381209/1200000: episode: 1515, duration: 3.799s, episode steps: 306, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.987789, mae: 17.621072, mean_q: -23.123412, mean_eps: 0.714208\n",
      "  381410/1200000: episode: 1516, duration: 2.577s, episode steps: 201, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.777888, mae: 17.619816, mean_q: -23.169271, mean_eps: 0.714018\n",
      "  381600/1200000: episode: 1517, duration: 2.388s, episode steps: 190, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: 1.185896, mae: 17.747588, mean_q: -23.242341, mean_eps: 0.713872\n",
      "  381843/1200000: episode: 1518, duration: 2.959s, episode steps: 243, steps per second:  82, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 1.113871, mae: 17.487018, mean_q: -22.935853, mean_eps: 0.713709\n",
      "  382058/1200000: episode: 1519, duration: 2.677s, episode steps: 215, steps per second:  80, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.860233, mae: 17.598340, mean_q: -23.100763, mean_eps: 0.713537\n",
      "  382244/1200000: episode: 1520, duration: 2.305s, episode steps: 186, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 1.030228, mae: 17.571186, mean_q: -23.061817, mean_eps: 0.713387\n",
      "  382454/1200000: episode: 1521, duration: 2.633s, episode steps: 210, steps per second:  80, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 1.000935, mae: 17.683120, mean_q: -23.198827, mean_eps: 0.713239\n",
      "  382662/1200000: episode: 1522, duration: 2.580s, episode steps: 208, steps per second:  81, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 1.039638, mae: 17.659039, mean_q: -23.190086, mean_eps: 0.713082\n",
      "  382908/1200000: episode: 1523, duration: 3.116s, episode steps: 246, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.736 [0.000, 3.000],  loss: 1.010244, mae: 17.599929, mean_q: -23.095141, mean_eps: 0.712912\n",
      "  383083/1200000: episode: 1524, duration: 2.187s, episode steps: 175, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.878598, mae: 17.654565, mean_q: -23.174379, mean_eps: 0.712754\n",
      "  383252/1200000: episode: 1525, duration: 2.203s, episode steps: 169, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 1.435969, mae: 17.653584, mean_q: -23.114687, mean_eps: 0.712625\n",
      "  383496/1200000: episode: 1526, duration: 3.098s, episode steps: 244, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.879821, mae: 17.643206, mean_q: -23.154022, mean_eps: 0.712470\n",
      "  383684/1200000: episode: 1527, duration: 2.414s, episode steps: 188, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: 0.768623, mae: 17.700095, mean_q: -23.231541, mean_eps: 0.712308\n",
      "  383871/1200000: episode: 1528, duration: 2.313s, episode steps: 187, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 1.106464, mae: 17.424232, mean_q: -22.850556, mean_eps: 0.712167\n",
      "  384098/1200000: episode: 1529, duration: 2.828s, episode steps: 227, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.973401, mae: 17.594434, mean_q: -23.098927, mean_eps: 0.712012\n",
      "  384489/1200000: episode: 1530, duration: 4.809s, episode steps: 391, steps per second:  81, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.775935, mae: 17.568011, mean_q: -23.105301, mean_eps: 0.711780\n",
      "  384692/1200000: episode: 1531, duration: 2.631s, episode steps: 203, steps per second:  77, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.917801, mae: 17.640957, mean_q: -23.138250, mean_eps: 0.711558\n",
      "  384869/1200000: episode: 1532, duration: 2.200s, episode steps: 177, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.908631, mae: 17.551791, mean_q: -23.021133, mean_eps: 0.711415\n",
      "  385096/1200000: episode: 1533, duration: 2.839s, episode steps: 227, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.794664, mae: 17.611366, mean_q: -23.150132, mean_eps: 0.711264\n",
      "  385261/1200000: episode: 1534, duration: 2.070s, episode steps: 165, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.303 [0.000, 3.000],  loss: 0.874445, mae: 17.720683, mean_q: -23.285992, mean_eps: 0.711116\n",
      "  385565/1200000: episode: 1535, duration: 3.804s, episode steps: 304, steps per second:  80, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.858570, mae: 17.615512, mean_q: -23.130277, mean_eps: 0.710941\n",
      "  385741/1200000: episode: 1536, duration: 2.195s, episode steps: 176, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.862077, mae: 17.655576, mean_q: -23.187980, mean_eps: 0.710761\n",
      "  385912/1200000: episode: 1537, duration: 2.129s, episode steps: 171, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.751700, mae: 17.686846, mean_q: -23.224822, mean_eps: 0.710631\n",
      "  386206/1200000: episode: 1538, duration: 3.623s, episode steps: 294, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.900519, mae: 17.531073, mean_q: -23.029202, mean_eps: 0.710456\n",
      "  386520/1200000: episode: 1539, duration: 4.049s, episode steps: 314, steps per second:  78, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 1.007842, mae: 17.590113, mean_q: -23.089308, mean_eps: 0.710228\n",
      "  386829/1200000: episode: 1540, duration: 4.548s, episode steps: 309, steps per second:  68, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 1.153175, mae: 17.669335, mean_q: -23.178568, mean_eps: 0.709995\n",
      "  387184/1200000: episode: 1541, duration: 4.772s, episode steps: 355, steps per second:  74, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.817248, mae: 17.591883, mean_q: -23.128467, mean_eps: 0.709746\n",
      "  387418/1200000: episode: 1542, duration: 3.464s, episode steps: 234, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.972990, mae: 17.678820, mean_q: -23.221550, mean_eps: 0.709525\n",
      "  387730/1200000: episode: 1543, duration: 3.962s, episode steps: 312, steps per second:  79, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.882453, mae: 17.581035, mean_q: -23.103475, mean_eps: 0.709320\n",
      "  387938/1200000: episode: 1544, duration: 2.717s, episode steps: 208, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 0.910941, mae: 17.742563, mean_q: -23.304126, mean_eps: 0.709125\n",
      "  388139/1200000: episode: 1545, duration: 2.578s, episode steps: 201, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 1.060725, mae: 17.624682, mean_q: -23.125364, mean_eps: 0.708971\n",
      "  388304/1200000: episode: 1546, duration: 2.187s, episode steps: 165, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.809357, mae: 17.782738, mean_q: -23.350902, mean_eps: 0.708834\n",
      "  388592/1200000: episode: 1547, duration: 3.552s, episode steps: 288, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.903439, mae: 17.561400, mean_q: -23.063827, mean_eps: 0.708664\n",
      "  388885/1200000: episode: 1548, duration: 3.593s, episode steps: 293, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.813174, mae: 17.616080, mean_q: -23.162548, mean_eps: 0.708446\n",
      "  389113/1200000: episode: 1549, duration: 2.876s, episode steps: 228, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.834428, mae: 17.594249, mean_q: -23.126293, mean_eps: 0.708251\n",
      "  389283/1200000: episode: 1550, duration: 2.168s, episode steps: 170, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.983069, mae: 17.672524, mean_q: -23.208453, mean_eps: 0.708102\n",
      "  389569/1200000: episode: 1551, duration: 3.589s, episode steps: 286, steps per second:  80, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.987184, mae: 17.697112, mean_q: -23.219556, mean_eps: 0.707931\n",
      "  389744/1200000: episode: 1552, duration: 2.211s, episode steps: 175, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 1.008464, mae: 17.540814, mean_q: -23.019000, mean_eps: 0.707758\n",
      "  390081/1200000: episode: 1553, duration: 4.119s, episode steps: 337, steps per second:  82, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.852036, mae: 17.458538, mean_q: -22.920502, mean_eps: 0.707566\n",
      "  390280/1200000: episode: 1554, duration: 2.557s, episode steps: 199, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.696419, mae: 16.798613, mean_q: -22.082246, mean_eps: 0.707365\n",
      "  390647/1200000: episode: 1555, duration: 4.540s, episode steps: 367, steps per second:  81, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.726232, mae: 16.755192, mean_q: -22.008014, mean_eps: 0.707153\n",
      "  390814/1200000: episode: 1556, duration: 2.050s, episode steps: 167, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.479 [0.000, 3.000],  loss: 0.832738, mae: 16.692157, mean_q: -21.913038, mean_eps: 0.706952\n",
      "  391104/1200000: episode: 1557, duration: 3.540s, episode steps: 290, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.732705, mae: 16.866684, mean_q: -22.156923, mean_eps: 0.706781\n",
      "  391276/1200000: episode: 1558, duration: 2.207s, episode steps: 172, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.686 [0.000, 3.000],  loss: 0.736182, mae: 16.749796, mean_q: -21.984456, mean_eps: 0.706608\n",
      "  391550/1200000: episode: 1559, duration: 3.400s, episode steps: 274, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.879740, mae: 16.739786, mean_q: -21.954613, mean_eps: 0.706441\n",
      "  391728/1200000: episode: 1560, duration: 2.265s, episode steps: 178, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.816818, mae: 16.763930, mean_q: -22.035895, mean_eps: 0.706271\n",
      "  391949/1200000: episode: 1561, duration: 2.759s, episode steps: 221, steps per second:  80, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.768816, mae: 16.751532, mean_q: -21.985730, mean_eps: 0.706121\n",
      "  392117/1200000: episode: 1562, duration: 2.126s, episode steps: 168, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.953403, mae: 16.712478, mean_q: -21.907356, mean_eps: 0.705976\n",
      "  392409/1200000: episode: 1563, duration: 3.655s, episode steps: 292, steps per second:  80, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.798704, mae: 16.816912, mean_q: -22.069453, mean_eps: 0.705803\n",
      "  392748/1200000: episode: 1564, duration: 4.160s, episode steps: 339, steps per second:  81, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.871449, mae: 16.770075, mean_q: -21.997254, mean_eps: 0.705566\n",
      "  392931/1200000: episode: 1565, duration: 2.305s, episode steps: 183, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 1.017746, mae: 16.815046, mean_q: -22.036670, mean_eps: 0.705371\n",
      "  393105/1200000: episode: 1566, duration: 2.256s, episode steps: 174, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.947961, mae: 16.729292, mean_q: -21.983716, mean_eps: 0.705237\n",
      "  393314/1200000: episode: 1567, duration: 2.640s, episode steps: 209, steps per second:  79, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.599293, mae: 16.498933, mean_q: -21.692766, mean_eps: 0.705093\n",
      "  393496/1200000: episode: 1568, duration: 2.257s, episode steps: 182, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.895876, mae: 16.741132, mean_q: -21.944308, mean_eps: 0.704947\n",
      "  393673/1200000: episode: 1569, duration: 2.305s, episode steps: 177, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.762244, mae: 16.721423, mean_q: -21.974405, mean_eps: 0.704812\n",
      "  393837/1200000: episode: 1570, duration: 2.089s, episode steps: 164, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.799405, mae: 16.787627, mean_q: -22.034636, mean_eps: 0.704684\n",
      "  394149/1200000: episode: 1571, duration: 3.971s, episode steps: 312, steps per second:  79, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.816698, mae: 16.773058, mean_q: -22.019035, mean_eps: 0.704506\n",
      "  394314/1200000: episode: 1572, duration: 2.124s, episode steps: 165, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.881795, mae: 16.744097, mean_q: -21.983940, mean_eps: 0.704327\n",
      "  394537/1200000: episode: 1573, duration: 2.834s, episode steps: 223, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.752245, mae: 16.738294, mean_q: -22.010811, mean_eps: 0.704181\n",
      "  394710/1200000: episode: 1574, duration: 2.220s, episode steps: 173, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.928912, mae: 16.735381, mean_q: -21.947641, mean_eps: 0.704033\n",
      "  394936/1200000: episode: 1575, duration: 2.821s, episode steps: 226, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.327 [0.000, 3.000],  loss: 0.832201, mae: 16.761825, mean_q: -22.002262, mean_eps: 0.703883\n",
      "  395110/1200000: episode: 1576, duration: 2.235s, episode steps: 174, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.964626, mae: 16.763559, mean_q: -21.987754, mean_eps: 0.703733\n",
      "  395420/1200000: episode: 1577, duration: 3.863s, episode steps: 310, steps per second:  80, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.852060, mae: 16.772480, mean_q: -22.017980, mean_eps: 0.703552\n",
      "  395894/1200000: episode: 1578, duration: 5.833s, episode steps: 474, steps per second:  81, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.793790, mae: 16.729321, mean_q: -21.975274, mean_eps: 0.703258\n",
      "  396137/1200000: episode: 1579, duration: 3.026s, episode steps: 243, steps per second:  80, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.641679, mae: 16.706747, mean_q: -21.973836, mean_eps: 0.702989\n",
      "  396334/1200000: episode: 1580, duration: 2.544s, episode steps: 197, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.782 [0.000, 3.000],  loss: 0.836838, mae: 16.866906, mean_q: -22.146511, mean_eps: 0.702824\n",
      "  396613/1200000: episode: 1581, duration: 3.435s, episode steps: 279, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.807400, mae: 16.768129, mean_q: -22.025840, mean_eps: 0.702645\n",
      "  396792/1200000: episode: 1582, duration: 2.249s, episode steps: 179, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.850574, mae: 16.679137, mean_q: -21.895323, mean_eps: 0.702474\n",
      "  396972/1200000: episode: 1583, duration: 2.277s, episode steps: 180, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: 0.967008, mae: 16.705888, mean_q: -21.889269, mean_eps: 0.702339\n",
      "  397248/1200000: episode: 1584, duration: 3.511s, episode steps: 276, steps per second:  79, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.844704, mae: 16.687007, mean_q: -21.919154, mean_eps: 0.702168\n",
      "  397520/1200000: episode: 1585, duration: 3.330s, episode steps: 272, steps per second:  82, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 0.760818, mae: 16.674870, mean_q: -21.905876, mean_eps: 0.701962\n",
      "  397861/1200000: episode: 1586, duration: 4.241s, episode steps: 341, steps per second:  80, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.833270, mae: 16.861181, mean_q: -22.148720, mean_eps: 0.701732\n",
      "  398037/1200000: episode: 1587, duration: 2.229s, episode steps: 176, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.597 [0.000, 3.000],  loss: 0.911046, mae: 16.768982, mean_q: -22.009989, mean_eps: 0.701539\n",
      "  398207/1200000: episode: 1588, duration: 2.235s, episode steps: 170, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.864452, mae: 16.674166, mean_q: -21.856956, mean_eps: 0.701409\n",
      "  398519/1200000: episode: 1589, duration: 3.941s, episode steps: 312, steps per second:  79, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.863084, mae: 16.746409, mean_q: -21.952153, mean_eps: 0.701228\n",
      "  398695/1200000: episode: 1590, duration: 2.263s, episode steps: 176, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 0.886257, mae: 16.808808, mean_q: -22.040382, mean_eps: 0.701045\n",
      "  398867/1200000: episode: 1591, duration: 2.565s, episode steps: 172, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.262 [0.000, 3.000],  loss: 1.152829, mae: 16.717787, mean_q: -21.897906, mean_eps: 0.700915\n",
      "  399087/1200000: episode: 1592, duration: 2.918s, episode steps: 220, steps per second:  75, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.805327, mae: 16.748786, mean_q: -21.989936, mean_eps: 0.700768\n",
      "  399266/1200000: episode: 1593, duration: 2.351s, episode steps: 179, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.828855, mae: 16.725599, mean_q: -21.958565, mean_eps: 0.700618\n",
      "  399442/1200000: episode: 1594, duration: 2.301s, episode steps: 176, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 1.044461, mae: 16.697583, mean_q: -21.887930, mean_eps: 0.700485\n",
      "  399691/1200000: episode: 1595, duration: 3.213s, episode steps: 249, steps per second:  77, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.617660, mae: 16.819235, mean_q: -22.119213, mean_eps: 0.700325\n",
      "  399918/1200000: episode: 1596, duration: 3.036s, episode steps: 227, steps per second:  75, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.951153, mae: 16.695879, mean_q: -21.907051, mean_eps: 0.700147\n",
      "  400093/1200000: episode: 1597, duration: 2.263s, episode steps: 175, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.656246, mae: 16.404641, mean_q: -21.549823, mean_eps: 0.699996\n",
      "  400339/1200000: episode: 1598, duration: 3.232s, episode steps: 246, steps per second:  76, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.775469, mae: 16.289212, mean_q: -21.405887, mean_eps: 0.699838\n",
      "  400601/1200000: episode: 1599, duration: 3.421s, episode steps: 262, steps per second:  77, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.809470, mae: 16.369404, mean_q: -21.490354, mean_eps: 0.699648\n",
      "  401073/1200000: episode: 1600, duration: 6.090s, episode steps: 472, steps per second:  78, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.679250, mae: 16.322856, mean_q: -21.450405, mean_eps: 0.699373\n",
      "  401364/1200000: episode: 1601, duration: 3.583s, episode steps: 291, steps per second:  81, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.838569, mae: 16.261870, mean_q: -21.345936, mean_eps: 0.699086\n",
      "  401530/1200000: episode: 1602, duration: 2.135s, episode steps: 166, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.741251, mae: 16.241908, mean_q: -21.330532, mean_eps: 0.698915\n",
      "  401761/1200000: episode: 1603, duration: 3.326s, episode steps: 231, steps per second:  69, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.751636, mae: 16.416999, mean_q: -21.529545, mean_eps: 0.698766\n",
      "  402212/1200000: episode: 1604, duration: 7.321s, episode steps: 451, steps per second:  62, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.820338, mae: 16.321861, mean_q: -21.436746, mean_eps: 0.698511\n",
      "  402463/1200000: episode: 1605, duration: 3.813s, episode steps: 251, steps per second:  66, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.760045, mae: 16.413894, mean_q: -21.556806, mean_eps: 0.698247\n",
      "  402650/1200000: episode: 1606, duration: 2.780s, episode steps: 187, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: 0.838107, mae: 16.472998, mean_q: -21.594155, mean_eps: 0.698083\n",
      "  402887/1200000: episode: 1607, duration: 3.500s, episode steps: 237, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.630384, mae: 16.237570, mean_q: -21.361504, mean_eps: 0.697924\n",
      "  403125/1200000: episode: 1608, duration: 3.316s, episode steps: 238, steps per second:  72, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.751184, mae: 16.246087, mean_q: -21.327220, mean_eps: 0.697746\n",
      "  403466/1200000: episode: 1609, duration: 4.866s, episode steps: 341, steps per second:  70, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.727499, mae: 16.286331, mean_q: -21.396956, mean_eps: 0.697529\n",
      "  403737/1200000: episode: 1610, duration: 3.877s, episode steps: 271, steps per second:  70, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.726214, mae: 16.382669, mean_q: -21.515820, mean_eps: 0.697299\n",
      "  403907/1200000: episode: 1611, duration: 2.448s, episode steps: 170, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.651682, mae: 16.307790, mean_q: -21.426895, mean_eps: 0.697134\n",
      "  404078/1200000: episode: 1612, duration: 2.488s, episode steps: 171, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.695557, mae: 16.322096, mean_q: -21.455955, mean_eps: 0.697006\n",
      "  404329/1200000: episode: 1613, duration: 3.538s, episode steps: 251, steps per second:  71, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.809901, mae: 16.202462, mean_q: -21.275071, mean_eps: 0.696848\n",
      "  404679/1200000: episode: 1614, duration: 4.907s, episode steps: 350, steps per second:  71, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.795171, mae: 16.290173, mean_q: -21.388975, mean_eps: 0.696622\n",
      "  404892/1200000: episode: 1615, duration: 3.022s, episode steps: 213, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.747626, mae: 16.302934, mean_q: -21.413771, mean_eps: 0.696411\n",
      "  405083/1200000: episode: 1616, duration: 2.702s, episode steps: 191, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 0.762687, mae: 16.389420, mean_q: -21.508050, mean_eps: 0.696260\n",
      "  405263/1200000: episode: 1617, duration: 2.581s, episode steps: 180, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.597145, mae: 16.354749, mean_q: -21.510034, mean_eps: 0.696121\n",
      "  405444/1200000: episode: 1618, duration: 2.614s, episode steps: 181, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.293 [0.000, 3.000],  loss: 0.697992, mae: 16.374624, mean_q: -21.516379, mean_eps: 0.695985\n",
      "  405620/1200000: episode: 1619, duration: 2.493s, episode steps: 176, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.858850, mae: 16.279906, mean_q: -21.376443, mean_eps: 0.695851\n",
      "  405831/1200000: episode: 1620, duration: 2.932s, episode steps: 211, steps per second:  72, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.586271, mae: 16.315941, mean_q: -21.473496, mean_eps: 0.695706\n",
      "  406014/1200000: episode: 1621, duration: 2.664s, episode steps: 183, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.894132, mae: 16.351540, mean_q: -21.475884, mean_eps: 0.695559\n",
      "  406320/1200000: episode: 1622, duration: 4.310s, episode steps: 306, steps per second:  71, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.880929, mae: 16.260656, mean_q: -21.339446, mean_eps: 0.695375\n",
      "  406499/1200000: episode: 1623, duration: 2.535s, episode steps: 179, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.425 [0.000, 3.000],  loss: 0.766536, mae: 16.306231, mean_q: -21.428179, mean_eps: 0.695193\n",
      "  406676/1200000: episode: 1624, duration: 2.525s, episode steps: 177, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 0.714690, mae: 16.338086, mean_q: -21.458057, mean_eps: 0.695060\n",
      "  407013/1200000: episode: 1625, duration: 4.818s, episode steps: 337, steps per second:  70, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.871383, mae: 16.281939, mean_q: -21.357244, mean_eps: 0.694867\n",
      "  407214/1200000: episode: 1626, duration: 2.952s, episode steps: 201, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.785514, mae: 16.192413, mean_q: -21.245738, mean_eps: 0.694665\n",
      "  407546/1200000: episode: 1627, duration: 4.795s, episode steps: 332, steps per second:  69, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.732381, mae: 16.304539, mean_q: -21.421021, mean_eps: 0.694465\n",
      "  407745/1200000: episode: 1628, duration: 2.927s, episode steps: 199, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.721734, mae: 16.294263, mean_q: -21.392428, mean_eps: 0.694266\n",
      "  407921/1200000: episode: 1629, duration: 2.644s, episode steps: 176, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.642988, mae: 16.336791, mean_q: -21.456178, mean_eps: 0.694126\n",
      "  408103/1200000: episode: 1630, duration: 2.593s, episode steps: 182, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: 0.634957, mae: 16.201485, mean_q: -21.303677, mean_eps: 0.693991\n",
      "  408304/1200000: episode: 1631, duration: 2.875s, episode steps: 201, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 0.766073, mae: 16.298999, mean_q: -21.414335, mean_eps: 0.693848\n",
      "  408551/1200000: episode: 1632, duration: 3.597s, episode steps: 247, steps per second:  69, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.753938, mae: 16.294317, mean_q: -21.386410, mean_eps: 0.693680\n",
      "  408725/1200000: episode: 1633, duration: 2.462s, episode steps: 174, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 0.932799, mae: 16.231654, mean_q: -21.310488, mean_eps: 0.693522\n",
      "  409153/1200000: episode: 1634, duration: 6.000s, episode steps: 428, steps per second:  71, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.670773, mae: 16.275131, mean_q: -21.367434, mean_eps: 0.693296\n",
      "  409367/1200000: episode: 1635, duration: 3.034s, episode steps: 214, steps per second:  71, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.854881, mae: 16.343586, mean_q: -21.474151, mean_eps: 0.693055\n",
      "  409533/1200000: episode: 1636, duration: 2.439s, episode steps: 166, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.611603, mae: 16.173109, mean_q: -21.282453, mean_eps: 0.692913\n",
      "  409768/1200000: episode: 1637, duration: 3.377s, episode steps: 235, steps per second:  70, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.611666, mae: 16.265536, mean_q: -21.391982, mean_eps: 0.692762\n",
      "  409949/1200000: episode: 1638, duration: 2.622s, episode steps: 181, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.354 [0.000, 3.000],  loss: 0.739420, mae: 16.219873, mean_q: -21.303963, mean_eps: 0.692606\n",
      "  410181/1200000: episode: 1639, duration: 3.358s, episode steps: 232, steps per second:  69, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.698536, mae: 15.713792, mean_q: -20.649733, mean_eps: 0.692452\n",
      "  410344/1200000: episode: 1640, duration: 2.407s, episode steps: 163, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.319 [0.000, 3.000],  loss: 0.662231, mae: 15.709120, mean_q: -20.650717, mean_eps: 0.692304\n",
      "  410609/1200000: episode: 1641, duration: 3.758s, episode steps: 265, steps per second:  71, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.595177, mae: 15.790228, mean_q: -20.755954, mean_eps: 0.692143\n",
      "  410825/1200000: episode: 1642, duration: 3.411s, episode steps: 216, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.706741, mae: 15.721209, mean_q: -20.653213, mean_eps: 0.691963\n",
      "  411002/1200000: episode: 1643, duration: 2.762s, episode steps: 177, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.608718, mae: 15.700896, mean_q: -20.645043, mean_eps: 0.691815\n",
      "  411223/1200000: episode: 1644, duration: 4.683s, episode steps: 221, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.773060, mae: 15.671924, mean_q: -20.553640, mean_eps: 0.691666\n",
      "  411564/1200000: episode: 1645, duration: 5.722s, episode steps: 341, steps per second:  60, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.666954, mae: 15.655043, mean_q: -20.544698, mean_eps: 0.691455\n",
      "  411743/1200000: episode: 1646, duration: 2.862s, episode steps: 179, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.637 [0.000, 3.000],  loss: 0.930691, mae: 15.589854, mean_q: -20.432515, mean_eps: 0.691260\n",
      "  412029/1200000: episode: 1647, duration: 3.823s, episode steps: 286, steps per second:  75, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.616617, mae: 15.600574, mean_q: -20.498080, mean_eps: 0.691086\n",
      "  412197/1200000: episode: 1648, duration: 2.710s, episode steps: 168, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.762 [0.000, 3.000],  loss: 0.570515, mae: 15.672918, mean_q: -20.624683, mean_eps: 0.690916\n",
      "  412536/1200000: episode: 1649, duration: 4.558s, episode steps: 339, steps per second:  74, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.740658, mae: 15.683560, mean_q: -20.580850, mean_eps: 0.690725\n",
      "  412770/1200000: episode: 1650, duration: 3.603s, episode steps: 234, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.778550, mae: 15.682056, mean_q: -20.569793, mean_eps: 0.690511\n",
      "  413144/1200000: episode: 1651, duration: 5.703s, episode steps: 374, steps per second:  66, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.786941, mae: 15.678832, mean_q: -20.560014, mean_eps: 0.690283\n",
      "  413390/1200000: episode: 1652, duration: 4.160s, episode steps: 246, steps per second:  59, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.634997, mae: 15.581918, mean_q: -20.465143, mean_eps: 0.690050\n",
      "  413576/1200000: episode: 1653, duration: 2.467s, episode steps: 186, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.695047, mae: 15.781415, mean_q: -20.703211, mean_eps: 0.689888\n",
      "  413761/1200000: episode: 1654, duration: 2.488s, episode steps: 185, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.670 [0.000, 3.000],  loss: 0.662423, mae: 15.741011, mean_q: -20.683294, mean_eps: 0.689749\n",
      "  413984/1200000: episode: 1655, duration: 2.829s, episode steps: 223, steps per second:  79, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.753466, mae: 15.699264, mean_q: -20.591159, mean_eps: 0.689596\n",
      "  414208/1200000: episode: 1656, duration: 2.918s, episode steps: 224, steps per second:  77, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.627577, mae: 15.682324, mean_q: -20.594516, mean_eps: 0.689428\n",
      "  414477/1200000: episode: 1657, duration: 3.496s, episode steps: 269, steps per second:  77, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.685914, mae: 15.593380, mean_q: -20.479141, mean_eps: 0.689244\n",
      "  414668/1200000: episode: 1658, duration: 2.497s, episode steps: 191, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.743 [0.000, 3.000],  loss: 0.518253, mae: 15.772834, mean_q: -20.759637, mean_eps: 0.689071\n",
      "  414833/1200000: episode: 1659, duration: 2.203s, episode steps: 165, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.693983, mae: 15.596368, mean_q: -20.485384, mean_eps: 0.688937\n",
      "  415215/1200000: episode: 1660, duration: 4.965s, episode steps: 382, steps per second:  77, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.620539, mae: 15.639328, mean_q: -20.536464, mean_eps: 0.688732\n",
      "  415394/1200000: episode: 1661, duration: 2.424s, episode steps: 179, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.496731, mae: 15.657446, mean_q: -20.598185, mean_eps: 0.688522\n",
      "  415610/1200000: episode: 1662, duration: 3.283s, episode steps: 216, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.736 [0.000, 3.000],  loss: 0.708701, mae: 15.719696, mean_q: -20.631539, mean_eps: 0.688374\n",
      "  415783/1200000: episode: 1663, duration: 2.270s, episode steps: 173, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.628393, mae: 15.665037, mean_q: -20.570318, mean_eps: 0.688228\n",
      "  415955/1200000: episode: 1664, duration: 2.343s, episode steps: 172, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.760489, mae: 15.723004, mean_q: -20.624685, mean_eps: 0.688099\n",
      "  416152/1200000: episode: 1665, duration: 2.551s, episode steps: 197, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.569 [0.000, 3.000],  loss: 0.767488, mae: 15.718606, mean_q: -20.617627, mean_eps: 0.687960\n",
      "  416460/1200000: episode: 1666, duration: 4.070s, episode steps: 308, steps per second:  76, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.738484, mae: 15.640178, mean_q: -20.524519, mean_eps: 0.687771\n",
      "  416663/1200000: episode: 1667, duration: 2.756s, episode steps: 203, steps per second:  74, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.823941, mae: 15.588239, mean_q: -20.425463, mean_eps: 0.687579\n",
      "  416869/1200000: episode: 1668, duration: 2.702s, episode steps: 206, steps per second:  76, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.715515, mae: 15.615875, mean_q: -20.493576, mean_eps: 0.687426\n",
      "  417175/1200000: episode: 1669, duration: 3.980s, episode steps: 306, steps per second:  77, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.590883, mae: 15.695273, mean_q: -20.635763, mean_eps: 0.687234\n",
      "  417407/1200000: episode: 1670, duration: 3.031s, episode steps: 232, steps per second:  77, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.698980, mae: 15.641599, mean_q: -20.523969, mean_eps: 0.687032\n",
      "  417584/1200000: episode: 1671, duration: 2.394s, episode steps: 177, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.695 [0.000, 3.000],  loss: 0.631448, mae: 15.664713, mean_q: -20.582574, mean_eps: 0.686879\n",
      "  417795/1200000: episode: 1672, duration: 2.807s, episode steps: 211, steps per second:  75, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.608871, mae: 15.815747, mean_q: -20.784637, mean_eps: 0.686733\n",
      "  418066/1200000: episode: 1673, duration: 3.590s, episode steps: 271, steps per second:  75, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.674310, mae: 15.655629, mean_q: -20.564232, mean_eps: 0.686553\n",
      "  418354/1200000: episode: 1674, duration: 3.767s, episode steps: 288, steps per second:  76, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.653241, mae: 15.682678, mean_q: -20.599823, mean_eps: 0.686343\n",
      "  418530/1200000: episode: 1675, duration: 2.427s, episode steps: 176, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.710344, mae: 15.710179, mean_q: -20.616794, mean_eps: 0.686169\n",
      "  418722/1200000: episode: 1676, duration: 2.625s, episode steps: 192, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.744036, mae: 15.756018, mean_q: -20.712699, mean_eps: 0.686031\n",
      "  419055/1200000: episode: 1677, duration: 4.863s, episode steps: 333, steps per second:  68, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.508465, mae: 15.610666, mean_q: -20.519241, mean_eps: 0.685834\n",
      "  419267/1200000: episode: 1678, duration: 3.097s, episode steps: 212, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.450350, mae: 15.671628, mean_q: -20.615416, mean_eps: 0.685630\n",
      "  419494/1200000: episode: 1679, duration: 4.003s, episode steps: 227, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.719746, mae: 15.651259, mean_q: -20.549388, mean_eps: 0.685465\n",
      "  419731/1200000: episode: 1680, duration: 3.369s, episode steps: 237, steps per second:  70, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.520687, mae: 15.689957, mean_q: -20.634549, mean_eps: 0.685291\n",
      "  419935/1200000: episode: 1681, duration: 2.692s, episode steps: 204, steps per second:  76, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.703482, mae: 15.709348, mean_q: -20.617097, mean_eps: 0.685126\n",
      "  420106/1200000: episode: 1682, duration: 2.375s, episode steps: 171, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.620893, mae: 15.334309, mean_q: -20.141297, mean_eps: 0.684985\n",
      "  420283/1200000: episode: 1683, duration: 2.539s, episode steps: 177, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: 0.435345, mae: 15.262699, mean_q: -20.076416, mean_eps: 0.684855\n",
      "  420597/1200000: episode: 1684, duration: 4.208s, episode steps: 314, steps per second:  75, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.550652, mae: 15.153088, mean_q: -19.917339, mean_eps: 0.684670\n",
      "  420778/1200000: episode: 1685, duration: 2.543s, episode steps: 181, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.646 [0.000, 3.000],  loss: 0.544345, mae: 15.226015, mean_q: -20.007230, mean_eps: 0.684485\n",
      "  421052/1200000: episode: 1686, duration: 3.699s, episode steps: 274, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.665884, mae: 15.172739, mean_q: -19.911694, mean_eps: 0.684314\n",
      "  421223/1200000: episode: 1687, duration: 2.311s, episode steps: 171, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.676561, mae: 15.126865, mean_q: -19.841251, mean_eps: 0.684147\n",
      "  421530/1200000: episode: 1688, duration: 4.148s, episode steps: 307, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.702499, mae: 15.236161, mean_q: -19.993407, mean_eps: 0.683968\n",
      "  421848/1200000: episode: 1689, duration: 4.450s, episode steps: 318, steps per second:  71, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.577055, mae: 15.116878, mean_q: -19.867615, mean_eps: 0.683734\n",
      "  422021/1200000: episode: 1690, duration: 2.353s, episode steps: 173, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.537403, mae: 15.187644, mean_q: -19.957387, mean_eps: 0.683550\n",
      "  422379/1200000: episode: 1691, duration: 4.808s, episode steps: 358, steps per second:  74, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.573123, mae: 15.163339, mean_q: -19.920290, mean_eps: 0.683350\n",
      "  422561/1200000: episode: 1692, duration: 2.582s, episode steps: 182, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.665 [0.000, 3.000],  loss: 0.664822, mae: 15.280646, mean_q: -20.070190, mean_eps: 0.683148\n",
      "  422861/1200000: episode: 1693, duration: 3.876s, episode steps: 300, steps per second:  77, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.642541, mae: 15.164469, mean_q: -19.911195, mean_eps: 0.682967\n",
      "  423025/1200000: episode: 1694, duration: 2.251s, episode steps: 164, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.625436, mae: 15.223250, mean_q: -19.975551, mean_eps: 0.682793\n",
      "  423243/1200000: episode: 1695, duration: 2.971s, episode steps: 218, steps per second:  73, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.731165, mae: 15.122860, mean_q: -19.838232, mean_eps: 0.682650\n",
      "  423607/1200000: episode: 1696, duration: 4.870s, episode steps: 364, steps per second:  75, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.582309, mae: 15.241143, mean_q: -20.013592, mean_eps: 0.682432\n",
      "  423879/1200000: episode: 1697, duration: 3.697s, episode steps: 272, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.702338, mae: 15.251382, mean_q: -20.029634, mean_eps: 0.682193\n",
      "  424098/1200000: episode: 1698, duration: 2.954s, episode steps: 219, steps per second:  74, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.696723, mae: 15.206778, mean_q: -19.977583, mean_eps: 0.682009\n",
      "  424278/1200000: episode: 1699, duration: 2.495s, episode steps: 180, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.486377, mae: 15.196962, mean_q: -19.996041, mean_eps: 0.681859\n",
      "  424450/1200000: episode: 1700, duration: 2.384s, episode steps: 172, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.609295, mae: 15.197710, mean_q: -19.960255, mean_eps: 0.681727\n",
      "  424701/1200000: episode: 1701, duration: 3.475s, episode steps: 251, steps per second:  72, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.571371, mae: 15.167761, mean_q: -19.922792, mean_eps: 0.681569\n",
      "  424874/1200000: episode: 1702, duration: 2.355s, episode steps: 173, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.444805, mae: 15.060431, mean_q: -19.791189, mean_eps: 0.681410\n",
      "  425179/1200000: episode: 1703, duration: 4.164s, episode steps: 305, steps per second:  73, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.548792, mae: 15.174712, mean_q: -19.957216, mean_eps: 0.681231\n",
      "  425524/1200000: episode: 1704, duration: 4.576s, episode steps: 345, steps per second:  75, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.594090, mae: 15.110558, mean_q: -19.849381, mean_eps: 0.680987\n",
      "  425730/1200000: episode: 1705, duration: 2.815s, episode steps: 206, steps per second:  73, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.630885, mae: 15.278270, mean_q: -20.051877, mean_eps: 0.680780\n",
      "  425917/1200000: episode: 1706, duration: 2.597s, episode steps: 187, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.478740, mae: 15.226292, mean_q: -20.020456, mean_eps: 0.680633\n",
      "  426305/1200000: episode: 1707, duration: 5.259s, episode steps: 388, steps per second:  74, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.636035, mae: 15.173244, mean_q: -19.935690, mean_eps: 0.680417\n",
      "  426539/1200000: episode: 1708, duration: 3.165s, episode steps: 234, steps per second:  74, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.780848, mae: 15.218065, mean_q: -19.947781, mean_eps: 0.680184\n",
      "  426844/1200000: episode: 1709, duration: 4.149s, episode steps: 305, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.712290, mae: 15.144026, mean_q: -19.867853, mean_eps: 0.679982\n",
      "  427030/1200000: episode: 1710, duration: 2.602s, episode steps: 186, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.631050, mae: 15.146293, mean_q: -19.909479, mean_eps: 0.679798\n",
      "  427209/1200000: episode: 1711, duration: 2.457s, episode steps: 179, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.446965, mae: 15.135419, mean_q: -19.904595, mean_eps: 0.679661\n",
      "  427443/1200000: episode: 1712, duration: 3.256s, episode steps: 234, steps per second:  72, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.747757, mae: 15.185904, mean_q: -19.923554, mean_eps: 0.679506\n",
      "  427776/1200000: episode: 1713, duration: 4.681s, episode steps: 333, steps per second:  71, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.633380, mae: 15.238205, mean_q: -20.010915, mean_eps: 0.679293\n",
      "  427983/1200000: episode: 1714, duration: 2.787s, episode steps: 207, steps per second:  74, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.569677, mae: 15.193295, mean_q: -19.962626, mean_eps: 0.679091\n",
      "  428286/1200000: episode: 1715, duration: 4.085s, episode steps: 303, steps per second:  74, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.651079, mae: 15.166546, mean_q: -19.914697, mean_eps: 0.678899\n",
      "  428545/1200000: episode: 1716, duration: 3.566s, episode steps: 259, steps per second:  73, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.611862, mae: 15.103984, mean_q: -19.828309, mean_eps: 0.678689\n",
      "  428767/1200000: episode: 1717, duration: 3.026s, episode steps: 222, steps per second:  73, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.620294, mae: 15.226504, mean_q: -20.017376, mean_eps: 0.678508\n",
      "  428963/1200000: episode: 1718, duration: 2.684s, episode steps: 196, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.684771, mae: 15.115456, mean_q: -19.833420, mean_eps: 0.678352\n",
      "  429169/1200000: episode: 1719, duration: 2.941s, episode steps: 206, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.786215, mae: 15.226445, mean_q: -19.954964, mean_eps: 0.678201\n",
      "  429437/1200000: episode: 1720, duration: 3.680s, episode steps: 268, steps per second:  73, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.688305, mae: 15.149869, mean_q: -19.878763, mean_eps: 0.678023\n",
      "  429635/1200000: episode: 1721, duration: 2.714s, episode steps: 198, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.374 [0.000, 3.000],  loss: 0.610605, mae: 15.287395, mean_q: -20.057603, mean_eps: 0.677848\n",
      "  429812/1200000: episode: 1722, duration: 2.425s, episode steps: 177, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.797 [0.000, 3.000],  loss: 0.664771, mae: 15.146231, mean_q: -19.881757, mean_eps: 0.677708\n",
      "  430092/1200000: episode: 1723, duration: 3.753s, episode steps: 280, steps per second:  75, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.587527, mae: 15.064511, mean_q: -19.765904, mean_eps: 0.677536\n",
      "  430259/1200000: episode: 1724, duration: 2.395s, episode steps: 167, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.525482, mae: 14.692232, mean_q: -19.337869, mean_eps: 0.677369\n",
      "  430737/1200000: episode: 1725, duration: 6.465s, episode steps: 478, steps per second:  74, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.616772, mae: 14.721599, mean_q: -19.322153, mean_eps: 0.677127\n",
      "  430985/1200000: episode: 1726, duration: 3.386s, episode steps: 248, steps per second:  73, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.589727, mae: 14.658272, mean_q: -19.236368, mean_eps: 0.676855\n",
      "  431450/1200000: episode: 1727, duration: 6.337s, episode steps: 465, steps per second:  73, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.595468, mae: 14.848830, mean_q: -19.496656, mean_eps: 0.676587\n",
      "  431788/1200000: episode: 1728, duration: 4.631s, episode steps: 338, steps per second:  73, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.555327, mae: 14.699913, mean_q: -19.300328, mean_eps: 0.676286\n",
      "  432003/1200000: episode: 1729, duration: 3.029s, episode steps: 215, steps per second:  71, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.532571, mae: 14.686789, mean_q: -19.299613, mean_eps: 0.676079\n",
      "  432199/1200000: episode: 1730, duration: 2.701s, episode steps: 196, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.544813, mae: 14.662412, mean_q: -19.261890, mean_eps: 0.675925\n",
      "  432420/1200000: episode: 1731, duration: 3.083s, episode steps: 221, steps per second:  72, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.629852, mae: 14.730815, mean_q: -19.330530, mean_eps: 0.675768\n",
      "  432590/1200000: episode: 1732, duration: 2.355s, episode steps: 170, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.817582, mae: 14.756363, mean_q: -19.330978, mean_eps: 0.675622\n",
      "  432823/1200000: episode: 1733, duration: 3.185s, episode steps: 233, steps per second:  73, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 0.551997, mae: 14.774327, mean_q: -19.394335, mean_eps: 0.675471\n",
      "  433116/1200000: episode: 1734, duration: 4.127s, episode steps: 293, steps per second:  71, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.616349, mae: 14.681266, mean_q: -19.277490, mean_eps: 0.675273\n",
      "  433320/1200000: episode: 1735, duration: 2.769s, episode steps: 204, steps per second:  74, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.533383, mae: 14.783704, mean_q: -19.408088, mean_eps: 0.675087\n",
      "  433560/1200000: episode: 1736, duration: 3.310s, episode steps: 240, steps per second:  73, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.644054, mae: 14.778515, mean_q: -19.401364, mean_eps: 0.674920\n",
      "  433731/1200000: episode: 1737, duration: 2.485s, episode steps: 171, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.613113, mae: 14.877103, mean_q: -19.554962, mean_eps: 0.674766\n",
      "  433911/1200000: episode: 1738, duration: 2.510s, episode steps: 180, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.477129, mae: 14.715490, mean_q: -19.345537, mean_eps: 0.674635\n",
      "  434158/1200000: episode: 1739, duration: 3.335s, episode steps: 247, steps per second:  74, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.533222, mae: 14.710861, mean_q: -19.316729, mean_eps: 0.674474\n",
      "  434321/1200000: episode: 1740, duration: 2.277s, episode steps: 163, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.628879, mae: 14.843739, mean_q: -19.476425, mean_eps: 0.674321\n",
      "  434593/1200000: episode: 1741, duration: 3.766s, episode steps: 272, steps per second:  72, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.580378, mae: 14.713085, mean_q: -19.314153, mean_eps: 0.674158\n",
      "  434770/1200000: episode: 1742, duration: 2.455s, episode steps: 177, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.825 [0.000, 3.000],  loss: 0.463437, mae: 14.686800, mean_q: -19.302887, mean_eps: 0.673989\n",
      "  435044/1200000: episode: 1743, duration: 4.192s, episode steps: 274, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.764708, mae: 14.825127, mean_q: -19.390820, mean_eps: 0.673820\n",
      "  435209/1200000: episode: 1744, duration: 2.643s, episode steps: 165, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.611086, mae: 14.706205, mean_q: -19.276665, mean_eps: 0.673655\n",
      "  435403/1200000: episode: 1745, duration: 3.054s, episode steps: 194, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.763 [0.000, 3.000],  loss: 0.546736, mae: 14.810996, mean_q: -19.434659, mean_eps: 0.673521\n",
      "  435852/1200000: episode: 1746, duration: 6.803s, episode steps: 449, steps per second:  66, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.597507, mae: 14.740989, mean_q: -19.354579, mean_eps: 0.673280\n",
      "  436148/1200000: episode: 1747, duration: 4.569s, episode steps: 296, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.650489, mae: 14.747410, mean_q: -19.337069, mean_eps: 0.673000\n",
      "  436501/1200000: episode: 1748, duration: 5.387s, episode steps: 353, steps per second:  66, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.539016, mae: 14.715633, mean_q: -19.322751, mean_eps: 0.672757\n",
      "  436803/1200000: episode: 1749, duration: 4.702s, episode steps: 302, steps per second:  64, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.561927, mae: 14.704090, mean_q: -19.300826, mean_eps: 0.672511\n",
      "  437072/1200000: episode: 1750, duration: 4.092s, episode steps: 269, steps per second:  66, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.622409, mae: 14.768274, mean_q: -19.360405, mean_eps: 0.672297\n",
      "  437257/1200000: episode: 1751, duration: 2.986s, episode steps: 185, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: 0.469632, mae: 14.753654, mean_q: -19.388747, mean_eps: 0.672127\n",
      "  437426/1200000: episode: 1752, duration: 2.619s, episode steps: 169, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 0.718787, mae: 14.866241, mean_q: -19.502911, mean_eps: 0.671994\n",
      "  437889/1200000: episode: 1753, duration: 7.156s, episode steps: 463, steps per second:  65, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.613039, mae: 14.804090, mean_q: -19.415614, mean_eps: 0.671757\n",
      "  438160/1200000: episode: 1754, duration: 4.177s, episode steps: 271, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.634344, mae: 14.718188, mean_q: -19.324547, mean_eps: 0.671482\n",
      "  438334/1200000: episode: 1755, duration: 2.792s, episode steps: 174, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.492499, mae: 14.714937, mean_q: -19.330555, mean_eps: 0.671315\n",
      "  438517/1200000: episode: 1756, duration: 2.864s, episode steps: 183, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 0.621335, mae: 14.805117, mean_q: -19.447248, mean_eps: 0.671181\n",
      "  438866/1200000: episode: 1757, duration: 5.310s, episode steps: 349, steps per second:  66, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.566174, mae: 14.776802, mean_q: -19.386027, mean_eps: 0.670982\n",
      "  439035/1200000: episode: 1758, duration: 2.661s, episode steps: 169, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.578256, mae: 14.700590, mean_q: -19.316266, mean_eps: 0.670787\n",
      "  439287/1200000: episode: 1759, duration: 3.866s, episode steps: 252, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.539320, mae: 14.697009, mean_q: -19.292153, mean_eps: 0.670630\n",
      "  439504/1200000: episode: 1760, duration: 3.373s, episode steps: 217, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.806966, mae: 14.768354, mean_q: -19.325561, mean_eps: 0.670454\n",
      "  439720/1200000: episode: 1761, duration: 3.360s, episode steps: 216, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.573209, mae: 14.675407, mean_q: -19.281068, mean_eps: 0.670291\n",
      "  439944/1200000: episode: 1762, duration: 3.522s, episode steps: 224, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.758365, mae: 14.840762, mean_q: -19.433778, mean_eps: 0.670126\n",
      "  440140/1200000: episode: 1763, duration: 3.030s, episode steps: 196, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.740 [0.000, 3.000],  loss: 0.972232, mae: 14.811578, mean_q: -19.370196, mean_eps: 0.669969\n",
      "  440304/1200000: episode: 1764, duration: 2.618s, episode steps: 164, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.711118, mae: 14.888764, mean_q: -19.515370, mean_eps: 0.669834\n",
      "  440551/1200000: episode: 1765, duration: 3.858s, episode steps: 247, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.545956, mae: 14.741802, mean_q: -19.361457, mean_eps: 0.669680\n",
      "  440740/1200000: episode: 1766, duration: 2.961s, episode steps: 189, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.564979, mae: 14.897008, mean_q: -19.585919, mean_eps: 0.669516\n",
      "  441091/1200000: episode: 1767, duration: 5.400s, episode steps: 351, steps per second:  65, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.668345, mae: 14.815819, mean_q: -19.426254, mean_eps: 0.669314\n",
      "  441257/1200000: episode: 1768, duration: 2.702s, episode steps: 166, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.633080, mae: 14.720746, mean_q: -19.313178, mean_eps: 0.669120\n",
      "  441458/1200000: episode: 1769, duration: 3.209s, episode steps: 201, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.624463, mae: 14.736990, mean_q: -19.320744, mean_eps: 0.668982\n",
      "  441728/1200000: episode: 1770, duration: 4.155s, episode steps: 270, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.723087, mae: 14.769261, mean_q: -19.348472, mean_eps: 0.668806\n",
      "  442152/1200000: episode: 1771, duration: 6.573s, episode steps: 424, steps per second:  65, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.642357, mae: 14.840881, mean_q: -19.476479, mean_eps: 0.668545\n",
      "  442387/1200000: episode: 1772, duration: 3.607s, episode steps: 235, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.522328, mae: 14.794498, mean_q: -19.432887, mean_eps: 0.668298\n",
      "  442663/1200000: episode: 1773, duration: 4.273s, episode steps: 276, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.541422, mae: 14.771812, mean_q: -19.388387, mean_eps: 0.668107\n",
      "  442850/1200000: episode: 1774, duration: 2.986s, episode steps: 187, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.695 [0.000, 3.000],  loss: 0.590937, mae: 14.771892, mean_q: -19.396713, mean_eps: 0.667933\n",
      "  443095/1200000: episode: 1775, duration: 3.805s, episode steps: 245, steps per second:  64, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.570126, mae: 14.785997, mean_q: -19.392913, mean_eps: 0.667771\n",
      "  443315/1200000: episode: 1776, duration: 3.501s, episode steps: 220, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.652081, mae: 14.833896, mean_q: -19.455290, mean_eps: 0.667597\n",
      "  443610/1200000: episode: 1777, duration: 4.675s, episode steps: 295, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.566592, mae: 14.750465, mean_q: -19.353266, mean_eps: 0.667404\n",
      "  443779/1200000: episode: 1778, duration: 2.691s, episode steps: 169, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.685370, mae: 14.801884, mean_q: -19.373571, mean_eps: 0.667230\n",
      "  443942/1200000: episode: 1779, duration: 2.596s, episode steps: 163, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.699 [0.000, 3.000],  loss: 0.453789, mae: 14.551824, mean_q: -19.118744, mean_eps: 0.667105\n",
      "  444345/1200000: episode: 1780, duration: 6.261s, episode steps: 403, steps per second:  64, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.542948, mae: 14.728893, mean_q: -19.347781, mean_eps: 0.666893\n",
      "  444519/1200000: episode: 1781, duration: 2.751s, episode steps: 174, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.701 [0.000, 3.000],  loss: 0.667608, mae: 14.639773, mean_q: -19.188065, mean_eps: 0.666676\n",
      "  444739/1200000: episode: 1782, duration: 3.485s, episode steps: 220, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.713524, mae: 14.762151, mean_q: -19.338673, mean_eps: 0.666529\n",
      "  444908/1200000: episode: 1783, duration: 2.714s, episode steps: 169, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.638044, mae: 14.851055, mean_q: -19.481565, mean_eps: 0.666383\n",
      "  445223/1200000: episode: 1784, duration: 4.993s, episode steps: 315, steps per second:  63, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.556432, mae: 14.782838, mean_q: -19.419794, mean_eps: 0.666201\n",
      "  445436/1200000: episode: 1785, duration: 3.277s, episode steps: 213, steps per second:  65, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.680185, mae: 14.792191, mean_q: -19.376827, mean_eps: 0.666003\n",
      "  445631/1200000: episode: 1786, duration: 3.076s, episode steps: 195, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.723 [0.000, 3.000],  loss: 0.693168, mae: 14.815220, mean_q: -19.401834, mean_eps: 0.665850\n",
      "  445815/1200000: episode: 1787, duration: 2.950s, episode steps: 184, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.880 [0.000, 3.000],  loss: 0.664208, mae: 14.771473, mean_q: -19.352112, mean_eps: 0.665708\n",
      "  446048/1200000: episode: 1788, duration: 3.605s, episode steps: 233, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.567110, mae: 14.760247, mean_q: -19.378646, mean_eps: 0.665552\n",
      "  446259/1200000: episode: 1789, duration: 3.222s, episode steps: 211, steps per second:  65, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.634365, mae: 14.739954, mean_q: -19.311536, mean_eps: 0.665385\n",
      "  446501/1200000: episode: 1790, duration: 3.846s, episode steps: 242, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.592650, mae: 14.790741, mean_q: -19.395092, mean_eps: 0.665215\n",
      "  446753/1200000: episode: 1791, duration: 3.819s, episode steps: 252, steps per second:  66, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.632585, mae: 14.782632, mean_q: -19.390344, mean_eps: 0.665030\n",
      "  446977/1200000: episode: 1792, duration: 3.515s, episode steps: 224, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.619132, mae: 14.711092, mean_q: -19.290326, mean_eps: 0.664852\n",
      "  447333/1200000: episode: 1793, duration: 5.548s, episode steps: 356, steps per second:  64, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.648604, mae: 14.679024, mean_q: -19.250094, mean_eps: 0.664634\n",
      "  447508/1200000: episode: 1794, duration: 2.782s, episode steps: 175, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.587613, mae: 14.963652, mean_q: -19.649491, mean_eps: 0.664435\n",
      "  447695/1200000: episode: 1795, duration: 3.034s, episode steps: 187, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.611535, mae: 14.824705, mean_q: -19.465853, mean_eps: 0.664299\n",
      "  447924/1200000: episode: 1796, duration: 3.623s, episode steps: 229, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.592573, mae: 14.790878, mean_q: -19.433176, mean_eps: 0.664143\n",
      "  448094/1200000: episode: 1797, duration: 2.677s, episode steps: 170, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.663890, mae: 14.676314, mean_q: -19.228550, mean_eps: 0.663994\n",
      "  448376/1200000: episode: 1798, duration: 4.372s, episode steps: 282, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.608777, mae: 14.748983, mean_q: -19.365352, mean_eps: 0.663824\n",
      "  448639/1200000: episode: 1799, duration: 4.131s, episode steps: 263, steps per second:  64, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.623900, mae: 14.805348, mean_q: -19.404721, mean_eps: 0.663620\n",
      "  448813/1200000: episode: 1800, duration: 2.776s, episode steps: 174, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.701 [0.000, 3.000],  loss: 0.643742, mae: 14.839245, mean_q: -19.496687, mean_eps: 0.663456\n",
      "  449124/1200000: episode: 1801, duration: 4.850s, episode steps: 311, steps per second:  64, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.623134, mae: 14.715316, mean_q: -19.322954, mean_eps: 0.663274\n",
      "  449290/1200000: episode: 1802, duration: 2.628s, episode steps: 166, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.581538, mae: 14.858784, mean_q: -19.503452, mean_eps: 0.663095\n",
      "  449464/1200000: episode: 1803, duration: 2.850s, episode steps: 174, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.695 [0.000, 3.000],  loss: 0.643054, mae: 14.673652, mean_q: -19.255615, mean_eps: 0.662968\n",
      "  449700/1200000: episode: 1804, duration: 3.701s, episode steps: 236, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.626689, mae: 14.586801, mean_q: -19.136016, mean_eps: 0.662814\n",
      "  449867/1200000: episode: 1805, duration: 2.672s, episode steps: 167, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.526973, mae: 14.823137, mean_q: -19.453501, mean_eps: 0.662663\n",
      "  450136/1200000: episode: 1806, duration: 4.260s, episode steps: 269, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.557278, mae: 14.768446, mean_q: -19.390829, mean_eps: 0.662499\n",
      "  450356/1200000: episode: 1807, duration: 3.450s, episode steps: 220, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.545023, mae: 14.852990, mean_q: -19.500255, mean_eps: 0.662316\n",
      "  450635/1200000: episode: 1808, duration: 4.242s, episode steps: 279, steps per second:  66, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.635910, mae: 14.875947, mean_q: -19.525896, mean_eps: 0.662129\n",
      "  450959/1200000: episode: 1809, duration: 5.199s, episode steps: 324, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.630305, mae: 14.834643, mean_q: -19.468320, mean_eps: 0.661903\n",
      "  451201/1200000: episode: 1810, duration: 3.853s, episode steps: 242, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.420186, mae: 14.860581, mean_q: -19.526653, mean_eps: 0.661690\n",
      "  451409/1200000: episode: 1811, duration: 3.231s, episode steps: 208, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.487138, mae: 14.866429, mean_q: -19.519644, mean_eps: 0.661522\n",
      "  451587/1200000: episode: 1812, duration: 2.866s, episode steps: 178, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.605352, mae: 14.807917, mean_q: -19.446335, mean_eps: 0.661377\n",
      "  451803/1200000: episode: 1813, duration: 3.403s, episode steps: 216, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.628636, mae: 14.801770, mean_q: -19.405476, mean_eps: 0.661229\n",
      "  451974/1200000: episode: 1814, duration: 2.746s, episode steps: 171, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.608069, mae: 14.900966, mean_q: -19.530055, mean_eps: 0.661084\n",
      "  452237/1200000: episode: 1815, duration: 4.180s, episode steps: 263, steps per second:  63, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.646028, mae: 14.839764, mean_q: -19.477378, mean_eps: 0.660921\n",
      "  452536/1200000: episode: 1816, duration: 4.763s, episode steps: 299, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.701245, mae: 14.878086, mean_q: -19.500531, mean_eps: 0.660710\n",
      "  452713/1200000: episode: 1817, duration: 2.853s, episode steps: 177, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.661953, mae: 14.875085, mean_q: -19.530990, mean_eps: 0.660532\n",
      "  452930/1200000: episode: 1818, duration: 3.499s, episode steps: 217, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.563628, mae: 14.730477, mean_q: -19.335352, mean_eps: 0.660384\n",
      "  453155/1200000: episode: 1819, duration: 3.514s, episode steps: 225, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.632416, mae: 14.723461, mean_q: -19.302517, mean_eps: 0.660218\n",
      "  453496/1200000: episode: 1820, duration: 5.378s, episode steps: 341, steps per second:  63, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.617985, mae: 14.817329, mean_q: -19.446252, mean_eps: 0.660006\n",
      "  453664/1200000: episode: 1821, duration: 2.666s, episode steps: 168, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.595803, mae: 14.892652, mean_q: -19.562172, mean_eps: 0.659815\n",
      "  454001/1200000: episode: 1822, duration: 5.369s, episode steps: 337, steps per second:  63, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.538391, mae: 14.777242, mean_q: -19.405864, mean_eps: 0.659626\n",
      "  454299/1200000: episode: 1823, duration: 4.621s, episode steps: 298, steps per second:  64, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.656001, mae: 14.874046, mean_q: -19.513597, mean_eps: 0.659388\n",
      "  454465/1200000: episode: 1824, duration: 2.630s, episode steps: 166, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.556327, mae: 14.822617, mean_q: -19.461956, mean_eps: 0.659214\n",
      "  454687/1200000: episode: 1825, duration: 3.566s, episode steps: 222, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.695708, mae: 14.756945, mean_q: -19.335675, mean_eps: 0.659068\n",
      "  454858/1200000: episode: 1826, duration: 2.779s, episode steps: 171, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.731835, mae: 14.803569, mean_q: -19.413479, mean_eps: 0.658921\n",
      "  455101/1200000: episode: 1827, duration: 3.844s, episode steps: 243, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.749915, mae: 14.866755, mean_q: -19.446690, mean_eps: 0.658766\n",
      "  455281/1200000: episode: 1828, duration: 2.879s, episode steps: 180, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.653895, mae: 14.734641, mean_q: -19.294047, mean_eps: 0.658607\n",
      "  455502/1200000: episode: 1829, duration: 3.585s, episode steps: 221, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.528740, mae: 14.833201, mean_q: -19.489619, mean_eps: 0.658457\n",
      "  455741/1200000: episode: 1830, duration: 3.728s, episode steps: 239, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.628093, mae: 14.770629, mean_q: -19.379236, mean_eps: 0.658284\n",
      "  455903/1200000: episode: 1831, duration: 3.079s, episode steps: 162, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.639618, mae: 14.821092, mean_q: -19.441321, mean_eps: 0.658134\n",
      "  456291/1200000: episode: 1832, duration: 6.158s, episode steps: 388, steps per second:  63, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.548409, mae: 14.882786, mean_q: -19.547094, mean_eps: 0.657928\n",
      "  456476/1200000: episode: 1833, duration: 3.519s, episode steps: 185, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.641844, mae: 14.961776, mean_q: -19.639651, mean_eps: 0.657713\n",
      "  456695/1200000: episode: 1834, duration: 3.572s, episode steps: 219, steps per second:  61, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.579805, mae: 14.758379, mean_q: -19.379696, mean_eps: 0.657561\n",
      "  456910/1200000: episode: 1835, duration: 4.390s, episode steps: 215, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.476653, mae: 14.826405, mean_q: -19.492600, mean_eps: 0.657398\n",
      "  457247/1200000: episode: 1836, duration: 5.695s, episode steps: 337, steps per second:  59, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.690021, mae: 14.772706, mean_q: -19.372109, mean_eps: 0.657192\n",
      "  457652/1200000: episode: 1837, duration: 9.431s, episode steps: 405, steps per second:  43, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.663639, mae: 14.853091, mean_q: -19.493714, mean_eps: 0.656913\n",
      "  457925/1200000: episode: 1838, duration: 4.397s, episode steps: 273, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.672965, mae: 14.860513, mean_q: -19.486502, mean_eps: 0.656659\n",
      "  458106/1200000: episode: 1839, duration: 2.902s, episode steps: 181, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.735 [0.000, 3.000],  loss: 0.632954, mae: 14.894334, mean_q: -19.560141, mean_eps: 0.656489\n",
      "  458313/1200000: episode: 1840, duration: 3.793s, episode steps: 207, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.604377, mae: 14.911571, mean_q: -19.566557, mean_eps: 0.656343\n",
      "  458533/1200000: episode: 1841, duration: 3.704s, episode steps: 220, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.673342, mae: 14.827218, mean_q: -19.457384, mean_eps: 0.656183\n",
      "  458882/1200000: episode: 1842, duration: 5.888s, episode steps: 349, steps per second:  59, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.527266, mae: 14.775374, mean_q: -19.396341, mean_eps: 0.655970\n",
      "  459109/1200000: episode: 1843, duration: 3.464s, episode steps: 227, steps per second:  66, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.527998, mae: 14.841565, mean_q: -19.512989, mean_eps: 0.655754\n",
      "  459281/1200000: episode: 1844, duration: 2.625s, episode steps: 172, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.746779, mae: 14.903004, mean_q: -19.529878, mean_eps: 0.655604\n",
      "  459461/1200000: episode: 1845, duration: 2.805s, episode steps: 180, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.575536, mae: 14.935349, mean_q: -19.612737, mean_eps: 0.655472\n",
      "  459645/1200000: episode: 1846, duration: 2.664s, episode steps: 184, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 0.594946, mae: 14.880733, mean_q: -19.523792, mean_eps: 0.655336\n",
      "  459861/1200000: episode: 1847, duration: 3.448s, episode steps: 216, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.529301, mae: 14.784827, mean_q: -19.424334, mean_eps: 0.655186\n",
      "  460271/1200000: episode: 1848, duration: 6.454s, episode steps: 410, steps per second:  64, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.528108, mae: 14.587085, mean_q: -19.162071, mean_eps: 0.654951\n",
      "  460618/1200000: episode: 1849, duration: 6.073s, episode steps: 347, steps per second:  57, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.524115, mae: 14.370359, mean_q: -18.861678, mean_eps: 0.654667\n",
      "  460894/1200000: episode: 1850, duration: 4.361s, episode steps: 276, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.532247, mae: 14.391303, mean_q: -18.899669, mean_eps: 0.654433\n",
      "  461116/1200000: episode: 1851, duration: 3.750s, episode steps: 222, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.532638, mae: 14.458139, mean_q: -18.976831, mean_eps: 0.654247\n",
      "  461439/1200000: episode: 1852, duration: 4.949s, episode steps: 323, steps per second:  65, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.627223, mae: 14.359668, mean_q: -18.836204, mean_eps: 0.654042\n",
      "  461602/1200000: episode: 1853, duration: 2.622s, episode steps: 163, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.677735, mae: 14.497275, mean_q: -19.035187, mean_eps: 0.653860\n",
      "  461807/1200000: episode: 1854, duration: 3.486s, episode steps: 205, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.475942, mae: 14.365598, mean_q: -18.845893, mean_eps: 0.653722\n",
      "  462059/1200000: episode: 1855, duration: 3.746s, episode steps: 252, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.574859, mae: 14.408796, mean_q: -18.917053, mean_eps: 0.653551\n",
      "  462238/1200000: episode: 1856, duration: 2.610s, episode steps: 179, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.637 [0.000, 3.000],  loss: 0.515733, mae: 14.392603, mean_q: -18.909725, mean_eps: 0.653389\n",
      "  462440/1200000: episode: 1857, duration: 3.226s, episode steps: 202, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.429517, mae: 14.399294, mean_q: -18.927115, mean_eps: 0.653246\n",
      "  462634/1200000: episode: 1858, duration: 3.385s, episode steps: 194, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.501096, mae: 14.315651, mean_q: -18.793296, mean_eps: 0.653098\n",
      "  462851/1200000: episode: 1859, duration: 3.411s, episode steps: 217, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.557589, mae: 14.426969, mean_q: -18.936967, mean_eps: 0.652944\n",
      "  463415/1200000: episode: 1860, duration: 9.683s, episode steps: 564, steps per second:  58, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.558657, mae: 14.467467, mean_q: -18.986813, mean_eps: 0.652651\n",
      "  463666/1200000: episode: 1861, duration: 5.475s, episode steps: 251, steps per second:  46, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.457548, mae: 14.375118, mean_q: -18.881513, mean_eps: 0.652345\n",
      "  463935/1200000: episode: 1862, duration: 4.348s, episode steps: 269, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.570556, mae: 14.424231, mean_q: -18.889048, mean_eps: 0.652150\n",
      "  464105/1200000: episode: 1863, duration: 2.622s, episode steps: 170, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.580254, mae: 14.418756, mean_q: -18.929357, mean_eps: 0.651985\n",
      "  464468/1200000: episode: 1864, duration: 5.534s, episode steps: 363, steps per second:  66, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.512054, mae: 14.439628, mean_q: -18.964097, mean_eps: 0.651786\n",
      "  464654/1200000: episode: 1865, duration: 2.883s, episode steps: 186, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.592143, mae: 14.422473, mean_q: -18.915029, mean_eps: 0.651580\n",
      "  464974/1200000: episode: 1866, duration: 4.699s, episode steps: 320, steps per second:  68, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 0.521366, mae: 14.475003, mean_q: -19.007366, mean_eps: 0.651390\n",
      "  465138/1200000: episode: 1867, duration: 2.525s, episode steps: 164, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.678514, mae: 14.383566, mean_q: -18.838683, mean_eps: 0.651208\n",
      "  465379/1200000: episode: 1868, duration: 3.684s, episode steps: 241, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.529106, mae: 14.437760, mean_q: -18.961053, mean_eps: 0.651057\n",
      "  465550/1200000: episode: 1869, duration: 3.143s, episode steps: 171, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.653640, mae: 14.377606, mean_q: -18.869813, mean_eps: 0.650902\n",
      "  465740/1200000: episode: 1870, duration: 3.600s, episode steps: 190, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.553 [0.000, 3.000],  loss: 0.532123, mae: 14.359929, mean_q: -18.861185, mean_eps: 0.650767\n",
      "  465943/1200000: episode: 1871, duration: 3.941s, episode steps: 203, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.608009, mae: 14.385780, mean_q: -18.884264, mean_eps: 0.650619\n",
      "  466180/1200000: episode: 1872, duration: 3.757s, episode steps: 237, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.620904, mae: 14.368639, mean_q: -18.856322, mean_eps: 0.650454\n",
      "  466362/1200000: episode: 1873, duration: 2.823s, episode steps: 182, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.458043, mae: 14.363076, mean_q: -18.845940, mean_eps: 0.650297\n",
      "  466569/1200000: episode: 1874, duration: 3.135s, episode steps: 207, steps per second:  66, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.606481, mae: 14.301249, mean_q: -18.743808, mean_eps: 0.650151\n",
      "  466772/1200000: episode: 1875, duration: 3.118s, episode steps: 203, steps per second:  65, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.552890, mae: 14.442118, mean_q: -18.963424, mean_eps: 0.649998\n",
      "  467127/1200000: episode: 1876, duration: 5.337s, episode steps: 355, steps per second:  67, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.540010, mae: 14.402980, mean_q: -18.915661, mean_eps: 0.649788\n",
      "  467482/1200000: episode: 1877, duration: 5.759s, episode steps: 355, steps per second:  62, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.561710, mae: 14.352591, mean_q: -18.831228, mean_eps: 0.649522\n",
      "  467703/1200000: episode: 1878, duration: 3.535s, episode steps: 221, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.544890, mae: 14.337960, mean_q: -18.848885, mean_eps: 0.649306\n",
      "  467873/1200000: episode: 1879, duration: 2.585s, episode steps: 170, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.541307, mae: 14.335257, mean_q: -18.810112, mean_eps: 0.649159\n",
      "  468151/1200000: episode: 1880, duration: 4.367s, episode steps: 278, steps per second:  64, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.572583, mae: 14.404476, mean_q: -18.906867, mean_eps: 0.648991\n",
      "  468601/1200000: episode: 1881, duration: 7.376s, episode steps: 450, steps per second:  61, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.581494, mae: 14.368093, mean_q: -18.847158, mean_eps: 0.648718\n",
      "  468853/1200000: episode: 1882, duration: 4.047s, episode steps: 252, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.643815, mae: 14.378172, mean_q: -18.880208, mean_eps: 0.648455\n",
      "  469029/1200000: episode: 1883, duration: 3.129s, episode steps: 176, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.437547, mae: 14.389863, mean_q: -18.898225, mean_eps: 0.648295\n",
      "  469274/1200000: episode: 1884, duration: 3.743s, episode steps: 245, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.593127, mae: 14.445056, mean_q: -18.957231, mean_eps: 0.648137\n",
      "  469446/1200000: episode: 1885, duration: 2.544s, episode steps: 172, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.430243, mae: 14.370326, mean_q: -18.886470, mean_eps: 0.647980\n",
      "  469629/1200000: episode: 1886, duration: 2.644s, episode steps: 183, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.326076, mae: 14.381890, mean_q: -18.927461, mean_eps: 0.647847\n",
      "  469994/1200000: episode: 1887, duration: 5.352s, episode steps: 365, steps per second:  68, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.512725, mae: 14.347649, mean_q: -18.825048, mean_eps: 0.647642\n",
      "  470240/1200000: episode: 1888, duration: 3.660s, episode steps: 246, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.589362, mae: 14.036345, mean_q: -18.412692, mean_eps: 0.647413\n",
      "  470466/1200000: episode: 1889, duration: 3.433s, episode steps: 226, steps per second:  66, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.463911, mae: 14.023524, mean_q: -18.389050, mean_eps: 0.647236\n",
      "  470674/1200000: episode: 1890, duration: 3.089s, episode steps: 208, steps per second:  67, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.434248, mae: 14.033521, mean_q: -18.429331, mean_eps: 0.647073\n",
      "  470911/1200000: episode: 1891, duration: 3.494s, episode steps: 237, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.533205, mae: 14.124226, mean_q: -18.541280, mean_eps: 0.646906\n",
      "  471182/1200000: episode: 1892, duration: 3.878s, episode steps: 271, steps per second:  70, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.633526, mae: 14.098581, mean_q: -18.475295, mean_eps: 0.646716\n",
      "  471586/1200000: episode: 1893, duration: 6.016s, episode steps: 404, steps per second:  67, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.570759, mae: 14.034739, mean_q: -18.412720, mean_eps: 0.646462\n",
      "  471763/1200000: episode: 1894, duration: 2.801s, episode steps: 177, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.581217, mae: 14.122941, mean_q: -18.545993, mean_eps: 0.646244\n",
      "  472211/1200000: episode: 1895, duration: 6.693s, episode steps: 448, steps per second:  67, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.494354, mae: 14.039054, mean_q: -18.432739, mean_eps: 0.646010\n",
      "  472376/1200000: episode: 1896, duration: 2.490s, episode steps: 165, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.435534, mae: 13.924317, mean_q: -18.298259, mean_eps: 0.645780\n",
      "  472627/1200000: episode: 1897, duration: 3.759s, episode steps: 251, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.440691, mae: 14.061468, mean_q: -18.489018, mean_eps: 0.645624\n",
      "  472812/1200000: episode: 1898, duration: 2.734s, episode steps: 185, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.805 [0.000, 3.000],  loss: 0.563070, mae: 14.037314, mean_q: -18.402430, mean_eps: 0.645461\n",
      "  473023/1200000: episode: 1899, duration: 3.123s, episode steps: 211, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.516597, mae: 14.080121, mean_q: -18.485783, mean_eps: 0.645312\n",
      "  473199/1200000: episode: 1900, duration: 2.579s, episode steps: 176, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.600487, mae: 14.001722, mean_q: -18.362645, mean_eps: 0.645167\n",
      "  473402/1200000: episode: 1901, duration: 3.079s, episode steps: 203, steps per second:  66, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.488015, mae: 14.059159, mean_q: -18.453474, mean_eps: 0.645025\n",
      "  473618/1200000: episode: 1902, duration: 3.281s, episode steps: 216, steps per second:  66, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.557672, mae: 14.027435, mean_q: -18.400530, mean_eps: 0.644868\n",
      "  473827/1200000: episode: 1903, duration: 4.111s, episode steps: 209, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.622885, mae: 14.026495, mean_q: -18.402916, mean_eps: 0.644709\n",
      "  474040/1200000: episode: 1904, duration: 3.544s, episode steps: 213, steps per second:  60, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.535101, mae: 14.100678, mean_q: -18.523971, mean_eps: 0.644550\n",
      "  474250/1200000: episode: 1905, duration: 3.175s, episode steps: 210, steps per second:  66, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.574942, mae: 14.025628, mean_q: -18.395408, mean_eps: 0.644392\n",
      "  474475/1200000: episode: 1906, duration: 3.332s, episode steps: 225, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.472345, mae: 14.120832, mean_q: -18.525419, mean_eps: 0.644228\n",
      "  474638/1200000: episode: 1907, duration: 2.500s, episode steps: 163, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.530842, mae: 13.982327, mean_q: -18.337348, mean_eps: 0.644083\n",
      "  474818/1200000: episode: 1908, duration: 2.706s, episode steps: 180, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.515361, mae: 14.123294, mean_q: -18.552209, mean_eps: 0.643954\n",
      "  474991/1200000: episode: 1909, duration: 2.909s, episode steps: 173, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.763 [0.000, 3.000],  loss: 0.471281, mae: 13.967441, mean_q: -18.339015, mean_eps: 0.643822\n",
      "  475253/1200000: episode: 1910, duration: 4.141s, episode steps: 262, steps per second:  63, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.511826, mae: 14.013110, mean_q: -18.406348, mean_eps: 0.643659\n",
      "  475417/1200000: episode: 1911, duration: 2.612s, episode steps: 164, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.574743, mae: 14.091723, mean_q: -18.500296, mean_eps: 0.643499\n",
      "  475652/1200000: episode: 1912, duration: 3.722s, episode steps: 235, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.462834, mae: 13.998515, mean_q: -18.407649, mean_eps: 0.643350\n",
      "  475840/1200000: episode: 1913, duration: 2.849s, episode steps: 188, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.686 [0.000, 3.000],  loss: 0.515840, mae: 14.121415, mean_q: -18.521497, mean_eps: 0.643191\n",
      "  476098/1200000: episode: 1914, duration: 3.878s, episode steps: 258, steps per second:  67, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.384472, mae: 14.003430, mean_q: -18.417747, mean_eps: 0.643024\n",
      "  476272/1200000: episode: 1915, duration: 2.674s, episode steps: 174, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.500049, mae: 14.044455, mean_q: -18.455667, mean_eps: 0.642862\n",
      "  476575/1200000: episode: 1916, duration: 4.516s, episode steps: 303, steps per second:  67, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.522648, mae: 14.049176, mean_q: -18.434987, mean_eps: 0.642683\n",
      "  476755/1200000: episode: 1917, duration: 2.689s, episode steps: 180, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: 0.450261, mae: 13.885993, mean_q: -18.227817, mean_eps: 0.642502\n",
      "  477005/1200000: episode: 1918, duration: 3.701s, episode steps: 250, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.620823, mae: 13.985459, mean_q: -18.312944, mean_eps: 0.642340\n",
      "  477229/1200000: episode: 1919, duration: 3.379s, episode steps: 224, steps per second:  66, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.888 [0.000, 3.000],  loss: 0.499601, mae: 14.075500, mean_q: -18.484162, mean_eps: 0.642163\n",
      "  477413/1200000: episode: 1920, duration: 2.763s, episode steps: 184, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.507147, mae: 13.996417, mean_q: -18.382046, mean_eps: 0.642010\n",
      "  477757/1200000: episode: 1921, duration: 5.555s, episode steps: 344, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.469158, mae: 14.003187, mean_q: -18.399861, mean_eps: 0.641812\n",
      "  477977/1200000: episode: 1922, duration: 3.650s, episode steps: 220, steps per second:  60, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.478995, mae: 14.140096, mean_q: -18.581333, mean_eps: 0.641600\n",
      "  478269/1200000: episode: 1923, duration: 4.722s, episode steps: 292, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.473180, mae: 13.996217, mean_q: -18.378637, mean_eps: 0.641408\n",
      "  478475/1200000: episode: 1924, duration: 3.229s, episode steps: 206, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.407685, mae: 14.026905, mean_q: -18.440433, mean_eps: 0.641221\n",
      "  478659/1200000: episode: 1925, duration: 2.812s, episode steps: 184, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.562117, mae: 14.077000, mean_q: -18.461055, mean_eps: 0.641075\n",
      "  478870/1200000: episode: 1926, duration: 3.189s, episode steps: 211, steps per second:  66, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.451736, mae: 13.919675, mean_q: -18.286704, mean_eps: 0.640927\n",
      "  479193/1200000: episode: 1927, duration: 4.788s, episode steps: 323, steps per second:  67, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.564458, mae: 14.053259, mean_q: -18.427047, mean_eps: 0.640727\n",
      "  479365/1200000: episode: 1928, duration: 2.688s, episode steps: 172, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.721 [0.000, 3.000],  loss: 0.575128, mae: 14.025718, mean_q: -18.403950, mean_eps: 0.640541\n",
      "  479710/1200000: episode: 1929, duration: 5.230s, episode steps: 345, steps per second:  66, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.466844, mae: 14.060330, mean_q: -18.470458, mean_eps: 0.640347\n",
      "  480011/1200000: episode: 1930, duration: 4.675s, episode steps: 301, steps per second:  64, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.633151, mae: 14.072082, mean_q: -18.451460, mean_eps: 0.640105\n",
      "  480189/1200000: episode: 1931, duration: 2.947s, episode steps: 178, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.736 [0.000, 3.000],  loss: 0.530830, mae: 13.404977, mean_q: -17.615970, mean_eps: 0.639925\n",
      "  480370/1200000: episode: 1932, duration: 3.470s, episode steps: 181, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.796 [0.000, 3.000],  loss: 0.481466, mae: 13.353039, mean_q: -17.540723, mean_eps: 0.639791\n",
      "  480541/1200000: episode: 1933, duration: 3.072s, episode steps: 171, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.419967, mae: 13.311118, mean_q: -17.484226, mean_eps: 0.639659\n",
      "  480734/1200000: episode: 1934, duration: 3.642s, episode steps: 193, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.536161, mae: 13.343065, mean_q: -17.515108, mean_eps: 0.639522\n",
      "  481041/1200000: episode: 1935, duration: 4.922s, episode steps: 307, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.414385, mae: 13.448631, mean_q: -17.679563, mean_eps: 0.639335\n",
      "  481218/1200000: episode: 1936, duration: 3.105s, episode steps: 177, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.485454, mae: 13.342945, mean_q: -17.508993, mean_eps: 0.639153\n",
      "  481516/1200000: episode: 1937, duration: 4.707s, episode steps: 298, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.454911, mae: 13.371869, mean_q: -17.560320, mean_eps: 0.638975\n",
      "  481822/1200000: episode: 1938, duration: 4.735s, episode steps: 306, steps per second:  65, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.410538, mae: 13.461143, mean_q: -17.673081, mean_eps: 0.638749\n",
      "  481994/1200000: episode: 1939, duration: 2.688s, episode steps: 172, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.523 [0.000, 3.000],  loss: 0.493291, mae: 13.489011, mean_q: -17.709350, mean_eps: 0.638569\n",
      "  482170/1200000: episode: 1940, duration: 2.859s, episode steps: 176, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 0.448409, mae: 13.325691, mean_q: -17.514359, mean_eps: 0.638439\n",
      "  482429/1200000: episode: 1941, duration: 4.026s, episode steps: 259, steps per second:  64, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.446367, mae: 13.391099, mean_q: -17.580053, mean_eps: 0.638276\n",
      "  482682/1200000: episode: 1942, duration: 3.961s, episode steps: 253, steps per second:  64, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.563137, mae: 13.471129, mean_q: -17.646775, mean_eps: 0.638084\n",
      "  482920/1200000: episode: 1943, duration: 3.855s, episode steps: 238, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.418354, mae: 13.403046, mean_q: -17.609489, mean_eps: 0.637900\n",
      "  483098/1200000: episode: 1944, duration: 2.782s, episode steps: 178, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: 0.484334, mae: 13.373797, mean_q: -17.576582, mean_eps: 0.637744\n",
      "  483310/1200000: episode: 1945, duration: 3.576s, episode steps: 212, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.396611, mae: 13.378204, mean_q: -17.580624, mean_eps: 0.637597\n",
      "  483658/1200000: episode: 1946, duration: 5.523s, episode steps: 348, steps per second:  63, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.398500, mae: 13.435183, mean_q: -17.636486, mean_eps: 0.637387\n",
      "  483869/1200000: episode: 1947, duration: 3.134s, episode steps: 211, steps per second:  67, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.461284, mae: 13.363939, mean_q: -17.531711, mean_eps: 0.637178\n",
      "  484160/1200000: episode: 1948, duration: 4.682s, episode steps: 291, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.507116, mae: 13.363609, mean_q: -17.534919, mean_eps: 0.636989\n",
      "  484452/1200000: episode: 1949, duration: 4.580s, episode steps: 292, steps per second:  64, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.570363, mae: 13.414762, mean_q: -17.592854, mean_eps: 0.636771\n",
      "  484665/1200000: episode: 1950, duration: 3.375s, episode steps: 213, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.566628, mae: 13.419276, mean_q: -17.586392, mean_eps: 0.636582\n",
      "  484846/1200000: episode: 1951, duration: 3.100s, episode steps: 181, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.442293, mae: 13.501900, mean_q: -17.732864, mean_eps: 0.636434\n",
      "  485087/1200000: episode: 1952, duration: 4.222s, episode steps: 241, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.471443, mae: 13.397463, mean_q: -17.573361, mean_eps: 0.636275\n",
      "  485318/1200000: episode: 1953, duration: 3.640s, episode steps: 231, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.438432, mae: 13.430689, mean_q: -17.635687, mean_eps: 0.636099\n",
      "  485737/1200000: episode: 1954, duration: 7.495s, episode steps: 419, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.437948, mae: 13.407196, mean_q: -17.610798, mean_eps: 0.635855\n",
      "  485991/1200000: episode: 1955, duration: 4.107s, episode steps: 254, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.372314, mae: 13.451141, mean_q: -17.688025, mean_eps: 0.635602\n",
      "  486176/1200000: episode: 1956, duration: 2.959s, episode steps: 185, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.370569, mae: 13.336511, mean_q: -17.527759, mean_eps: 0.635438\n",
      "  486524/1200000: episode: 1957, duration: 5.465s, episode steps: 348, steps per second:  64, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.465576, mae: 13.413723, mean_q: -17.587685, mean_eps: 0.635238\n",
      "  486887/1200000: episode: 1958, duration: 5.674s, episode steps: 363, steps per second:  64, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.453705, mae: 13.420581, mean_q: -17.628565, mean_eps: 0.634971\n",
      "  487082/1200000: episode: 1959, duration: 3.172s, episode steps: 195, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.831 [0.000, 3.000],  loss: 0.522587, mae: 13.274930, mean_q: -17.413536, mean_eps: 0.634762\n",
      "  487259/1200000: episode: 1960, duration: 2.825s, episode steps: 177, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.757 [0.000, 3.000],  loss: 0.522935, mae: 13.517490, mean_q: -17.752042, mean_eps: 0.634622\n",
      "  487504/1200000: episode: 1961, duration: 3.886s, episode steps: 245, steps per second:  63, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.553399, mae: 13.400414, mean_q: -17.581928, mean_eps: 0.634464\n",
      "  487668/1200000: episode: 1962, duration: 2.618s, episode steps: 164, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.593031, mae: 13.455783, mean_q: -17.620554, mean_eps: 0.634311\n",
      "  487892/1200000: episode: 1963, duration: 3.465s, episode steps: 224, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.414887, mae: 13.417697, mean_q: -17.618723, mean_eps: 0.634165\n",
      "  488056/1200000: episode: 1964, duration: 2.613s, episode steps: 164, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.384 [0.000, 3.000],  loss: 0.584639, mae: 13.479332, mean_q: -17.688680, mean_eps: 0.634020\n",
      "  488438/1200000: episode: 1965, duration: 5.939s, episode steps: 382, steps per second:  64, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.525281, mae: 13.435635, mean_q: -17.632830, mean_eps: 0.633815\n",
      "  488662/1200000: episode: 1966, duration: 3.447s, episode steps: 224, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.452334, mae: 13.369848, mean_q: -17.568375, mean_eps: 0.633588\n",
      "  488848/1200000: episode: 1967, duration: 3.040s, episode steps: 186, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.348050, mae: 13.397546, mean_q: -17.619765, mean_eps: 0.633434\n",
      "  489100/1200000: episode: 1968, duration: 3.905s, episode steps: 252, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.421002, mae: 13.427039, mean_q: -17.634028, mean_eps: 0.633270\n",
      "  489269/1200000: episode: 1969, duration: 2.726s, episode steps: 169, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.410585, mae: 13.413567, mean_q: -17.636877, mean_eps: 0.633112\n",
      "  489444/1200000: episode: 1970, duration: 2.832s, episode steps: 175, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.460395, mae: 13.353953, mean_q: -17.526543, mean_eps: 0.632983\n",
      "  489845/1200000: episode: 1971, duration: 6.224s, episode steps: 401, steps per second:  64, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.425620, mae: 13.475273, mean_q: -17.695001, mean_eps: 0.632767\n",
      "  490053/1200000: episode: 1972, duration: 3.336s, episode steps: 208, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.421262, mae: 13.429381, mean_q: -17.644591, mean_eps: 0.632539\n",
      "  490281/1200000: episode: 1973, duration: 3.641s, episode steps: 228, steps per second:  63, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.511330, mae: 13.076011, mean_q: -17.135551, mean_eps: 0.632375\n",
      "  490504/1200000: episode: 1974, duration: 3.597s, episode steps: 223, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.457087, mae: 13.114539, mean_q: -17.200616, mean_eps: 0.632206\n",
      "  490784/1200000: episode: 1975, duration: 4.543s, episode steps: 280, steps per second:  62, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.465953, mae: 13.060964, mean_q: -17.146289, mean_eps: 0.632017\n",
      "  490963/1200000: episode: 1976, duration: 2.783s, episode steps: 179, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.726 [0.000, 3.000],  loss: 0.480341, mae: 13.123887, mean_q: -17.236738, mean_eps: 0.631845\n",
      "  491148/1200000: episode: 1977, duration: 2.934s, episode steps: 185, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.568609, mae: 13.227698, mean_q: -17.357675, mean_eps: 0.631709\n",
      "  491327/1200000: episode: 1978, duration: 2.819s, episode steps: 179, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.855 [0.000, 3.000],  loss: 0.521742, mae: 13.150010, mean_q: -17.257059, mean_eps: 0.631572\n",
      "  491490/1200000: episode: 1979, duration: 2.635s, episode steps: 163, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.344 [0.000, 3.000],  loss: 0.387135, mae: 13.275655, mean_q: -17.464832, mean_eps: 0.631444\n",
      "  491734/1200000: episode: 1980, duration: 4.073s, episode steps: 244, steps per second:  60, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.485811, mae: 13.072288, mean_q: -17.172872, mean_eps: 0.631291\n",
      "  491943/1200000: episode: 1981, duration: 3.590s, episode steps: 209, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.429814, mae: 13.131330, mean_q: -17.234882, mean_eps: 0.631122\n",
      "  492109/1200000: episode: 1982, duration: 2.890s, episode steps: 166, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.747 [0.000, 3.000],  loss: 0.500598, mae: 13.268655, mean_q: -17.416828, mean_eps: 0.630981\n",
      "  492318/1200000: episode: 1983, duration: 3.672s, episode steps: 209, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.503761, mae: 13.158817, mean_q: -17.265718, mean_eps: 0.630840\n",
      "  492602/1200000: episode: 1984, duration: 4.720s, episode steps: 284, steps per second:  60, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.404448, mae: 13.022186, mean_q: -17.099046, mean_eps: 0.630655\n",
      "  492911/1200000: episode: 1985, duration: 5.301s, episode steps: 309, steps per second:  58, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.401524, mae: 13.198093, mean_q: -17.334768, mean_eps: 0.630433\n",
      "  493093/1200000: episode: 1986, duration: 3.120s, episode steps: 182, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.791 [0.000, 3.000],  loss: 0.546049, mae: 13.175530, mean_q: -17.294424, mean_eps: 0.630249\n",
      "  493264/1200000: episode: 1987, duration: 3.314s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.420612, mae: 12.986434, mean_q: -17.073576, mean_eps: 0.630116\n",
      "  493620/1200000: episode: 1988, duration: 6.169s, episode steps: 356, steps per second:  58, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.389784, mae: 13.061283, mean_q: -17.147439, mean_eps: 0.629919\n",
      "  493831/1200000: episode: 1989, duration: 3.303s, episode steps: 211, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.562413, mae: 13.146802, mean_q: -17.234051, mean_eps: 0.629706\n",
      "  494007/1200000: episode: 1990, duration: 2.935s, episode steps: 176, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.675938, mae: 13.117491, mean_q: -17.175538, mean_eps: 0.629561\n",
      "  494180/1200000: episode: 1991, duration: 2.812s, episode steps: 173, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.407300, mae: 13.171134, mean_q: -17.282279, mean_eps: 0.629430\n",
      "  494398/1200000: episode: 1992, duration: 3.447s, episode steps: 218, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.466598, mae: 13.058149, mean_q: -17.143969, mean_eps: 0.629284\n",
      "  494606/1200000: episode: 1993, duration: 3.510s, episode steps: 208, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.369658, mae: 13.090890, mean_q: -17.204662, mean_eps: 0.629124\n",
      "  494819/1200000: episode: 1994, duration: 3.976s, episode steps: 213, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.408688, mae: 13.046440, mean_q: -17.136472, mean_eps: 0.628966\n",
      "  495287/1200000: episode: 1995, duration: 7.650s, episode steps: 468, steps per second:  61, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.437579, mae: 13.076008, mean_q: -17.173342, mean_eps: 0.628711\n",
      "  495466/1200000: episode: 1996, duration: 2.911s, episode steps: 179, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.299861, mae: 13.037725, mean_q: -17.135626, mean_eps: 0.628468\n",
      "  495707/1200000: episode: 1997, duration: 3.866s, episode steps: 241, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.476891, mae: 13.076116, mean_q: -17.158748, mean_eps: 0.628310\n",
      "  495973/1200000: episode: 1998, duration: 4.232s, episode steps: 266, steps per second:  63, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.516613, mae: 13.129607, mean_q: -17.235090, mean_eps: 0.628120\n",
      "  496370/1200000: episode: 1999, duration: 6.111s, episode steps: 397, steps per second:  65, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.460419, mae: 13.143545, mean_q: -17.246356, mean_eps: 0.627872\n",
      "  496560/1200000: episode: 2000, duration: 2.965s, episode steps: 190, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.451634, mae: 13.186062, mean_q: -17.310247, mean_eps: 0.627652\n",
      "  496771/1200000: episode: 2001, duration: 3.214s, episode steps: 211, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.443556, mae: 13.219668, mean_q: -17.377931, mean_eps: 0.627501\n",
      "  496935/1200000: episode: 2002, duration: 2.525s, episode steps: 164, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 0.489725, mae: 13.095687, mean_q: -17.181663, mean_eps: 0.627361\n",
      "  497451/1200000: episode: 2003, duration: 8.205s, episode steps: 516, steps per second:  63, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.433255, mae: 13.146590, mean_q: -17.267595, mean_eps: 0.627106\n",
      "  497630/1200000: episode: 2004, duration: 3.158s, episode steps: 179, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.402 [0.000, 3.000],  loss: 0.448508, mae: 13.076913, mean_q: -17.164293, mean_eps: 0.626845\n",
      "  497971/1200000: episode: 2005, duration: 5.609s, episode steps: 341, steps per second:  61, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.470048, mae: 13.092880, mean_q: -17.183885, mean_eps: 0.626650\n",
      "  498277/1200000: episode: 2006, duration: 5.652s, episode steps: 306, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.443439, mae: 13.124381, mean_q: -17.228991, mean_eps: 0.626407\n",
      "  498488/1200000: episode: 2007, duration: 3.651s, episode steps: 211, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.398733, mae: 13.166859, mean_q: -17.305022, mean_eps: 0.626214\n",
      "  498682/1200000: episode: 2008, duration: 3.455s, episode steps: 194, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.881 [0.000, 3.000],  loss: 0.291425, mae: 13.047379, mean_q: -17.151602, mean_eps: 0.626062\n",
      "  498865/1200000: episode: 2009, duration: 3.055s, episode steps: 183, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.852 [0.000, 3.000],  loss: 0.484230, mae: 13.094582, mean_q: -17.183122, mean_eps: 0.625920\n",
      "  499085/1200000: episode: 2010, duration: 3.785s, episode steps: 220, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.453946, mae: 13.118038, mean_q: -17.228682, mean_eps: 0.625769\n",
      "  499278/1200000: episode: 2011, duration: 3.333s, episode steps: 193, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.819 [0.000, 3.000],  loss: 0.416351, mae: 13.092096, mean_q: -17.172593, mean_eps: 0.625614\n",
      "  499551/1200000: episode: 2012, duration: 4.753s, episode steps: 273, steps per second:  57, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.506417, mae: 13.179526, mean_q: -17.288257, mean_eps: 0.625439\n",
      "  499858/1200000: episode: 2013, duration: 5.208s, episode steps: 307, steps per second:  59, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.445080, mae: 13.209492, mean_q: -17.348080, mean_eps: 0.625222\n",
      "  500029/1200000: episode: 2014, duration: 2.971s, episode steps: 171, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.370722, mae: 12.947774, mean_q: -17.004316, mean_eps: 0.625043\n",
      "  500525/1200000: episode: 2015, duration: 8.416s, episode steps: 496, steps per second:  59, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.329427, mae: 12.830707, mean_q: -16.848276, mean_eps: 0.624793\n",
      "  500842/1200000: episode: 2016, duration: 6.674s, episode steps: 317, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.495990, mae: 12.905533, mean_q: -16.897759, mean_eps: 0.624488\n",
      "  501089/1200000: episode: 2017, duration: 4.064s, episode steps: 247, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.419243, mae: 12.893087, mean_q: -16.901683, mean_eps: 0.624276\n",
      "  501274/1200000: episode: 2018, duration: 2.876s, episode steps: 185, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.827 [0.000, 3.000],  loss: 0.393663, mae: 12.907757, mean_q: -16.935194, mean_eps: 0.624114\n",
      "  501504/1200000: episode: 2019, duration: 3.783s, episode steps: 230, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.452909, mae: 12.913893, mean_q: -16.948716, mean_eps: 0.623959\n",
      "  501744/1200000: episode: 2020, duration: 4.265s, episode steps: 240, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.413147, mae: 12.876548, mean_q: -16.892962, mean_eps: 0.623782\n",
      "  501970/1200000: episode: 2021, duration: 3.658s, episode steps: 226, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.392582, mae: 12.983764, mean_q: -17.056173, mean_eps: 0.623608\n",
      "  502199/1200000: episode: 2022, duration: 3.796s, episode steps: 229, steps per second:  60, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.403752, mae: 12.919340, mean_q: -16.960491, mean_eps: 0.623437\n",
      "  502384/1200000: episode: 2023, duration: 3.076s, episode steps: 185, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.697 [0.000, 3.000],  loss: 0.319033, mae: 12.870114, mean_q: -16.900702, mean_eps: 0.623282\n",
      "  502748/1200000: episode: 2024, duration: 5.952s, episode steps: 364, steps per second:  61, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.370904, mae: 12.820271, mean_q: -16.833239, mean_eps: 0.623076\n",
      "  502927/1200000: episode: 2025, duration: 3.001s, episode steps: 179, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.594899, mae: 12.868399, mean_q: -16.850669, mean_eps: 0.622872\n",
      "  503253/1200000: episode: 2026, duration: 5.230s, episode steps: 326, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.320592, mae: 12.899890, mean_q: -16.956320, mean_eps: 0.622683\n",
      "  503475/1200000: episode: 2027, duration: 3.684s, episode steps: 222, steps per second:  60, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.793 [0.000, 3.000],  loss: 0.431703, mae: 12.949288, mean_q: -16.966982, mean_eps: 0.622477\n",
      "  503644/1200000: episode: 2028, duration: 2.887s, episode steps: 169, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.704 [0.000, 3.000],  loss: 0.328906, mae: 12.853983, mean_q: -16.894179, mean_eps: 0.622331\n",
      "  503862/1200000: episode: 2029, duration: 3.417s, episode steps: 218, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.363623, mae: 12.903575, mean_q: -16.942667, mean_eps: 0.622186\n",
      "  504067/1200000: episode: 2030, duration: 3.193s, episode steps: 205, steps per second:  64, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.438139, mae: 12.855012, mean_q: -16.859709, mean_eps: 0.622027\n",
      "  504234/1200000: episode: 2031, duration: 2.698s, episode steps: 167, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.683499, mae: 12.936725, mean_q: -16.907459, mean_eps: 0.621887\n",
      "  504465/1200000: episode: 2032, duration: 3.593s, episode steps: 231, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.399285, mae: 12.920633, mean_q: -16.953156, mean_eps: 0.621738\n",
      "  504636/1200000: episode: 2033, duration: 2.752s, episode steps: 171, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.953 [0.000, 3.000],  loss: 0.460490, mae: 12.929481, mean_q: -16.914838, mean_eps: 0.621587\n",
      "  504956/1200000: episode: 2034, duration: 4.900s, episode steps: 320, steps per second:  65, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.350488, mae: 12.835140, mean_q: -16.862931, mean_eps: 0.621403\n",
      "  505224/1200000: episode: 2035, duration: 4.183s, episode steps: 268, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.491348, mae: 12.844359, mean_q: -16.812130, mean_eps: 0.621183\n",
      "  505546/1200000: episode: 2036, duration: 5.986s, episode steps: 322, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.524301, mae: 12.869987, mean_q: -16.869722, mean_eps: 0.620962\n",
      "  505791/1200000: episode: 2037, duration: 4.603s, episode steps: 245, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.494442, mae: 12.796315, mean_q: -16.790180, mean_eps: 0.620749\n",
      "  506216/1200000: episode: 2038, duration: 7.757s, episode steps: 425, steps per second:  55, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.435721, mae: 12.906880, mean_q: -16.947553, mean_eps: 0.620498\n",
      "  506388/1200000: episode: 2039, duration: 2.954s, episode steps: 172, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.401 [0.000, 3.000],  loss: 0.455695, mae: 12.829249, mean_q: -16.823879, mean_eps: 0.620274\n",
      "  506598/1200000: episode: 2040, duration: 4.083s, episode steps: 210, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.454277, mae: 12.857972, mean_q: -16.879660, mean_eps: 0.620131\n",
      "  506842/1200000: episode: 2041, duration: 5.098s, episode steps: 244, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.375495, mae: 12.870418, mean_q: -16.899477, mean_eps: 0.619960\n",
      "  507131/1200000: episode: 2042, duration: 4.860s, episode steps: 289, steps per second:  59, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.588764, mae: 12.924603, mean_q: -16.930155, mean_eps: 0.619760\n",
      "  507352/1200000: episode: 2043, duration: 3.768s, episode steps: 221, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.374115, mae: 12.867216, mean_q: -16.899416, mean_eps: 0.619569\n",
      "  507679/1200000: episode: 2044, duration: 5.303s, episode steps: 327, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.374356, mae: 12.808899, mean_q: -16.816789, mean_eps: 0.619364\n",
      "  507844/1200000: episode: 2045, duration: 3.290s, episode steps: 165, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.407270, mae: 12.902844, mean_q: -16.943283, mean_eps: 0.619179\n",
      "  508008/1200000: episode: 2046, duration: 2.910s, episode steps: 164, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.256 [0.000, 3.000],  loss: 0.393172, mae: 12.815193, mean_q: -16.831007, mean_eps: 0.619056\n",
      "  508246/1200000: episode: 2047, duration: 4.237s, episode steps: 238, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.355470, mae: 12.823830, mean_q: -16.842144, mean_eps: 0.618905\n",
      "  508494/1200000: episode: 2048, duration: 4.221s, episode steps: 248, steps per second:  59, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.399763, mae: 12.849196, mean_q: -16.848496, mean_eps: 0.618723\n",
      "  508704/1200000: episode: 2049, duration: 3.617s, episode steps: 210, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.305632, mae: 12.845188, mean_q: -16.882616, mean_eps: 0.618551\n",
      "  508949/1200000: episode: 2050, duration: 4.608s, episode steps: 245, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.419565, mae: 12.829656, mean_q: -16.845681, mean_eps: 0.618380\n",
      "  509217/1200000: episode: 2051, duration: 5.192s, episode steps: 268, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.440381, mae: 12.930722, mean_q: -16.981666, mean_eps: 0.618188\n",
      "  509557/1200000: episode: 2052, duration: 5.854s, episode steps: 340, steps per second:  58, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.394239, mae: 12.879577, mean_q: -16.905004, mean_eps: 0.617960\n",
      "  509758/1200000: episode: 2053, duration: 3.549s, episode steps: 201, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.119 [0.000, 3.000],  loss: 0.448379, mae: 12.903063, mean_q: -16.917616, mean_eps: 0.617757\n",
      "  510004/1200000: episode: 2054, duration: 4.220s, episode steps: 246, steps per second:  58, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.474823, mae: 12.821368, mean_q: -16.831277, mean_eps: 0.617590\n",
      "  510178/1200000: episode: 2055, duration: 3.106s, episode steps: 174, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.845 [0.000, 3.000],  loss: 0.333315, mae: 12.347421, mean_q: -16.199972, mean_eps: 0.617432\n",
      "  510386/1200000: episode: 2056, duration: 3.715s, episode steps: 208, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.359750, mae: 12.435225, mean_q: -16.315393, mean_eps: 0.617289\n",
      "  510662/1200000: episode: 2057, duration: 4.706s, episode steps: 276, steps per second:  59, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.497252, mae: 12.426213, mean_q: -16.289309, mean_eps: 0.617107\n",
      "  510875/1200000: episode: 2058, duration: 3.748s, episode steps: 213, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.484138, mae: 12.413705, mean_q: -16.265805, mean_eps: 0.616924\n",
      "  511057/1200000: episode: 2059, duration: 3.343s, episode steps: 182, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.505 [0.000, 3.000],  loss: 0.450791, mae: 12.457760, mean_q: -16.334105, mean_eps: 0.616776\n",
      "  511394/1200000: episode: 2060, duration: 5.791s, episode steps: 337, steps per second:  58, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.413551, mae: 12.432782, mean_q: -16.285784, mean_eps: 0.616581\n",
      "  511587/1200000: episode: 2061, duration: 3.665s, episode steps: 193, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.586382, mae: 12.519683, mean_q: -16.371567, mean_eps: 0.616382\n",
      "  511905/1200000: episode: 2062, duration: 5.536s, episode steps: 318, steps per second:  57, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.527050, mae: 12.428066, mean_q: -16.285182, mean_eps: 0.616191\n",
      "  512128/1200000: episode: 2063, duration: 3.852s, episode steps: 223, steps per second:  58, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.333640, mae: 12.482415, mean_q: -16.382780, mean_eps: 0.615988\n",
      "  512307/1200000: episode: 2064, duration: 3.521s, episode steps: 179, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.492457, mae: 12.420614, mean_q: -16.257161, mean_eps: 0.615837\n",
      "  512613/1200000: episode: 2065, duration: 4.909s, episode steps: 306, steps per second:  62, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.398263, mae: 12.450684, mean_q: -16.323051, mean_eps: 0.615655\n",
      "  512944/1200000: episode: 2066, duration: 5.311s, episode steps: 331, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.395260, mae: 12.468519, mean_q: -16.350499, mean_eps: 0.615417\n",
      "  513128/1200000: episode: 2067, duration: 2.963s, episode steps: 184, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.723 [0.000, 3.000],  loss: 0.528673, mae: 12.571286, mean_q: -16.492153, mean_eps: 0.615223\n",
      "  513434/1200000: episode: 2068, duration: 4.827s, episode steps: 306, steps per second:  63, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.377413, mae: 12.459894, mean_q: -16.340104, mean_eps: 0.615040\n",
      "  513666/1200000: episode: 2069, duration: 3.721s, episode steps: 232, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.346355, mae: 12.422486, mean_q: -16.313206, mean_eps: 0.614838\n",
      "  513847/1200000: episode: 2070, duration: 3.048s, episode steps: 181, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 0.447786, mae: 12.463013, mean_q: -16.313050, mean_eps: 0.614683\n",
      "  514104/1200000: episode: 2071, duration: 4.178s, episode steps: 257, steps per second:  62, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.459762, mae: 12.502375, mean_q: -16.373097, mean_eps: 0.614519\n",
      "  514335/1200000: episode: 2072, duration: 3.775s, episode steps: 231, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.481318, mae: 12.485708, mean_q: -16.367092, mean_eps: 0.614336\n",
      "  514639/1200000: episode: 2073, duration: 4.903s, episode steps: 304, steps per second:  62, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.389025, mae: 12.478957, mean_q: -16.350324, mean_eps: 0.614135\n",
      "  514850/1200000: episode: 2074, duration: 3.370s, episode steps: 211, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.435286, mae: 12.405634, mean_q: -16.265468, mean_eps: 0.613942\n",
      "  515214/1200000: episode: 2075, duration: 5.948s, episode steps: 364, steps per second:  61, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.479828, mae: 12.427970, mean_q: -16.253802, mean_eps: 0.613726\n",
      "  515396/1200000: episode: 2076, duration: 3.137s, episode steps: 182, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.874 [0.000, 3.000],  loss: 0.398221, mae: 12.429502, mean_q: -16.294335, mean_eps: 0.613522\n",
      "  515704/1200000: episode: 2077, duration: 4.964s, episode steps: 308, steps per second:  62, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.492356, mae: 12.357192, mean_q: -16.175535, mean_eps: 0.613338\n",
      "  516138/1200000: episode: 2078, duration: 7.235s, episode steps: 434, steps per second:  60, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.397182, mae: 12.389782, mean_q: -16.256184, mean_eps: 0.613060\n",
      "  516319/1200000: episode: 2079, duration: 3.032s, episode steps: 181, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.751 [0.000, 3.000],  loss: 0.338316, mae: 12.389946, mean_q: -16.262946, mean_eps: 0.612829\n",
      "  516489/1200000: episode: 2080, duration: 2.759s, episode steps: 170, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.456543, mae: 12.542221, mean_q: -16.426539, mean_eps: 0.612697\n",
      "  516701/1200000: episode: 2081, duration: 3.434s, episode steps: 212, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.422706, mae: 12.458480, mean_q: -16.334082, mean_eps: 0.612554\n",
      "  516912/1200000: episode: 2082, duration: 3.459s, episode steps: 211, steps per second:  61, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.447405, mae: 12.362044, mean_q: -16.193275, mean_eps: 0.612395\n",
      "  517087/1200000: episode: 2083, duration: 2.809s, episode steps: 175, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.914 [0.000, 3.000],  loss: 0.397791, mae: 12.536143, mean_q: -16.414241, mean_eps: 0.612251\n",
      "  517265/1200000: episode: 2084, duration: 2.870s, episode steps: 178, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.295024, mae: 12.456343, mean_q: -16.346261, mean_eps: 0.612118\n",
      "  517614/1200000: episode: 2085, duration: 5.610s, episode steps: 349, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.295 [0.000, 3.000],  loss: 0.423724, mae: 12.480806, mean_q: -16.353678, mean_eps: 0.611921\n",
      "  517836/1200000: episode: 2086, duration: 3.622s, episode steps: 222, steps per second:  61, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.477204, mae: 12.458001, mean_q: -16.332658, mean_eps: 0.611707\n",
      "  518006/1200000: episode: 2087, duration: 2.702s, episode steps: 170, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.479868, mae: 12.477472, mean_q: -16.344017, mean_eps: 0.611560\n",
      "  518257/1200000: episode: 2088, duration: 4.167s, episode steps: 251, steps per second:  60, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.404090, mae: 12.490392, mean_q: -16.370427, mean_eps: 0.611402\n",
      "  518489/1200000: episode: 2089, duration: 3.753s, episode steps: 232, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.450788, mae: 12.471563, mean_q: -16.327214, mean_eps: 0.611221\n",
      "  518874/1200000: episode: 2090, duration: 6.119s, episode steps: 385, steps per second:  63, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.424993, mae: 12.466058, mean_q: -16.332797, mean_eps: 0.610989\n",
      "  519103/1200000: episode: 2091, duration: 3.762s, episode steps: 229, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.427968, mae: 12.448026, mean_q: -16.334716, mean_eps: 0.610759\n",
      "  519337/1200000: episode: 2092, duration: 3.794s, episode steps: 234, steps per second:  62, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.488857, mae: 12.601020, mean_q: -16.493833, mean_eps: 0.610585\n",
      "  519651/1200000: episode: 2093, duration: 5.168s, episode steps: 314, steps per second:  61, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.432604, mae: 12.439372, mean_q: -16.308157, mean_eps: 0.610380\n",
      "  519991/1200000: episode: 2094, duration: 5.506s, episode steps: 340, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.420058, mae: 12.483280, mean_q: -16.363677, mean_eps: 0.610135\n",
      "  520163/1200000: episode: 2095, duration: 2.801s, episode steps: 172, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: 0.518694, mae: 12.397007, mean_q: -16.239770, mean_eps: 0.609943\n",
      "  520331/1200000: episode: 2096, duration: 2.765s, episode steps: 168, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.438718, mae: 12.502949, mean_q: -16.385105, mean_eps: 0.609815\n",
      "  520783/1200000: episode: 2097, duration: 7.268s, episode steps: 452, steps per second:  62, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.383004, mae: 12.398570, mean_q: -16.252614, mean_eps: 0.609583\n",
      "  520992/1200000: episode: 2098, duration: 3.360s, episode steps: 209, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.476047, mae: 12.399067, mean_q: -16.252965, mean_eps: 0.609335\n",
      "  521226/1200000: episode: 2099, duration: 3.817s, episode steps: 234, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.363907, mae: 12.319008, mean_q: -16.151986, mean_eps: 0.609169\n",
      "  521454/1200000: episode: 2100, duration: 3.801s, episode steps: 228, steps per second:  60, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.439068, mae: 12.359347, mean_q: -16.209942, mean_eps: 0.608995\n",
      "  521668/1200000: episode: 2101, duration: 3.417s, episode steps: 214, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.391985, mae: 12.297811, mean_q: -16.115390, mean_eps: 0.608830\n",
      "  521939/1200000: episode: 2102, duration: 4.475s, episode steps: 271, steps per second:  61, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.447131, mae: 12.456854, mean_q: -16.335285, mean_eps: 0.608648\n",
      "  522188/1200000: episode: 2103, duration: 4.093s, episode steps: 249, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.771 [0.000, 3.000],  loss: 0.338619, mae: 12.281611, mean_q: -16.127027, mean_eps: 0.608453\n",
      "  522525/1200000: episode: 2104, duration: 5.405s, episode steps: 337, steps per second:  62, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.446944, mae: 12.483949, mean_q: -16.365258, mean_eps: 0.608233\n",
      "  522687/1200000: episode: 2105, duration: 2.634s, episode steps: 162, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.384499, mae: 12.379026, mean_q: -16.241860, mean_eps: 0.608046\n",
      "  522897/1200000: episode: 2106, duration: 3.629s, episode steps: 210, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.340972, mae: 12.482311, mean_q: -16.400839, mean_eps: 0.607906\n",
      "  523107/1200000: episode: 2107, duration: 3.502s, episode steps: 210, steps per second:  60, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.406554, mae: 12.437098, mean_q: -16.299911, mean_eps: 0.607749\n",
      "  523344/1200000: episode: 2108, duration: 3.860s, episode steps: 237, steps per second:  61, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.488501, mae: 12.368753, mean_q: -16.199540, mean_eps: 0.607581\n",
      "  523516/1200000: episode: 2109, duration: 3.347s, episode steps: 172, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.547955, mae: 12.457138, mean_q: -16.313122, mean_eps: 0.607428\n",
      "  523739/1200000: episode: 2110, duration: 3.900s, episode steps: 223, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.508402, mae: 12.385922, mean_q: -16.215885, mean_eps: 0.607280\n",
      "  524023/1200000: episode: 2111, duration: 4.936s, episode steps: 284, steps per second:  58, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.421192, mae: 12.467984, mean_q: -16.336436, mean_eps: 0.607090\n",
      "  524331/1200000: episode: 2112, duration: 5.423s, episode steps: 308, steps per second:  57, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.439703, mae: 12.443415, mean_q: -16.300252, mean_eps: 0.606868\n",
      "  524538/1200000: episode: 2113, duration: 3.647s, episode steps: 207, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.551167, mae: 12.507390, mean_q: -16.382421, mean_eps: 0.606675\n",
      "  524822/1200000: episode: 2114, duration: 4.906s, episode steps: 284, steps per second:  58, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.442806, mae: 12.383360, mean_q: -16.217729, mean_eps: 0.606490\n",
      "  525054/1200000: episode: 2115, duration: 4.113s, episode steps: 232, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.538351, mae: 12.479140, mean_q: -16.350257, mean_eps: 0.606297\n",
      "  525326/1200000: episode: 2116, duration: 4.772s, episode steps: 272, steps per second:  57, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.508480, mae: 12.381543, mean_q: -16.240657, mean_eps: 0.606108\n",
      "  525580/1200000: episode: 2117, duration: 4.571s, episode steps: 254, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.378248, mae: 12.320315, mean_q: -16.171783, mean_eps: 0.605911\n",
      "  525980/1200000: episode: 2118, duration: 7.091s, episode steps: 400, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.415786, mae: 12.399178, mean_q: -16.252619, mean_eps: 0.605665\n",
      "  526212/1200000: episode: 2119, duration: 4.106s, episode steps: 232, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.347044, mae: 12.403014, mean_q: -16.271016, mean_eps: 0.605428\n",
      "  526381/1200000: episode: 2120, duration: 3.026s, episode steps: 169, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.822 [0.000, 3.000],  loss: 0.423997, mae: 12.447797, mean_q: -16.310807, mean_eps: 0.605278\n",
      "  526588/1200000: episode: 2121, duration: 3.860s, episode steps: 207, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.438612, mae: 12.385897, mean_q: -16.243306, mean_eps: 0.605137\n",
      "  526795/1200000: episode: 2122, duration: 3.697s, episode steps: 207, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.382989, mae: 12.478647, mean_q: -16.350703, mean_eps: 0.604982\n",
      "  526959/1200000: episode: 2123, duration: 2.955s, episode steps: 164, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.495313, mae: 12.337135, mean_q: -16.154384, mean_eps: 0.604843\n",
      "  527186/1200000: episode: 2124, duration: 4.282s, episode steps: 227, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.426260, mae: 12.345193, mean_q: -16.178897, mean_eps: 0.604696\n",
      "  527364/1200000: episode: 2125, duration: 3.069s, episode steps: 178, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.803 [0.000, 3.000],  loss: 0.556935, mae: 12.394527, mean_q: -16.213161, mean_eps: 0.604544\n",
      "  527574/1200000: episode: 2126, duration: 3.687s, episode steps: 210, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.394911, mae: 12.420555, mean_q: -16.284679, mean_eps: 0.604399\n",
      "  527755/1200000: episode: 2127, duration: 3.300s, episode steps: 181, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.394830, mae: 12.374736, mean_q: -16.227124, mean_eps: 0.604252\n",
      "  527933/1200000: episode: 2128, duration: 3.137s, episode steps: 178, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 0.351741, mae: 12.427888, mean_q: -16.319807, mean_eps: 0.604117\n",
      "  528273/1200000: episode: 2129, duration: 5.905s, episode steps: 340, steps per second:  58, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.479367, mae: 12.428957, mean_q: -16.264593, mean_eps: 0.603923\n",
      "  528660/1200000: episode: 2130, duration: 6.921s, episode steps: 387, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.443789, mae: 12.348688, mean_q: -16.177087, mean_eps: 0.603651\n",
      "  528895/1200000: episode: 2131, duration: 4.535s, episode steps: 235, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.380907, mae: 12.399382, mean_q: -16.266886, mean_eps: 0.603417\n",
      "  529070/1200000: episode: 2132, duration: 3.065s, episode steps: 175, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.525594, mae: 12.454863, mean_q: -16.290990, mean_eps: 0.603263\n",
      "  529367/1200000: episode: 2133, duration: 5.275s, episode steps: 297, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.502294, mae: 12.403787, mean_q: -16.239112, mean_eps: 0.603086\n",
      "  529643/1200000: episode: 2134, duration: 4.871s, episode steps: 276, steps per second:  57, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.419803, mae: 12.463160, mean_q: -16.339162, mean_eps: 0.602872\n",
      "  529884/1200000: episode: 2135, duration: 4.264s, episode steps: 241, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.396565, mae: 12.444573, mean_q: -16.325014, mean_eps: 0.602678\n",
      "  530101/1200000: episode: 2136, duration: 3.906s, episode steps: 217, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.412025, mae: 12.193564, mean_q: -15.957695, mean_eps: 0.602506\n",
      "  530312/1200000: episode: 2137, duration: 3.785s, episode steps: 211, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.380111, mae: 12.122289, mean_q: -15.879713, mean_eps: 0.602346\n",
      "  530537/1200000: episode: 2138, duration: 4.070s, episode steps: 225, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.490258, mae: 12.109838, mean_q: -15.859198, mean_eps: 0.602182\n",
      "  530712/1200000: episode: 2139, duration: 3.363s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.417056, mae: 12.118828, mean_q: -15.868295, mean_eps: 0.602032\n",
      "  530917/1200000: episode: 2140, duration: 3.708s, episode steps: 205, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.367992, mae: 12.062532, mean_q: -15.806240, mean_eps: 0.601889\n",
      "  531131/1200000: episode: 2141, duration: 3.800s, episode steps: 214, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.438499, mae: 12.111143, mean_q: -15.843002, mean_eps: 0.601732\n",
      "  531470/1200000: episode: 2142, duration: 6.042s, episode steps: 339, steps per second:  56, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.352860, mae: 12.135065, mean_q: -15.907988, mean_eps: 0.601525\n",
      "  531679/1200000: episode: 2143, duration: 3.637s, episode steps: 209, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.455106, mae: 12.151493, mean_q: -15.920817, mean_eps: 0.601320\n",
      "  532019/1200000: episode: 2144, duration: 5.999s, episode steps: 340, steps per second:  57, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.449846, mae: 12.034943, mean_q: -15.743252, mean_eps: 0.601114\n",
      "  532224/1200000: episode: 2145, duration: 3.986s, episode steps: 205, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.554040, mae: 12.011218, mean_q: -15.685242, mean_eps: 0.600909\n",
      "  532527/1200000: episode: 2146, duration: 5.446s, episode steps: 303, steps per second:  56, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.428261, mae: 12.059643, mean_q: -15.790014, mean_eps: 0.600719\n",
      "  532713/1200000: episode: 2147, duration: 3.330s, episode steps: 186, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.465056, mae: 12.136303, mean_q: -15.888684, mean_eps: 0.600535\n",
      "  532887/1200000: episode: 2148, duration: 3.146s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.178 [0.000, 3.000],  loss: 0.519646, mae: 12.112897, mean_q: -15.836500, mean_eps: 0.600400\n",
      "  533058/1200000: episode: 2149, duration: 3.086s, episode steps: 171, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.431684, mae: 12.195110, mean_q: -15.969446, mean_eps: 0.600271\n",
      "  533249/1200000: episode: 2150, duration: 3.377s, episode steps: 191, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.357618, mae: 11.941034, mean_q: -15.649055, mean_eps: 0.600135\n",
      "  533496/1200000: episode: 2151, duration: 4.511s, episode steps: 247, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.476806, mae: 12.112543, mean_q: -15.881086, mean_eps: 0.599971\n",
      "  533674/1200000: episode: 2152, duration: 3.150s, episode steps: 178, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.413003, mae: 12.044670, mean_q: -15.765419, mean_eps: 0.599812\n",
      "  533846/1200000: episode: 2153, duration: 3.025s, episode steps: 172, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.442 [0.000, 3.000],  loss: 0.572485, mae: 12.033457, mean_q: -15.741389, mean_eps: 0.599680\n",
      "  534063/1200000: episode: 2154, duration: 3.984s, episode steps: 217, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.369895, mae: 12.067732, mean_q: -15.835860, mean_eps: 0.599534\n",
      "  534290/1200000: episode: 2155, duration: 4.115s, episode steps: 227, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.859 [0.000, 3.000],  loss: 0.352802, mae: 12.046560, mean_q: -15.779507, mean_eps: 0.599368\n",
      "  534494/1200000: episode: 2156, duration: 3.638s, episode steps: 204, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.448136, mae: 12.152374, mean_q: -15.911833, mean_eps: 0.599206\n",
      "  534732/1200000: episode: 2157, duration: 4.234s, episode steps: 238, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.457794, mae: 12.101563, mean_q: -15.835398, mean_eps: 0.599041\n",
      "  534979/1200000: episode: 2158, duration: 4.327s, episode steps: 247, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.399139, mae: 12.060356, mean_q: -15.797288, mean_eps: 0.598859\n",
      "  535220/1200000: episode: 2159, duration: 4.180s, episode steps: 241, steps per second:  58, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.452635, mae: 12.000042, mean_q: -15.723764, mean_eps: 0.598676\n",
      "  535383/1200000: episode: 2160, duration: 2.987s, episode steps: 163, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 0.443266, mae: 12.070787, mean_q: -15.805748, mean_eps: 0.598524\n",
      "  535564/1200000: episode: 2161, duration: 3.181s, episode steps: 181, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.449802, mae: 11.993372, mean_q: -15.717782, mean_eps: 0.598395\n",
      "  535828/1200000: episode: 2162, duration: 4.656s, episode steps: 264, steps per second:  57, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.404997, mae: 12.029739, mean_q: -15.745557, mean_eps: 0.598228\n",
      "  536040/1200000: episode: 2163, duration: 3.904s, episode steps: 212, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.404710, mae: 11.949871, mean_q: -15.642368, mean_eps: 0.598050\n",
      "  536208/1200000: episode: 2164, duration: 3.017s, episode steps: 168, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.410045, mae: 12.056893, mean_q: -15.788529, mean_eps: 0.597907\n",
      "  536436/1200000: episode: 2165, duration: 3.969s, episode steps: 228, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.488599, mae: 12.087419, mean_q: -15.812629, mean_eps: 0.597759\n",
      "  536770/1200000: episode: 2166, duration: 5.939s, episode steps: 334, steps per second:  56, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.390894, mae: 12.027022, mean_q: -15.755598, mean_eps: 0.597548\n",
      "  536944/1200000: episode: 2167, duration: 3.252s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 0.409365, mae: 11.985659, mean_q: -15.715542, mean_eps: 0.597358\n",
      "  537172/1200000: episode: 2168, duration: 3.984s, episode steps: 228, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.309493, mae: 12.062186, mean_q: -15.817652, mean_eps: 0.597207\n",
      "  537347/1200000: episode: 2169, duration: 3.218s, episode steps: 175, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.521080, mae: 12.013733, mean_q: -15.695967, mean_eps: 0.597056\n",
      "  537577/1200000: episode: 2170, duration: 4.089s, episode steps: 230, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.411995, mae: 11.978446, mean_q: -15.677775, mean_eps: 0.596904\n",
      "  537762/1200000: episode: 2171, duration: 3.381s, episode steps: 185, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 0.473524, mae: 12.007072, mean_q: -15.711945, mean_eps: 0.596748\n",
      "  538090/1200000: episode: 2172, duration: 5.841s, episode steps: 328, steps per second:  56, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.428143, mae: 12.020820, mean_q: -15.714944, mean_eps: 0.596556\n",
      "  538327/1200000: episode: 2173, duration: 4.298s, episode steps: 237, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.517915, mae: 12.095810, mean_q: -15.825929, mean_eps: 0.596344\n",
      "  538482/1200000: episode: 2174, duration: 2.735s, episode steps: 155, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.472962, mae: 11.969571, mean_q: -15.654390, mean_eps: 0.596197\n",
      "  538726/1200000: episode: 2175, duration: 4.354s, episode steps: 244, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.352811, mae: 12.030338, mean_q: -15.757062, mean_eps: 0.596047\n",
      "  538962/1200000: episode: 2176, duration: 4.169s, episode steps: 236, steps per second:  57, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.468453, mae: 12.074963, mean_q: -15.798403, mean_eps: 0.595867\n",
      "  539237/1200000: episode: 2177, duration: 4.837s, episode steps: 275, steps per second:  57, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.443645, mae: 12.094876, mean_q: -15.830569, mean_eps: 0.595676\n",
      "  539406/1200000: episode: 2178, duration: 3.099s, episode steps: 169, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.369697, mae: 12.063992, mean_q: -15.801542, mean_eps: 0.595509\n",
      "  539585/1200000: episode: 2179, duration: 3.236s, episode steps: 179, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.732 [0.000, 3.000],  loss: 0.521202, mae: 12.073410, mean_q: -15.774410, mean_eps: 0.595379\n",
      "  539755/1200000: episode: 2180, duration: 3.102s, episode steps: 170, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.441374, mae: 12.030222, mean_q: -15.719286, mean_eps: 0.595248\n",
      "  539923/1200000: episode: 2181, duration: 3.070s, episode steps: 168, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.732 [0.000, 3.000],  loss: 0.301195, mae: 12.009899, mean_q: -15.749165, mean_eps: 0.595121\n",
      "  540198/1200000: episode: 2182, duration: 4.919s, episode steps: 275, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.406957, mae: 11.862727, mean_q: -15.512614, mean_eps: 0.594955\n",
      "  540405/1200000: episode: 2183, duration: 3.686s, episode steps: 207, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.430882, mae: 11.818106, mean_q: -15.471246, mean_eps: 0.594774\n",
      "  540577/1200000: episode: 2184, duration: 3.119s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.756 [0.000, 3.000],  loss: 0.402525, mae: 11.839227, mean_q: -15.507071, mean_eps: 0.594632\n",
      "  540783/1200000: episode: 2185, duration: 3.653s, episode steps: 206, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.343197, mae: 11.805135, mean_q: -15.484538, mean_eps: 0.594490\n",
      "  541180/1200000: episode: 2186, duration: 6.920s, episode steps: 397, steps per second:  57, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.399533, mae: 11.743720, mean_q: -15.380719, mean_eps: 0.594264\n",
      "  541359/1200000: episode: 2187, duration: 3.335s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.493379, mae: 11.925683, mean_q: -15.609932, mean_eps: 0.594048\n",
      "  541579/1200000: episode: 2188, duration: 3.878s, episode steps: 220, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.832 [0.000, 3.000],  loss: 0.418811, mae: 11.705328, mean_q: -15.322040, mean_eps: 0.593899\n",
      "  541786/1200000: episode: 2189, duration: 3.659s, episode steps: 207, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.932 [0.000, 3.000],  loss: 0.441780, mae: 11.854423, mean_q: -15.515601, mean_eps: 0.593739\n",
      "  541967/1200000: episode: 2190, duration: 3.333s, episode steps: 181, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.718 [0.000, 3.000],  loss: 0.382212, mae: 11.756380, mean_q: -15.405368, mean_eps: 0.593593\n",
      "  542292/1200000: episode: 2191, duration: 5.740s, episode steps: 325, steps per second:  57, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.445687, mae: 11.821274, mean_q: -15.478472, mean_eps: 0.593403\n",
      "  542504/1200000: episode: 2192, duration: 3.751s, episode steps: 212, steps per second:  57, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.442479, mae: 11.804087, mean_q: -15.435579, mean_eps: 0.593202\n",
      "  542674/1200000: episode: 2193, duration: 3.120s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.397009, mae: 11.852028, mean_q: -15.527906, mean_eps: 0.593059\n",
      "  542853/1200000: episode: 2194, duration: 3.148s, episode steps: 179, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.648 [0.000, 3.000],  loss: 0.421804, mae: 11.836714, mean_q: -15.497098, mean_eps: 0.592928\n",
      "  543035/1200000: episode: 2195, duration: 3.189s, episode steps: 182, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.879 [0.000, 3.000],  loss: 0.356260, mae: 11.822313, mean_q: -15.492345, mean_eps: 0.592792\n",
      "  543304/1200000: episode: 2196, duration: 4.987s, episode steps: 269, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.363922, mae: 11.835866, mean_q: -15.496540, mean_eps: 0.592623\n",
      "  543510/1200000: episode: 2197, duration: 3.651s, episode steps: 206, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.474737, mae: 11.861540, mean_q: -15.523721, mean_eps: 0.592445\n",
      "  543679/1200000: episode: 2198, duration: 3.175s, episode steps: 169, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.449186, mae: 11.742668, mean_q: -15.357132, mean_eps: 0.592304\n",
      "  543856/1200000: episode: 2199, duration: 4.363s, episode steps: 177, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.921 [0.000, 3.000],  loss: 0.371934, mae: 11.853910, mean_q: -15.516071, mean_eps: 0.592175\n",
      "  544083/1200000: episode: 2200, duration: 4.070s, episode steps: 227, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.404654, mae: 11.862578, mean_q: -15.528228, mean_eps: 0.592023\n",
      "  544359/1200000: episode: 2201, duration: 5.036s, episode steps: 276, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.451872, mae: 11.762823, mean_q: -15.394933, mean_eps: 0.591835\n",
      "  544627/1200000: episode: 2202, duration: 4.904s, episode steps: 268, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.437630, mae: 11.786818, mean_q: -15.411594, mean_eps: 0.591631\n",
      "  544873/1200000: episode: 2203, duration: 4.370s, episode steps: 246, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.503753, mae: 11.828988, mean_q: -15.458702, mean_eps: 0.591438\n",
      "  545047/1200000: episode: 2204, duration: 3.168s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 0.464814, mae: 11.716386, mean_q: -15.327930, mean_eps: 0.591280\n",
      "  545223/1200000: episode: 2205, duration: 3.201s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.376001, mae: 11.839334, mean_q: -15.500195, mean_eps: 0.591149\n",
      "  545386/1200000: episode: 2206, duration: 2.957s, episode steps: 163, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.687 [0.000, 3.000],  loss: 0.418932, mae: 11.649622, mean_q: -15.265089, mean_eps: 0.591022\n",
      "  545654/1200000: episode: 2207, duration: 4.878s, episode steps: 268, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.451013, mae: 11.736613, mean_q: -15.340592, mean_eps: 0.590860\n",
      "  545908/1200000: episode: 2208, duration: 4.578s, episode steps: 254, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.460682, mae: 11.783955, mean_q: -15.422815, mean_eps: 0.590665\n",
      "  546210/1200000: episode: 2209, duration: 5.321s, episode steps: 302, steps per second:  57, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.430866, mae: 11.780134, mean_q: -15.411953, mean_eps: 0.590456\n",
      "  546397/1200000: episode: 2210, duration: 3.347s, episode steps: 187, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.368847, mae: 11.831595, mean_q: -15.486098, mean_eps: 0.590273\n",
      "  546641/1200000: episode: 2211, duration: 4.471s, episode steps: 244, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.359365, mae: 11.820595, mean_q: -15.492767, mean_eps: 0.590111\n",
      "  546813/1200000: episode: 2212, duration: 3.156s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.369603, mae: 11.699523, mean_q: -15.324112, mean_eps: 0.589955\n",
      "  547035/1200000: episode: 2213, duration: 3.981s, episode steps: 222, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.968 [0.000, 3.000],  loss: 0.363930, mae: 11.742768, mean_q: -15.384515, mean_eps: 0.589807\n",
      "  547276/1200000: episode: 2214, duration: 4.419s, episode steps: 241, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.418072, mae: 11.786525, mean_q: -15.427968, mean_eps: 0.589634\n",
      "  547453/1200000: episode: 2215, duration: 3.137s, episode steps: 177, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.451579, mae: 11.817739, mean_q: -15.469724, mean_eps: 0.589477\n",
      "  547631/1200000: episode: 2216, duration: 3.231s, episode steps: 178, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.491098, mae: 11.834753, mean_q: -15.462239, mean_eps: 0.589344\n",
      "  547887/1200000: episode: 2217, duration: 4.591s, episode steps: 256, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.446269, mae: 11.683608, mean_q: -15.293451, mean_eps: 0.589181\n",
      "  548090/1200000: episode: 2218, duration: 3.684s, episode steps: 203, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.798 [0.000, 3.000],  loss: 0.362554, mae: 11.797294, mean_q: -15.470114, mean_eps: 0.589009\n",
      "  548298/1200000: episode: 2219, duration: 3.687s, episode steps: 208, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.558292, mae: 11.743510, mean_q: -15.321929, mean_eps: 0.588855\n",
      "  548709/1200000: episode: 2220, duration: 7.354s, episode steps: 411, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.344910, mae: 11.788474, mean_q: -15.445151, mean_eps: 0.588623\n",
      "  548950/1200000: episode: 2221, duration: 4.332s, episode steps: 241, steps per second:  56, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.361813, mae: 11.783135, mean_q: -15.417306, mean_eps: 0.588378\n",
      "  549126/1200000: episode: 2222, duration: 3.192s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.943 [0.000, 3.000],  loss: 0.356019, mae: 11.722789, mean_q: -15.335164, mean_eps: 0.588222\n",
      "  549313/1200000: episode: 2223, duration: 3.598s, episode steps: 187, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.102 [0.000, 3.000],  loss: 0.377002, mae: 11.762683, mean_q: -15.402141, mean_eps: 0.588086\n",
      "  549521/1200000: episode: 2224, duration: 3.690s, episode steps: 208, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.460027, mae: 11.711982, mean_q: -15.318695, mean_eps: 0.587938\n",
      "  549821/1200000: episode: 2225, duration: 5.317s, episode steps: 300, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.457826, mae: 11.849056, mean_q: -15.486364, mean_eps: 0.587747\n",
      "  550024/1200000: episode: 2226, duration: 3.786s, episode steps: 203, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 0.480124, mae: 11.718458, mean_q: -15.310586, mean_eps: 0.587558\n",
      "  550219/1200000: episode: 2227, duration: 3.551s, episode steps: 195, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.662 [0.000, 3.000],  loss: 0.366762, mae: 11.316292, mean_q: -14.821741, mean_eps: 0.587409\n",
      "  550438/1200000: episode: 2228, duration: 3.919s, episode steps: 219, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.393359, mae: 11.351186, mean_q: -14.858583, mean_eps: 0.587254\n",
      "  550614/1200000: episode: 2229, duration: 3.320s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.406335, mae: 11.403954, mean_q: -14.928791, mean_eps: 0.587106\n",
      "  550788/1200000: episode: 2230, duration: 3.382s, episode steps: 174, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.925 [0.000, 3.000],  loss: 0.435229, mae: 11.424037, mean_q: -14.957715, mean_eps: 0.586975\n",
      "  550954/1200000: episode: 2231, duration: 3.153s, episode steps: 166, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.723 [0.000, 3.000],  loss: 0.474970, mae: 11.377791, mean_q: -14.880867, mean_eps: 0.586847\n",
      "  551122/1200000: episode: 2232, duration: 3.070s, episode steps: 168, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.385767, mae: 11.410559, mean_q: -14.940500, mean_eps: 0.586722\n",
      "  551303/1200000: episode: 2233, duration: 3.295s, episode steps: 181, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.505071, mae: 11.255138, mean_q: -14.707175, mean_eps: 0.586591\n",
      "  551580/1200000: episode: 2234, duration: 4.910s, episode steps: 277, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.393128, mae: 11.376156, mean_q: -14.893332, mean_eps: 0.586419\n",
      "  551800/1200000: episode: 2235, duration: 3.986s, episode steps: 220, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.329597, mae: 11.237326, mean_q: -14.732149, mean_eps: 0.586233\n",
      "  551986/1200000: episode: 2236, duration: 3.335s, episode steps: 186, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.382308, mae: 11.351399, mean_q: -14.837574, mean_eps: 0.586081\n",
      "  552203/1200000: episode: 2237, duration: 3.873s, episode steps: 217, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.314066, mae: 11.276449, mean_q: -14.763638, mean_eps: 0.585929\n",
      "  552431/1200000: episode: 2238, duration: 4.133s, episode steps: 228, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.419398, mae: 11.354233, mean_q: -14.843773, mean_eps: 0.585763\n",
      "  552617/1200000: episode: 2239, duration: 3.352s, episode steps: 186, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.350988, mae: 11.224389, mean_q: -14.665703, mean_eps: 0.585607\n",
      "  552865/1200000: episode: 2240, duration: 4.403s, episode steps: 248, steps per second:  56, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.363123, mae: 11.284295, mean_q: -14.779071, mean_eps: 0.585445\n",
      "  553164/1200000: episode: 2241, duration: 5.465s, episode steps: 299, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.486949, mae: 11.300115, mean_q: -14.744758, mean_eps: 0.585240\n",
      "  553346/1200000: episode: 2242, duration: 3.308s, episode steps: 182, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.698 [0.000, 3.000],  loss: 0.396172, mae: 11.273036, mean_q: -14.725777, mean_eps: 0.585059\n",
      "  553523/1200000: episode: 2243, duration: 3.298s, episode steps: 177, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.921 [0.000, 3.000],  loss: 0.311036, mae: 11.318272, mean_q: -14.815577, mean_eps: 0.584924\n",
      "  553694/1200000: episode: 2244, duration: 3.117s, episode steps: 171, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.328337, mae: 11.273839, mean_q: -14.764055, mean_eps: 0.584794\n",
      "  553867/1200000: episode: 2245, duration: 3.174s, episode steps: 173, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: 0.314568, mae: 11.291858, mean_q: -14.780378, mean_eps: 0.584665\n",
      "  554070/1200000: episode: 2246, duration: 3.616s, episode steps: 203, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.478322, mae: 11.366008, mean_q: -14.876004, mean_eps: 0.584524\n",
      "  554284/1200000: episode: 2247, duration: 3.906s, episode steps: 214, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.323202, mae: 11.227504, mean_q: -14.713436, mean_eps: 0.584368\n",
      "  554460/1200000: episode: 2248, duration: 3.249s, episode steps: 176, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.699 [0.000, 3.000],  loss: 0.361578, mae: 11.348365, mean_q: -14.853013, mean_eps: 0.584221\n",
      "  554662/1200000: episode: 2249, duration: 3.605s, episode steps: 202, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.433255, mae: 11.357045, mean_q: -14.870614, mean_eps: 0.584080\n",
      "  554837/1200000: episode: 2250, duration: 3.266s, episode steps: 175, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.418619, mae: 11.235751, mean_q: -14.685828, mean_eps: 0.583938\n",
      "  555044/1200000: episode: 2251, duration: 3.696s, episode steps: 207, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.826 [0.000, 3.000],  loss: 0.388488, mae: 11.380818, mean_q: -14.893261, mean_eps: 0.583795\n",
      "  555260/1200000: episode: 2252, duration: 3.876s, episode steps: 216, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.336907, mae: 11.294222, mean_q: -14.799562, mean_eps: 0.583636\n",
      "  555564/1200000: episode: 2253, duration: 5.555s, episode steps: 304, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.412508, mae: 11.312316, mean_q: -14.802485, mean_eps: 0.583441\n",
      "  555779/1200000: episode: 2254, duration: 3.853s, episode steps: 215, steps per second:  56, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.504445, mae: 11.319662, mean_q: -14.783162, mean_eps: 0.583247\n",
      "  555959/1200000: episode: 2255, duration: 3.265s, episode steps: 180, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.452628, mae: 11.308535, mean_q: -14.824519, mean_eps: 0.583099\n",
      "  556139/1200000: episode: 2256, duration: 3.274s, episode steps: 180, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.839 [0.000, 3.000],  loss: 0.541930, mae: 11.337781, mean_q: -14.801376, mean_eps: 0.582964\n",
      "  556447/1200000: episode: 2257, duration: 5.613s, episode steps: 308, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.384189, mae: 11.286665, mean_q: -14.776043, mean_eps: 0.582781\n",
      "  556607/1200000: episode: 2258, duration: 2.988s, episode steps: 160, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.694 [0.000, 3.000],  loss: 0.304380, mae: 11.244671, mean_q: -14.753960, mean_eps: 0.582605\n",
      "  556855/1200000: episode: 2259, duration: 4.718s, episode steps: 248, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.454300, mae: 11.322912, mean_q: -14.801710, mean_eps: 0.582452\n",
      "  557041/1200000: episode: 2260, duration: 3.440s, episode steps: 186, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.339344, mae: 11.293909, mean_q: -14.798045, mean_eps: 0.582289\n",
      "  557283/1200000: episode: 2261, duration: 4.370s, episode steps: 242, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.378413, mae: 11.302000, mean_q: -14.814913, mean_eps: 0.582129\n",
      "  557619/1200000: episode: 2262, duration: 6.233s, episode steps: 336, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.338040, mae: 11.277361, mean_q: -14.765234, mean_eps: 0.581912\n",
      "  557809/1200000: episode: 2263, duration: 3.482s, episode steps: 190, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.338388, mae: 11.270436, mean_q: -14.733081, mean_eps: 0.581715\n",
      "  558001/1200000: episode: 2264, duration: 3.454s, episode steps: 192, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.802 [0.000, 3.000],  loss: 0.393903, mae: 11.290051, mean_q: -14.754361, mean_eps: 0.581572\n",
      "  558351/1200000: episode: 2265, duration: 6.425s, episode steps: 350, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.317186, mae: 11.318186, mean_q: -14.823409, mean_eps: 0.581368\n",
      "  558569/1200000: episode: 2266, duration: 4.099s, episode steps: 218, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.503673, mae: 11.337493, mean_q: -14.793663, mean_eps: 0.581155\n",
      "  558846/1200000: episode: 2267, duration: 5.158s, episode steps: 277, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.350708, mae: 11.268293, mean_q: -14.748871, mean_eps: 0.580970\n",
      "  559084/1200000: episode: 2268, duration: 4.319s, episode steps: 238, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.431926, mae: 11.264709, mean_q: -14.721198, mean_eps: 0.580777\n",
      "  559426/1200000: episode: 2269, duration: 6.188s, episode steps: 342, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.377523, mae: 11.302164, mean_q: -14.789976, mean_eps: 0.580559\n",
      "  559661/1200000: episode: 2270, duration: 4.433s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.387459, mae: 11.309099, mean_q: -14.801701, mean_eps: 0.580343\n",
      "  559868/1200000: episode: 2271, duration: 3.882s, episode steps: 207, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.438681, mae: 11.215468, mean_q: -14.668997, mean_eps: 0.580177\n",
      "  560044/1200000: episode: 2272, duration: 3.174s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.344044, mae: 11.310363, mean_q: -14.803254, mean_eps: 0.580033\n",
      "  560262/1200000: episode: 2273, duration: 4.118s, episode steps: 218, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.497179, mae: 11.355682, mean_q: -14.863606, mean_eps: 0.579886\n",
      "  560508/1200000: episode: 2274, duration: 4.535s, episode steps: 246, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.388257, mae: 11.262772, mean_q: -14.756956, mean_eps: 0.579712\n",
      "  560694/1200000: episode: 2275, duration: 3.538s, episode steps: 186, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.925 [0.000, 3.000],  loss: 0.307342, mae: 11.392535, mean_q: -14.924834, mean_eps: 0.579550\n",
      "  560980/1200000: episode: 2276, duration: 5.197s, episode steps: 286, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.356626, mae: 11.253396, mean_q: -14.750548, mean_eps: 0.579373\n",
      "  561167/1200000: episode: 2277, duration: 3.391s, episode steps: 187, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.749 [0.000, 3.000],  loss: 0.337983, mae: 11.224376, mean_q: -14.717701, mean_eps: 0.579195\n",
      "  561338/1200000: episode: 2278, duration: 3.267s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.678 [0.000, 3.000],  loss: 0.432376, mae: 11.395802, mean_q: -14.911570, mean_eps: 0.579061\n",
      "  561567/1200000: episode: 2279, duration: 4.174s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.368656, mae: 11.272593, mean_q: -14.766802, mean_eps: 0.578911\n",
      "  561743/1200000: episode: 2280, duration: 3.380s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.384298, mae: 11.344252, mean_q: -14.850225, mean_eps: 0.578759\n",
      "  561959/1200000: episode: 2281, duration: 3.970s, episode steps: 216, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.388928, mae: 11.325283, mean_q: -14.834637, mean_eps: 0.578612\n",
      "  562163/1200000: episode: 2282, duration: 3.702s, episode steps: 204, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.392548, mae: 11.380409, mean_q: -14.898470, mean_eps: 0.578455\n",
      "  562345/1200000: episode: 2283, duration: 3.424s, episode steps: 182, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.742 [0.000, 3.000],  loss: 0.314648, mae: 11.173464, mean_q: -14.645998, mean_eps: 0.578310\n",
      "  562558/1200000: episode: 2284, duration: 3.867s, episode steps: 213, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.278798, mae: 11.254723, mean_q: -14.772458, mean_eps: 0.578162\n",
      "  562794/1200000: episode: 2285, duration: 4.250s, episode steps: 236, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.347661, mae: 11.301357, mean_q: -14.818909, mean_eps: 0.577993\n",
      "  562960/1200000: episode: 2286, duration: 3.150s, episode steps: 166, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.753 [0.000, 3.000],  loss: 0.379035, mae: 11.279534, mean_q: -14.749713, mean_eps: 0.577843\n",
      "  563146/1200000: episode: 2287, duration: 3.470s, episode steps: 186, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.715 [0.000, 3.000],  loss: 0.419114, mae: 11.314247, mean_q: -14.837975, mean_eps: 0.577711\n",
      "  563329/1200000: episode: 2288, duration: 3.437s, episode steps: 183, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: 0.415442, mae: 11.298933, mean_q: -14.790836, mean_eps: 0.577572\n",
      "  563506/1200000: episode: 2289, duration: 3.309s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.723 [0.000, 3.000],  loss: 0.408375, mae: 11.283021, mean_q: -14.782341, mean_eps: 0.577437\n",
      "  563747/1200000: episode: 2290, duration: 4.439s, episode steps: 241, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.340711, mae: 11.213210, mean_q: -14.699954, mean_eps: 0.577281\n",
      "  563938/1200000: episode: 2291, duration: 3.442s, episode steps: 191, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.486021, mae: 11.245049, mean_q: -14.705308, mean_eps: 0.577118\n",
      "  564140/1200000: episode: 2292, duration: 3.852s, episode steps: 202, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.434420, mae: 11.339452, mean_q: -14.839330, mean_eps: 0.576971\n",
      "  564309/1200000: episode: 2293, duration: 3.228s, episode steps: 169, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.781 [0.000, 3.000],  loss: 0.382244, mae: 11.306429, mean_q: -14.800846, mean_eps: 0.576832\n",
      "  564586/1200000: episode: 2294, duration: 5.009s, episode steps: 277, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.318740, mae: 11.240968, mean_q: -14.736303, mean_eps: 0.576665\n",
      "  564849/1200000: episode: 2295, duration: 4.807s, episode steps: 263, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.485103, mae: 11.326431, mean_q: -14.810509, mean_eps: 0.576462\n",
      "  565124/1200000: episode: 2296, duration: 5.049s, episode steps: 275, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.422196, mae: 11.345147, mean_q: -14.835200, mean_eps: 0.576260\n",
      "  565299/1200000: episode: 2297, duration: 3.285s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.495824, mae: 11.340219, mean_q: -14.843988, mean_eps: 0.576092\n",
      "  565505/1200000: episode: 2298, duration: 3.954s, episode steps: 206, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.694 [0.000, 3.000],  loss: 0.339126, mae: 11.325523, mean_q: -14.856387, mean_eps: 0.575949\n",
      "  565776/1200000: episode: 2299, duration: 4.985s, episode steps: 271, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.408687, mae: 11.343499, mean_q: -14.830235, mean_eps: 0.575770\n",
      "  566010/1200000: episode: 2300, duration: 4.285s, episode steps: 234, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.415683, mae: 11.361889, mean_q: -14.885556, mean_eps: 0.575581\n",
      "  566300/1200000: episode: 2301, duration: 5.289s, episode steps: 290, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.299363, mae: 11.251178, mean_q: -14.740453, mean_eps: 0.575384\n",
      "  566635/1200000: episode: 2302, duration: 6.220s, episode steps: 335, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.328409, mae: 11.280169, mean_q: -14.788997, mean_eps: 0.575150\n",
      "  566819/1200000: episode: 2303, duration: 3.420s, episode steps: 184, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.402 [0.000, 3.000],  loss: 0.463736, mae: 11.367215, mean_q: -14.873710, mean_eps: 0.574955\n",
      "  567034/1200000: episode: 2304, duration: 4.026s, episode steps: 215, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.981 [0.000, 3.000],  loss: 0.336674, mae: 11.266849, mean_q: -14.774084, mean_eps: 0.574805\n",
      "  567236/1200000: episode: 2305, duration: 3.744s, episode steps: 202, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.376713, mae: 11.299285, mean_q: -14.812961, mean_eps: 0.574649\n",
      "  567453/1200000: episode: 2306, duration: 3.952s, episode steps: 217, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.336786, mae: 11.380072, mean_q: -14.916121, mean_eps: 0.574492\n",
      "  567747/1200000: episode: 2307, duration: 5.490s, episode steps: 294, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.401880, mae: 11.327439, mean_q: -14.858184, mean_eps: 0.574300\n",
      "  567992/1200000: episode: 2308, duration: 4.467s, episode steps: 245, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.415824, mae: 11.303331, mean_q: -14.800540, mean_eps: 0.574098\n",
      "  568306/1200000: episode: 2309, duration: 5.689s, episode steps: 314, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.325677, mae: 11.288457, mean_q: -14.817015, mean_eps: 0.573889\n",
      "  568551/1200000: episode: 2310, duration: 4.802s, episode steps: 245, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.366973, mae: 11.306310, mean_q: -14.813028, mean_eps: 0.573679\n",
      "  568736/1200000: episode: 2311, duration: 3.472s, episode steps: 185, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 0.427028, mae: 11.331108, mean_q: -14.819131, mean_eps: 0.573518\n",
      "  568947/1200000: episode: 2312, duration: 3.992s, episode steps: 211, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.337886, mae: 11.346014, mean_q: -14.871045, mean_eps: 0.573369\n",
      "  569254/1200000: episode: 2313, duration: 5.726s, episode steps: 307, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.340443, mae: 11.301812, mean_q: -14.811829, mean_eps: 0.573175\n",
      "  569472/1200000: episode: 2314, duration: 3.993s, episode steps: 218, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.913 [0.000, 3.000],  loss: 0.384642, mae: 11.287541, mean_q: -14.757290, mean_eps: 0.572978\n",
      "  569720/1200000: episode: 2315, duration: 4.583s, episode steps: 248, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.451681, mae: 11.279012, mean_q: -14.740176, mean_eps: 0.572803\n",
      "  569903/1200000: episode: 2316, duration: 3.601s, episode steps: 183, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.293999, mae: 11.178350, mean_q: -14.647421, mean_eps: 0.572642\n",
      "  570232/1200000: episode: 2317, duration: 6.297s, episode steps: 329, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.336071, mae: 11.109784, mean_q: -14.548733, mean_eps: 0.572450\n",
      "  570443/1200000: episode: 2318, duration: 3.894s, episode steps: 211, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.276733, mae: 11.078511, mean_q: -14.515676, mean_eps: 0.572247\n",
      "  570678/1200000: episode: 2319, duration: 4.569s, episode steps: 235, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.381506, mae: 11.037150, mean_q: -14.432781, mean_eps: 0.572080\n",
      "  570851/1200000: episode: 2320, duration: 3.249s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.376927, mae: 11.108700, mean_q: -14.557898, mean_eps: 0.571927\n",
      "  571022/1200000: episode: 2321, duration: 3.222s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 0.378567, mae: 11.158904, mean_q: -14.622318, mean_eps: 0.571798\n",
      "  571210/1200000: episode: 2322, duration: 3.485s, episode steps: 188, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: 0.251618, mae: 11.027108, mean_q: -14.461296, mean_eps: 0.571663\n",
      "  571380/1200000: episode: 2323, duration: 3.238s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.396718, mae: 11.090632, mean_q: -14.519479, mean_eps: 0.571529\n",
      "  571590/1200000: episode: 2324, duration: 4.076s, episode steps: 210, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.314770, mae: 11.076341, mean_q: -14.513872, mean_eps: 0.571387\n",
      "  571828/1200000: episode: 2325, duration: 4.644s, episode steps: 238, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.345355, mae: 11.058548, mean_q: -14.482369, mean_eps: 0.571219\n",
      "  572070/1200000: episode: 2326, duration: 4.518s, episode steps: 242, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.373339, mae: 11.107301, mean_q: -14.543136, mean_eps: 0.571039\n",
      "  572235/1200000: episode: 2327, duration: 3.102s, episode steps: 165, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: 0.279287, mae: 10.977828, mean_q: -14.393952, mean_eps: 0.570886\n",
      "  572406/1200000: episode: 2328, duration: 3.232s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.355285, mae: 11.117639, mean_q: -14.541488, mean_eps: 0.570760\n",
      "  572577/1200000: episode: 2329, duration: 3.253s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.393351, mae: 11.132460, mean_q: -14.601177, mean_eps: 0.570632\n",
      "  572808/1200000: episode: 2330, duration: 4.337s, episode steps: 231, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.251316, mae: 11.040615, mean_q: -14.485278, mean_eps: 0.570481\n",
      "  573284/1200000: episode: 2331, duration: 9.130s, episode steps: 476, steps per second:  52, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.384777, mae: 11.062860, mean_q: -14.467087, mean_eps: 0.570216\n",
      "  573494/1200000: episode: 2332, duration: 3.916s, episode steps: 210, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.397665, mae: 11.018251, mean_q: -14.397270, mean_eps: 0.569959\n",
      "  573653/1200000: episode: 2333, duration: 3.115s, episode steps: 159, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.306522, mae: 10.946087, mean_q: -14.320637, mean_eps: 0.569820\n",
      "  573821/1200000: episode: 2334, duration: 3.278s, episode steps: 168, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.393820, mae: 11.065862, mean_q: -14.499389, mean_eps: 0.569698\n",
      "  574062/1200000: episode: 2335, duration: 4.429s, episode steps: 241, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.333429, mae: 10.986507, mean_q: -14.396892, mean_eps: 0.569544\n",
      "  574229/1200000: episode: 2336, duration: 3.231s, episode steps: 167, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.313675, mae: 11.105837, mean_q: -14.542349, mean_eps: 0.569391\n",
      "  574527/1200000: episode: 2337, duration: 5.488s, episode steps: 298, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.228346, mae: 11.090174, mean_q: -14.568596, mean_eps: 0.569217\n",
      "  574708/1200000: episode: 2338, duration: 3.432s, episode steps: 181, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.657 [0.000, 3.000],  loss: 0.331810, mae: 11.084714, mean_q: -14.510929, mean_eps: 0.569037\n",
      "  574875/1200000: episode: 2339, duration: 3.341s, episode steps: 167, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.288455, mae: 11.060784, mean_q: -14.503856, mean_eps: 0.568907\n",
      "  575071/1200000: episode: 2340, duration: 3.665s, episode steps: 196, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.336499, mae: 11.058422, mean_q: -14.481379, mean_eps: 0.568771\n",
      "  575323/1200000: episode: 2341, duration: 4.686s, episode steps: 252, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.355299, mae: 11.145571, mean_q: -14.613361, mean_eps: 0.568603\n",
      "  575534/1200000: episode: 2342, duration: 4.054s, episode steps: 211, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.397672, mae: 11.042064, mean_q: -14.460958, mean_eps: 0.568429\n",
      "  575727/1200000: episode: 2343, duration: 3.649s, episode steps: 193, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.513 [0.000, 3.000],  loss: 0.346835, mae: 11.042998, mean_q: -14.483365, mean_eps: 0.568277\n",
      "  575958/1200000: episode: 2344, duration: 4.418s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.345527, mae: 11.014691, mean_q: -14.428604, mean_eps: 0.568118\n",
      "  576128/1200000: episode: 2345, duration: 3.208s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.384092, mae: 11.081078, mean_q: -14.526075, mean_eps: 0.567968\n",
      "  576351/1200000: episode: 2346, duration: 4.282s, episode steps: 223, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.343095, mae: 11.074061, mean_q: -14.518340, mean_eps: 0.567821\n",
      "  576558/1200000: episode: 2347, duration: 3.902s, episode steps: 207, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.947 [0.000, 3.000],  loss: 0.317631, mae: 11.102218, mean_q: -14.550846, mean_eps: 0.567659\n",
      "  576859/1200000: episode: 2348, duration: 5.621s, episode steps: 301, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.390119, mae: 11.051078, mean_q: -14.462255, mean_eps: 0.567469\n",
      "  577112/1200000: episode: 2349, duration: 4.866s, episode steps: 253, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.256084, mae: 11.073583, mean_q: -14.514461, mean_eps: 0.567261\n",
      "  577282/1200000: episode: 2350, duration: 3.192s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.327103, mae: 10.978929, mean_q: -14.387058, mean_eps: 0.567103\n",
      "  577493/1200000: episode: 2351, duration: 4.019s, episode steps: 211, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.309550, mae: 10.944568, mean_q: -14.335641, mean_eps: 0.566960\n",
      "  577678/1200000: episode: 2352, duration: 3.565s, episode steps: 185, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.269779, mae: 10.937459, mean_q: -14.320818, mean_eps: 0.566811\n",
      "  577915/1200000: episode: 2353, duration: 4.406s, episode steps: 237, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.357628, mae: 11.086915, mean_q: -14.486276, mean_eps: 0.566653\n",
      "  578103/1200000: episode: 2354, duration: 3.651s, episode steps: 188, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.745 [0.000, 3.000],  loss: 0.245462, mae: 11.010311, mean_q: -14.433268, mean_eps: 0.566494\n",
      "  578425/1200000: episode: 2355, duration: 5.906s, episode steps: 322, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.403904, mae: 11.077548, mean_q: -14.492208, mean_eps: 0.566302\n",
      "  578640/1200000: episode: 2356, duration: 4.146s, episode steps: 215, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.315464, mae: 11.120309, mean_q: -14.579106, mean_eps: 0.566101\n",
      "  578808/1200000: episode: 2357, duration: 3.255s, episode steps: 168, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.313612, mae: 10.954792, mean_q: -14.369660, mean_eps: 0.565957\n",
      "  579107/1200000: episode: 2358, duration: 5.722s, episode steps: 299, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.353362, mae: 10.969177, mean_q: -14.353218, mean_eps: 0.565782\n",
      "  579421/1200000: episode: 2359, duration: 6.017s, episode steps: 314, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.349678, mae: 11.040501, mean_q: -14.462260, mean_eps: 0.565552\n",
      "  579635/1200000: episode: 2360, duration: 4.120s, episode steps: 214, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.344037, mae: 11.004108, mean_q: -14.431519, mean_eps: 0.565354\n",
      "  579807/1200000: episode: 2361, duration: 3.234s, episode steps: 172, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.272172, mae: 11.008812, mean_q: -14.459457, mean_eps: 0.565210\n",
      "  580011/1200000: episode: 2362, duration: 3.973s, episode steps: 204, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.294565, mae: 10.992009, mean_q: -14.415910, mean_eps: 0.565069\n",
      "  580208/1200000: episode: 2363, duration: 3.783s, episode steps: 197, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.294533, mae: 10.810243, mean_q: -14.155901, mean_eps: 0.564918\n",
      "  580373/1200000: episode: 2364, duration: 3.137s, episode steps: 165, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.329394, mae: 10.765812, mean_q: -14.086862, mean_eps: 0.564782\n",
      "  580585/1200000: episode: 2365, duration: 4.048s, episode steps: 212, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.268294, mae: 10.811714, mean_q: -14.179284, mean_eps: 0.564641\n",
      "  580752/1200000: episode: 2366, duration: 3.169s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.357841, mae: 10.782478, mean_q: -14.124772, mean_eps: 0.564499\n",
      "  580919/1200000: episode: 2367, duration: 3.158s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.297937, mae: 10.754871, mean_q: -14.082042, mean_eps: 0.564374\n",
      "  581093/1200000: episode: 2368, duration: 3.303s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.793 [0.000, 3.000],  loss: 0.346193, mae: 10.746615, mean_q: -14.047367, mean_eps: 0.564246\n",
      "  581278/1200000: episode: 2369, duration: 3.630s, episode steps: 185, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.773 [0.000, 3.000],  loss: 0.397834, mae: 10.798243, mean_q: -14.107987, mean_eps: 0.564111\n",
      "  581466/1200000: episode: 2370, duration: 3.704s, episode steps: 188, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.883 [0.000, 3.000],  loss: 0.252610, mae: 10.749147, mean_q: -14.092343, mean_eps: 0.563971\n",
      "  581642/1200000: episode: 2371, duration: 3.247s, episode steps: 176, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.631 [0.000, 3.000],  loss: 0.332273, mae: 10.722817, mean_q: -14.036760, mean_eps: 0.563835\n",
      "  581887/1200000: episode: 2372, duration: 4.656s, episode steps: 245, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.878 [0.000, 3.000],  loss: 0.308919, mae: 10.766937, mean_q: -14.102208, mean_eps: 0.563677\n",
      "  582057/1200000: episode: 2373, duration: 3.323s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.319019, mae: 10.790861, mean_q: -14.135958, mean_eps: 0.563521\n",
      "  582336/1200000: episode: 2374, duration: 5.429s, episode steps: 279, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.378248, mae: 10.765049, mean_q: -14.088814, mean_eps: 0.563353\n",
      "  582558/1200000: episode: 2375, duration: 4.454s, episode steps: 222, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 0.347115, mae: 10.798526, mean_q: -14.147719, mean_eps: 0.563165\n",
      "  582906/1200000: episode: 2376, duration: 6.522s, episode steps: 348, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.365955, mae: 10.719098, mean_q: -14.022592, mean_eps: 0.562951\n",
      "  583087/1200000: episode: 2377, duration: 3.554s, episode steps: 181, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.785 [0.000, 3.000],  loss: 0.238737, mae: 10.718718, mean_q: -14.063568, mean_eps: 0.562753\n",
      "  583257/1200000: episode: 2378, duration: 3.646s, episode steps: 170, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.794 [0.000, 3.000],  loss: 0.451498, mae: 10.769974, mean_q: -14.093248, mean_eps: 0.562621\n",
      "  583479/1200000: episode: 2379, duration: 4.276s, episode steps: 222, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.354027, mae: 10.710760, mean_q: -14.020103, mean_eps: 0.562474\n",
      "  583730/1200000: episode: 2380, duration: 4.870s, episode steps: 251, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.373486, mae: 10.715858, mean_q: -14.002075, mean_eps: 0.562297\n",
      "  583908/1200000: episode: 2381, duration: 3.435s, episode steps: 178, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.702 [0.000, 3.000],  loss: 0.397820, mae: 10.710142, mean_q: -14.000892, mean_eps: 0.562136\n",
      "  584069/1200000: episode: 2382, duration: 3.175s, episode steps: 161, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.348717, mae: 10.783226, mean_q: -14.123851, mean_eps: 0.562009\n",
      "  584231/1200000: episode: 2383, duration: 3.173s, episode steps: 162, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.551028, mae: 10.736383, mean_q: -13.973661, mean_eps: 0.561888\n",
      "  584416/1200000: episode: 2384, duration: 3.666s, episode steps: 185, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.264586, mae: 10.768217, mean_q: -14.120007, mean_eps: 0.561758\n",
      "  584626/1200000: episode: 2385, duration: 4.021s, episode steps: 210, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.358535, mae: 10.760096, mean_q: -14.093449, mean_eps: 0.561610\n",
      "  584800/1200000: episode: 2386, duration: 3.294s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.307508, mae: 10.783584, mean_q: -14.122451, mean_eps: 0.561466\n",
      "  585024/1200000: episode: 2387, duration: 4.309s, episode steps: 224, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.448422, mae: 10.817298, mean_q: -14.157278, mean_eps: 0.561316\n",
      "  585248/1200000: episode: 2388, duration: 4.328s, episode steps: 224, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.232 [0.000, 3.000],  loss: 0.333519, mae: 10.815450, mean_q: -14.162432, mean_eps: 0.561148\n",
      "  585434/1200000: episode: 2389, duration: 3.651s, episode steps: 186, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.742 [0.000, 3.000],  loss: 0.454685, mae: 10.843106, mean_q: -14.184804, mean_eps: 0.560995\n",
      "  585652/1200000: episode: 2390, duration: 4.352s, episode steps: 218, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.370521, mae: 10.777371, mean_q: -14.112335, mean_eps: 0.560843\n",
      "  585965/1200000: episode: 2391, duration: 5.816s, episode steps: 313, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.355364, mae: 10.753552, mean_q: -14.087891, mean_eps: 0.560644\n",
      "  586140/1200000: episode: 2392, duration: 3.336s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.319148, mae: 10.664160, mean_q: -13.999907, mean_eps: 0.560461\n",
      "  586353/1200000: episode: 2393, duration: 4.182s, episode steps: 213, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.367730, mae: 10.817781, mean_q: -14.176946, mean_eps: 0.560316\n",
      "  586719/1200000: episode: 2394, duration: 6.880s, episode steps: 366, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.348251, mae: 10.747269, mean_q: -14.061177, mean_eps: 0.560098\n",
      "  586928/1200000: episode: 2395, duration: 4.048s, episode steps: 209, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.329148, mae: 10.799296, mean_q: -14.172281, mean_eps: 0.559883\n",
      "  587238/1200000: episode: 2396, duration: 6.272s, episode steps: 310, steps per second:  49, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.364211, mae: 10.718065, mean_q: -14.028970, mean_eps: 0.559688\n",
      "  587449/1200000: episode: 2397, duration: 4.641s, episode steps: 211, steps per second:  45, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.378676, mae: 10.843642, mean_q: -14.189295, mean_eps: 0.559493\n",
      "  587615/1200000: episode: 2398, duration: 3.012s, episode steps: 166, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.320092, mae: 10.723587, mean_q: -14.038800, mean_eps: 0.559351\n",
      "  587814/1200000: episode: 2399, duration: 4.161s, episode steps: 199, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.719 [0.000, 3.000],  loss: 0.336600, mae: 10.724015, mean_q: -14.056892, mean_eps: 0.559214\n",
      "  588055/1200000: episode: 2400, duration: 4.861s, episode steps: 241, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.348200, mae: 10.753070, mean_q: -14.094594, mean_eps: 0.559049\n",
      "  588409/1200000: episode: 2401, duration: 6.653s, episode steps: 354, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.291571, mae: 10.678617, mean_q: -13.992298, mean_eps: 0.558826\n",
      "  588579/1200000: episode: 2402, duration: 3.637s, episode steps: 170, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.321990, mae: 10.703297, mean_q: -14.016639, mean_eps: 0.558630\n",
      "  588808/1200000: episode: 2403, duration: 4.768s, episode steps: 229, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.378152, mae: 10.763982, mean_q: -14.085029, mean_eps: 0.558480\n",
      "  589155/1200000: episode: 2404, duration: 6.580s, episode steps: 347, steps per second:  53, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.343691, mae: 10.800541, mean_q: -14.150058, mean_eps: 0.558264\n",
      "  589337/1200000: episode: 2405, duration: 3.529s, episode steps: 182, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.863 [0.000, 3.000],  loss: 0.336887, mae: 10.693866, mean_q: -14.007987, mean_eps: 0.558066\n",
      "  589576/1200000: episode: 2406, duration: 4.367s, episode steps: 239, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.292689, mae: 10.769697, mean_q: -14.110565, mean_eps: 0.557908\n",
      "  589815/1200000: episode: 2407, duration: 4.322s, episode steps: 239, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.320540, mae: 10.735675, mean_q: -14.060063, mean_eps: 0.557729\n",
      "  589992/1200000: episode: 2408, duration: 3.316s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.227572, mae: 10.868607, mean_q: -14.257774, mean_eps: 0.557573\n",
      "  590207/1200000: episode: 2409, duration: 3.987s, episode steps: 215, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.412108, mae: 10.444621, mean_q: -13.638725, mean_eps: 0.557426\n",
      "  590384/1200000: episode: 2410, duration: 3.318s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.329718, mae: 10.404788, mean_q: -13.609710, mean_eps: 0.557279\n",
      "  590560/1200000: episode: 2411, duration: 3.169s, episode steps: 176, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.270476, mae: 10.655571, mean_q: -13.967218, mean_eps: 0.557146\n",
      "  590804/1200000: episode: 2412, duration: 4.504s, episode steps: 244, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.406250, mae: 10.472993, mean_q: -13.658494, mean_eps: 0.556989\n",
      "  590982/1200000: episode: 2413, duration: 3.319s, episode steps: 178, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.502448, mae: 10.492532, mean_q: -13.704757, mean_eps: 0.556831\n",
      "  591200/1200000: episode: 2414, duration: 3.967s, episode steps: 218, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.383401, mae: 10.467506, mean_q: -13.686379, mean_eps: 0.556682\n",
      "  591428/1200000: episode: 2415, duration: 4.470s, episode steps: 228, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.312356, mae: 10.444577, mean_q: -13.675342, mean_eps: 0.556515\n",
      "  591602/1200000: episode: 2416, duration: 3.182s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.420920, mae: 10.471461, mean_q: -13.663906, mean_eps: 0.556364\n",
      "  591774/1200000: episode: 2417, duration: 3.204s, episode steps: 172, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.767 [0.000, 3.000],  loss: 0.326493, mae: 10.503196, mean_q: -13.726135, mean_eps: 0.556234\n",
      "  592000/1200000: episode: 2418, duration: 4.369s, episode steps: 226, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.433302, mae: 10.517146, mean_q: -13.748986, mean_eps: 0.556085\n",
      "  592320/1200000: episode: 2419, duration: 5.755s, episode steps: 320, steps per second:  56, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.318374, mae: 10.490316, mean_q: -13.734518, mean_eps: 0.555880\n",
      "  592492/1200000: episode: 2420, duration: 3.215s, episode steps: 172, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.779 [0.000, 3.000],  loss: 0.349642, mae: 10.490412, mean_q: -13.700993, mean_eps: 0.555696\n",
      "  592701/1200000: episode: 2421, duration: 3.959s, episode steps: 209, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.336847, mae: 10.533081, mean_q: -13.768794, mean_eps: 0.555553\n",
      "  592927/1200000: episode: 2422, duration: 4.145s, episode steps: 226, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.377542, mae: 10.450572, mean_q: -13.648290, mean_eps: 0.555390\n",
      "  593097/1200000: episode: 2423, duration: 3.202s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.243590, mae: 10.457373, mean_q: -13.684505, mean_eps: 0.555241\n",
      "  593361/1200000: episode: 2424, duration: 4.821s, episode steps: 264, steps per second:  55, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.397337, mae: 10.485602, mean_q: -13.698289, mean_eps: 0.555079\n",
      "  593652/1200000: episode: 2425, duration: 5.286s, episode steps: 291, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.345789, mae: 10.514164, mean_q: -13.760127, mean_eps: 0.554870\n",
      "  593822/1200000: episode: 2426, duration: 3.104s, episode steps: 170, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.753 [0.000, 3.000],  loss: 0.380741, mae: 10.439600, mean_q: -13.641254, mean_eps: 0.554698\n",
      "  594000/1200000: episode: 2427, duration: 3.301s, episode steps: 178, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.356272, mae: 10.423447, mean_q: -13.622201, mean_eps: 0.554567\n",
      "  594199/1200000: episode: 2428, duration: 3.670s, episode steps: 199, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.343915, mae: 10.488344, mean_q: -13.710692, mean_eps: 0.554426\n",
      "  594396/1200000: episode: 2429, duration: 3.651s, episode steps: 197, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.670 [0.000, 3.000],  loss: 0.301981, mae: 10.453917, mean_q: -13.662895, mean_eps: 0.554277\n",
      "  594581/1200000: episode: 2430, duration: 3.417s, episode steps: 185, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.487718, mae: 10.528031, mean_q: -13.729173, mean_eps: 0.554134\n",
      "  594754/1200000: episode: 2431, duration: 3.510s, episode steps: 173, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.237 [0.000, 3.000],  loss: 0.326469, mae: 10.471853, mean_q: -13.716065, mean_eps: 0.554000\n",
      "  594942/1200000: episode: 2432, duration: 3.516s, episode steps: 188, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.984 [0.000, 3.000],  loss: 0.369653, mae: 10.509216, mean_q: -13.729604, mean_eps: 0.553864\n",
      "  595179/1200000: episode: 2433, duration: 4.438s, episode steps: 237, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.391676, mae: 10.505404, mean_q: -13.739331, mean_eps: 0.553705\n",
      "  595461/1200000: episode: 2434, duration: 5.288s, episode steps: 282, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.318050, mae: 10.518419, mean_q: -13.764178, mean_eps: 0.553510\n",
      "  595677/1200000: episode: 2435, duration: 4.013s, episode steps: 216, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.350798, mae: 10.480303, mean_q: -13.693724, mean_eps: 0.553324\n",
      "  595874/1200000: episode: 2436, duration: 3.657s, episode steps: 197, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.379644, mae: 10.519854, mean_q: -13.737201, mean_eps: 0.553169\n",
      "  596090/1200000: episode: 2437, duration: 4.085s, episode steps: 216, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.353967, mae: 10.423039, mean_q: -13.634086, mean_eps: 0.553014\n",
      "  596342/1200000: episode: 2438, duration: 4.848s, episode steps: 252, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.367565, mae: 10.482989, mean_q: -13.701046, mean_eps: 0.552838\n",
      "  596519/1200000: episode: 2439, duration: 3.273s, episode steps: 177, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.305 [0.000, 3.000],  loss: 0.390999, mae: 10.525479, mean_q: -13.778000, mean_eps: 0.552678\n",
      "  596716/1200000: episode: 2440, duration: 3.699s, episode steps: 197, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.348782, mae: 10.599019, mean_q: -13.869925, mean_eps: 0.552537\n",
      "  596904/1200000: episode: 2441, duration: 3.609s, episode steps: 188, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.272141, mae: 10.487734, mean_q: -13.730829, mean_eps: 0.552393\n",
      "  597109/1200000: episode: 2442, duration: 3.921s, episode steps: 205, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.399290, mae: 10.546173, mean_q: -13.775337, mean_eps: 0.552246\n",
      "  597328/1200000: episode: 2443, duration: 4.129s, episode steps: 219, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.389781, mae: 10.564266, mean_q: -13.807827, mean_eps: 0.552087\n",
      "  597496/1200000: episode: 2444, duration: 3.271s, episode steps: 168, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.397189, mae: 10.484600, mean_q: -13.725365, mean_eps: 0.551941\n",
      "  597694/1200000: episode: 2445, duration: 3.580s, episode steps: 198, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.304103, mae: 10.385827, mean_q: -13.572946, mean_eps: 0.551804\n",
      "  597910/1200000: episode: 2446, duration: 3.987s, episode steps: 216, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.319979, mae: 10.478285, mean_q: -13.703826, mean_eps: 0.551649\n",
      "  598374/1200000: episode: 2447, duration: 8.409s, episode steps: 464, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.360792, mae: 10.543823, mean_q: -13.780970, mean_eps: 0.551394\n",
      "  598678/1200000: episode: 2448, duration: 6.202s, episode steps: 304, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.281609, mae: 10.512386, mean_q: -13.791660, mean_eps: 0.551106\n",
      "  598862/1200000: episode: 2449, duration: 3.686s, episode steps: 184, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.333884, mae: 10.469923, mean_q: -13.720193, mean_eps: 0.550923\n",
      "  599030/1200000: episode: 2450, duration: 3.406s, episode steps: 168, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.393112, mae: 10.366749, mean_q: -13.558688, mean_eps: 0.550791\n",
      "  599272/1200000: episode: 2451, duration: 5.053s, episode steps: 242, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.322591, mae: 10.569887, mean_q: -13.832630, mean_eps: 0.550637\n",
      "  599466/1200000: episode: 2452, duration: 4.022s, episode steps: 194, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.404959, mae: 10.545062, mean_q: -13.776335, mean_eps: 0.550474\n",
      "  599786/1200000: episode: 2453, duration: 6.502s, episode steps: 320, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.338305, mae: 10.485046, mean_q: -13.707149, mean_eps: 0.550281\n",
      "  599982/1200000: episode: 2454, duration: 4.135s, episode steps: 196, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: 0.392051, mae: 10.577981, mean_q: -13.802251, mean_eps: 0.550087\n",
      "  600195/1200000: episode: 2455, duration: 4.368s, episode steps: 213, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.309722, mae: 10.284438, mean_q: -13.450261, mean_eps: 0.549934\n",
      "  600374/1200000: episode: 2456, duration: 3.684s, episode steps: 179, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.408503, mae: 10.318419, mean_q: -13.484292, mean_eps: 0.549787\n",
      "  600584/1200000: episode: 2457, duration: 4.290s, episode steps: 210, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.336495, mae: 10.231622, mean_q: -13.381374, mean_eps: 0.549641\n",
      "  600760/1200000: episode: 2458, duration: 3.583s, episode steps: 176, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.767 [0.000, 3.000],  loss: 0.280095, mae: 10.210308, mean_q: -13.356194, mean_eps: 0.549496\n",
      "  600936/1200000: episode: 2459, duration: 3.653s, episode steps: 176, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.670 [0.000, 3.000],  loss: 0.284849, mae: 10.199654, mean_q: -13.360024, mean_eps: 0.549364\n",
      "  601171/1200000: episode: 2460, duration: 4.820s, episode steps: 235, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.362 [0.000, 3.000],  loss: 0.376113, mae: 10.111703, mean_q: -13.206466, mean_eps: 0.549210\n",
      "  601380/1200000: episode: 2461, duration: 4.404s, episode steps: 209, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.294948, mae: 10.213318, mean_q: -13.357893, mean_eps: 0.549044\n",
      "  601634/1200000: episode: 2462, duration: 5.212s, episode steps: 254, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.358534, mae: 10.220066, mean_q: -13.341903, mean_eps: 0.548870\n",
      "  601835/1200000: episode: 2463, duration: 4.125s, episode steps: 201, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.697 [0.000, 3.000],  loss: 0.320014, mae: 10.227986, mean_q: -13.373088, mean_eps: 0.548700\n",
      "  602254/1200000: episode: 2464, duration: 8.823s, episode steps: 419, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.311344, mae: 10.200624, mean_q: -13.337356, mean_eps: 0.548467\n",
      "  602420/1200000: episode: 2465, duration: 3.504s, episode steps: 166, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.463383, mae: 10.196103, mean_q: -13.299580, mean_eps: 0.548248\n",
      "  602593/1200000: episode: 2466, duration: 3.562s, episode steps: 173, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.734 [0.000, 3.000],  loss: 0.384065, mae: 10.128980, mean_q: -13.247809, mean_eps: 0.548121\n",
      "  602771/1200000: episode: 2467, duration: 3.790s, episode steps: 178, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.242 [0.000, 3.000],  loss: 0.381748, mae: 10.237588, mean_q: -13.381817, mean_eps: 0.547989\n",
      "  603112/1200000: episode: 2468, duration: 7.087s, episode steps: 341, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.324806, mae: 10.221540, mean_q: -13.354942, mean_eps: 0.547794\n",
      "  603368/1200000: episode: 2469, duration: 5.137s, episode steps: 256, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.286855, mae: 10.121224, mean_q: -13.233520, mean_eps: 0.547570\n",
      "  603537/1200000: episode: 2470, duration: 3.851s, episode steps: 169, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.521 [0.000, 3.000],  loss: 0.389809, mae: 10.259108, mean_q: -13.425388, mean_eps: 0.547411\n",
      "  603764/1200000: episode: 2471, duration: 4.776s, episode steps: 227, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.443557, mae: 10.204356, mean_q: -13.304409, mean_eps: 0.547262\n",
      "  604000/1200000: episode: 2472, duration: 4.899s, episode steps: 236, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.371013, mae: 10.173035, mean_q: -13.300971, mean_eps: 0.547089\n",
      "  604190/1200000: episode: 2473, duration: 3.937s, episode steps: 190, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.737 [0.000, 3.000],  loss: 0.280312, mae: 10.174917, mean_q: -13.315993, mean_eps: 0.546929\n",
      "  604371/1200000: episode: 2474, duration: 3.869s, episode steps: 181, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.354238, mae: 10.231965, mean_q: -13.393781, mean_eps: 0.546790\n",
      "  604602/1200000: episode: 2475, duration: 4.783s, episode steps: 231, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.307 [0.000, 3.000],  loss: 0.372085, mae: 10.203385, mean_q: -13.374440, mean_eps: 0.546636\n",
      "  604769/1200000: episode: 2476, duration: 3.522s, episode steps: 167, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.874 [0.000, 3.000],  loss: 0.363189, mae: 10.115836, mean_q: -13.181928, mean_eps: 0.546486\n",
      "  604984/1200000: episode: 2477, duration: 4.506s, episode steps: 215, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.383911, mae: 10.204025, mean_q: -13.333791, mean_eps: 0.546343\n",
      "  605159/1200000: episode: 2478, duration: 3.677s, episode steps: 175, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: 0.409885, mae: 10.226799, mean_q: -13.344365, mean_eps: 0.546197\n",
      "  605341/1200000: episode: 2479, duration: 3.739s, episode steps: 182, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.303281, mae: 10.167045, mean_q: -13.310935, mean_eps: 0.546063\n",
      "  605585/1200000: episode: 2480, duration: 5.571s, episode steps: 244, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.346091, mae: 10.268559, mean_q: -13.422283, mean_eps: 0.545903\n",
      "  605760/1200000: episode: 2481, duration: 3.995s, episode steps: 175, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.323335, mae: 10.243364, mean_q: -13.401314, mean_eps: 0.545746\n",
      "  605990/1200000: episode: 2482, duration: 4.989s, episode steps: 230, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.787 [0.000, 3.000],  loss: 0.257471, mae: 10.188560, mean_q: -13.350906, mean_eps: 0.545594\n",
      "  606171/1200000: episode: 2483, duration: 3.938s, episode steps: 181, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.421576, mae: 10.216737, mean_q: -13.348233, mean_eps: 0.545440\n",
      "  606390/1200000: episode: 2484, duration: 4.739s, episode steps: 219, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.416906, mae: 10.184454, mean_q: -13.298366, mean_eps: 0.545290\n",
      "  606603/1200000: episode: 2485, duration: 4.772s, episode steps: 213, steps per second:  45, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.298550, mae: 10.230176, mean_q: -13.365398, mean_eps: 0.545128\n",
      "  606836/1200000: episode: 2486, duration: 5.064s, episode steps: 233, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.293932, mae: 10.237929, mean_q: -13.399236, mean_eps: 0.544961\n",
      "  607119/1200000: episode: 2487, duration: 5.892s, episode steps: 283, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.425112, mae: 10.273881, mean_q: -13.426237, mean_eps: 0.544767\n",
      "  607317/1200000: episode: 2488, duration: 4.367s, episode steps: 198, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.717 [0.000, 3.000],  loss: 0.333741, mae: 10.285414, mean_q: -13.461062, mean_eps: 0.544587\n",
      "  607483/1200000: episode: 2489, duration: 3.528s, episode steps: 166, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.245413, mae: 10.087963, mean_q: -13.218841, mean_eps: 0.544450\n",
      "  607687/1200000: episode: 2490, duration: 4.211s, episode steps: 204, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.325376, mae: 10.262455, mean_q: -13.416233, mean_eps: 0.544312\n",
      "  607925/1200000: episode: 2491, duration: 4.950s, episode steps: 238, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.301665, mae: 10.196382, mean_q: -13.362138, mean_eps: 0.544146\n",
      "  608358/1200000: episode: 2492, duration: 8.911s, episode steps: 433, steps per second:  49, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.383993, mae: 10.144094, mean_q: -13.256932, mean_eps: 0.543894\n",
      "  608526/1200000: episode: 2493, duration: 3.619s, episode steps: 168, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.685 [0.000, 3.000],  loss: 0.380721, mae: 10.288058, mean_q: -13.466271, mean_eps: 0.543669\n",
      "  608701/1200000: episode: 2494, duration: 3.673s, episode steps: 175, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.823 [0.000, 3.000],  loss: 0.307888, mae: 10.135870, mean_q: -13.268469, mean_eps: 0.543540\n",
      "  608877/1200000: episode: 2495, duration: 3.695s, episode steps: 176, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.350677, mae: 10.281223, mean_q: -13.451696, mean_eps: 0.543409\n",
      "  609054/1200000: episode: 2496, duration: 3.620s, episode steps: 177, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.329717, mae: 10.350283, mean_q: -13.540524, mean_eps: 0.543276\n",
      "  609231/1200000: episode: 2497, duration: 3.686s, episode steps: 177, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.337141, mae: 10.194799, mean_q: -13.356696, mean_eps: 0.543144\n",
      "  609416/1200000: episode: 2498, duration: 3.880s, episode steps: 185, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 0.371714, mae: 10.235824, mean_q: -13.396050, mean_eps: 0.543008\n",
      "  609592/1200000: episode: 2499, duration: 3.777s, episode steps: 176, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.756 [0.000, 3.000],  loss: 0.342544, mae: 10.130767, mean_q: -13.264036, mean_eps: 0.542872\n",
      "  609857/1200000: episode: 2500, duration: 5.330s, episode steps: 265, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: 0.360205, mae: 10.183286, mean_q: -13.322710, mean_eps: 0.542707\n",
      "  610023/1200000: episode: 2501, duration: 3.534s, episode steps: 166, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.693 [0.000, 3.000],  loss: 0.325876, mae: 10.123802, mean_q: -13.248596, mean_eps: 0.542545\n",
      "  610194/1200000: episode: 2502, duration: 3.639s, episode steps: 171, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.637 [0.000, 3.000],  loss: 0.363897, mae: 10.073634, mean_q: -13.172701, mean_eps: 0.542419\n",
      "  610406/1200000: episode: 2503, duration: 4.394s, episode steps: 212, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.377065, mae: 10.041386, mean_q: -13.113492, mean_eps: 0.542275\n",
      "  610574/1200000: episode: 2504, duration: 3.517s, episode steps: 168, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.845 [0.000, 3.000],  loss: 0.301947, mae: 9.909686, mean_q: -12.972191, mean_eps: 0.542133\n",
      "  610762/1200000: episode: 2505, duration: 4.010s, episode steps: 188, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.660 [0.000, 3.000],  loss: 0.373765, mae: 10.132892, mean_q: -13.255684, mean_eps: 0.541999\n",
      "  610969/1200000: episode: 2506, duration: 4.329s, episode steps: 207, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.283545, mae: 10.017646, mean_q: -13.130058, mean_eps: 0.541851\n",
      "  611161/1200000: episode: 2507, duration: 3.990s, episode steps: 192, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.740 [0.000, 3.000],  loss: 0.280259, mae: 10.027531, mean_q: -13.147640, mean_eps: 0.541702\n",
      "  611329/1200000: episode: 2508, duration: 3.600s, episode steps: 168, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.756 [0.000, 3.000],  loss: 0.365536, mae: 9.895447, mean_q: -12.936471, mean_eps: 0.541567\n",
      "  611513/1200000: episode: 2509, duration: 3.947s, episode steps: 184, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.251272, mae: 9.984682, mean_q: -13.075157, mean_eps: 0.541435\n",
      "  611734/1200000: episode: 2510, duration: 4.603s, episode steps: 221, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.309405, mae: 10.038593, mean_q: -13.143012, mean_eps: 0.541283\n",
      "  611920/1200000: episode: 2511, duration: 3.958s, episode steps: 186, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.710 [0.000, 3.000],  loss: 0.325227, mae: 10.045047, mean_q: -13.152847, mean_eps: 0.541130\n",
      "  612156/1200000: episode: 2512, duration: 4.841s, episode steps: 236, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.397828, mae: 9.970037, mean_q: -13.035367, mean_eps: 0.540972\n",
      "  612329/1200000: episode: 2513, duration: 3.690s, episode steps: 173, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.734 [0.000, 3.000],  loss: 0.341557, mae: 10.084455, mean_q: -13.196204, mean_eps: 0.540818\n",
      "  612500/1200000: episode: 2514, duration: 3.472s, episode steps: 171, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.287 [0.000, 3.000],  loss: 0.261727, mae: 10.026303, mean_q: -13.159226, mean_eps: 0.540690\n",
      "  612760/1200000: episode: 2515, duration: 5.366s, episode steps: 260, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.307205, mae: 9.979897, mean_q: -13.065912, mean_eps: 0.540528\n",
      "  612963/1200000: episode: 2516, duration: 4.372s, episode steps: 203, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.289506, mae: 9.992415, mean_q: -13.074338, mean_eps: 0.540354\n",
      "  613189/1200000: episode: 2517, duration: 4.675s, episode steps: 226, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.726 [0.000, 3.000],  loss: 0.325231, mae: 9.992112, mean_q: -13.090064, mean_eps: 0.540193\n",
      "  613482/1200000: episode: 2518, duration: 6.104s, episode steps: 293, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.368536, mae: 10.029466, mean_q: -13.122073, mean_eps: 0.539999\n",
      "  613735/1200000: episode: 2519, duration: 5.121s, episode steps: 253, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.379002, mae: 9.882305, mean_q: -12.912022, mean_eps: 0.539794\n",
      "  613902/1200000: episode: 2520, duration: 3.543s, episode steps: 167, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.383 [0.000, 3.000],  loss: 0.396961, mae: 10.148307, mean_q: -13.272954, mean_eps: 0.539636\n",
      "  614084/1200000: episode: 2521, duration: 3.740s, episode steps: 182, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.780 [0.000, 3.000],  loss: 0.267034, mae: 10.040294, mean_q: -13.132842, mean_eps: 0.539506\n",
      "  614358/1200000: episode: 2522, duration: 6.025s, episode steps: 274, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.285316, mae: 9.927313, mean_q: -12.998104, mean_eps: 0.539335\n",
      "  614542/1200000: episode: 2523, duration: 3.959s, episode steps: 184, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.365317, mae: 10.073526, mean_q: -13.158866, mean_eps: 0.539163\n",
      "  614716/1200000: episode: 2524, duration: 3.624s, episode steps: 174, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.345393, mae: 10.056124, mean_q: -13.164110, mean_eps: 0.539029\n",
      "  614902/1200000: episode: 2525, duration: 3.884s, episode steps: 186, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.288905, mae: 10.018989, mean_q: -13.112987, mean_eps: 0.538894\n",
      "  615082/1200000: episode: 2526, duration: 3.752s, episode steps: 180, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.314486, mae: 10.186041, mean_q: -13.342455, mean_eps: 0.538756\n",
      "  615295/1200000: episode: 2527, duration: 4.589s, episode steps: 213, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.307908, mae: 9.996384, mean_q: -13.074718, mean_eps: 0.538609\n",
      "  615530/1200000: episode: 2528, duration: 4.836s, episode steps: 235, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.287760, mae: 10.012625, mean_q: -13.095266, mean_eps: 0.538441\n",
      "  615760/1200000: episode: 2529, duration: 4.832s, episode steps: 230, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.330814, mae: 10.005798, mean_q: -13.115335, mean_eps: 0.538267\n",
      "  616035/1200000: episode: 2530, duration: 5.920s, episode steps: 275, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.344495, mae: 9.991014, mean_q: -13.078549, mean_eps: 0.538077\n",
      "  616203/1200000: episode: 2531, duration: 3.593s, episode steps: 168, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.297817, mae: 10.024886, mean_q: -13.093760, mean_eps: 0.537911\n",
      "  616455/1200000: episode: 2532, duration: 5.230s, episode steps: 252, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.271398, mae: 10.035732, mean_q: -13.140829, mean_eps: 0.537754\n",
      "  616665/1200000: episode: 2533, duration: 4.472s, episode steps: 210, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.399770, mae: 9.978678, mean_q: -13.036067, mean_eps: 0.537580\n",
      "  616870/1200000: episode: 2534, duration: 4.382s, episode steps: 205, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.413302, mae: 10.001443, mean_q: -13.072802, mean_eps: 0.537425\n",
      "  617208/1200000: episode: 2535, duration: 7.177s, episode steps: 338, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.340241, mae: 10.008484, mean_q: -13.084231, mean_eps: 0.537221\n",
      "  617490/1200000: episode: 2536, duration: 5.839s, episode steps: 282, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.316880, mae: 9.921137, mean_q: -12.979292, mean_eps: 0.536989\n",
      "  617666/1200000: episode: 2537, duration: 3.813s, episode steps: 176, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.291552, mae: 9.940012, mean_q: -13.013013, mean_eps: 0.536817\n",
      "  617900/1200000: episode: 2538, duration: 4.974s, episode steps: 234, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.392600, mae: 10.030557, mean_q: -13.093524, mean_eps: 0.536663\n",
      "  618152/1200000: episode: 2539, duration: 5.375s, episode steps: 252, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.290100, mae: 10.079323, mean_q: -13.168616, mean_eps: 0.536481\n",
      "  618379/1200000: episode: 2540, duration: 4.802s, episode steps: 227, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.302328, mae: 10.053279, mean_q: -13.173626, mean_eps: 0.536301\n",
      "  618553/1200000: episode: 2541, duration: 3.922s, episode steps: 174, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.263525, mae: 9.957345, mean_q: -13.043759, mean_eps: 0.536151\n",
      "  618767/1200000: episode: 2542, duration: 4.595s, episode steps: 214, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.262635, mae: 9.999342, mean_q: -13.114641, mean_eps: 0.536005\n",
      "  618975/1200000: episode: 2543, duration: 4.556s, episode steps: 208, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.308919, mae: 10.084939, mean_q: -13.212777, mean_eps: 0.535847\n",
      "  619179/1200000: episode: 2544, duration: 4.207s, episode steps: 204, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.274600, mae: 9.950100, mean_q: -13.028913, mean_eps: 0.535693\n",
      "  619392/1200000: episode: 2545, duration: 4.425s, episode steps: 213, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.328925, mae: 9.857327, mean_q: -12.898940, mean_eps: 0.535536\n",
      "  619577/1200000: episode: 2546, duration: 3.837s, episode steps: 185, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.768 [0.000, 3.000],  loss: 0.376237, mae: 10.018000, mean_q: -13.108201, mean_eps: 0.535387\n",
      "  619862/1200000: episode: 2547, duration: 5.979s, episode steps: 285, steps per second:  48, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: 0.303002, mae: 10.013672, mean_q: -13.095488, mean_eps: 0.535211\n",
      "  620040/1200000: episode: 2548, duration: 3.734s, episode steps: 178, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.229006, mae: 10.064228, mean_q: -13.192970, mean_eps: 0.535037\n",
      "  620318/1200000: episode: 2549, duration: 5.800s, episode steps: 278, steps per second:  48, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.331550, mae: 10.017030, mean_q: -13.144636, mean_eps: 0.534866\n",
      "  620523/1200000: episode: 2550, duration: 4.351s, episode steps: 205, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.281509, mae: 9.944408, mean_q: -13.041200, mean_eps: 0.534685\n",
      "  620695/1200000: episode: 2551, duration: 4.036s, episode steps: 172, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.409838, mae: 10.018180, mean_q: -13.119855, mean_eps: 0.534544\n",
      "  620858/1200000: episode: 2552, duration: 3.472s, episode steps: 163, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.282 [0.000, 3.000],  loss: 0.284619, mae: 10.040170, mean_q: -13.147464, mean_eps: 0.534418\n",
      "  621090/1200000: episode: 2553, duration: 4.940s, episode steps: 232, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.237 [0.000, 3.000],  loss: 0.345919, mae: 10.100958, mean_q: -13.234730, mean_eps: 0.534270\n",
      "  621342/1200000: episode: 2554, duration: 5.586s, episode steps: 252, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.232935, mae: 9.939468, mean_q: -13.031060, mean_eps: 0.534088\n",
      "  621571/1200000: episode: 2555, duration: 4.831s, episode steps: 229, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.350075, mae: 9.943930, mean_q: -13.014721, mean_eps: 0.533908\n",
      "  621749/1200000: episode: 2556, duration: 3.851s, episode steps: 178, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.346467, mae: 10.049518, mean_q: -13.159745, mean_eps: 0.533755\n",
      "  621980/1200000: episode: 2557, duration: 4.922s, episode steps: 231, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.264 [0.000, 3.000],  loss: 0.264357, mae: 9.980665, mean_q: -13.079690, mean_eps: 0.533602\n",
      "  622163/1200000: episode: 2558, duration: 4.283s, episode steps: 183, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.825 [0.000, 3.000],  loss: 0.245769, mae: 9.978003, mean_q: -13.095063, mean_eps: 0.533447\n",
      "  622327/1200000: episode: 2559, duration: 3.467s, episode steps: 164, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.671 [0.000, 3.000],  loss: 0.333582, mae: 10.007158, mean_q: -13.093419, mean_eps: 0.533317\n",
      "  622518/1200000: episode: 2560, duration: 4.062s, episode steps: 191, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.686 [0.000, 3.000],  loss: 0.247280, mae: 9.956951, mean_q: -13.066931, mean_eps: 0.533184\n",
      "  622764/1200000: episode: 2561, duration: 5.262s, episode steps: 246, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.329545, mae: 10.022571, mean_q: -13.128935, mean_eps: 0.533020\n",
      "  622936/1200000: episode: 2562, duration: 3.668s, episode steps: 172, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.270648, mae: 9.988653, mean_q: -13.103570, mean_eps: 0.532863\n",
      "  623154/1200000: episode: 2563, duration: 4.574s, episode steps: 218, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.465779, mae: 9.973374, mean_q: -13.022240, mean_eps: 0.532717\n",
      "  623380/1200000: episode: 2564, duration: 4.782s, episode steps: 226, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.310210, mae: 10.008753, mean_q: -13.118771, mean_eps: 0.532550\n",
      "  623543/1200000: episode: 2565, duration: 3.547s, episode steps: 163, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.306390, mae: 10.162489, mean_q: -13.321677, mean_eps: 0.532404\n",
      "  623783/1200000: episode: 2566, duration: 5.106s, episode steps: 240, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.304537, mae: 10.048177, mean_q: -13.176239, mean_eps: 0.532253\n",
      "  624016/1200000: episode: 2567, duration: 4.891s, episode steps: 233, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.266 [0.000, 3.000],  loss: 0.297836, mae: 10.025414, mean_q: -13.159822, mean_eps: 0.532076\n",
      "  624227/1200000: episode: 2568, duration: 4.568s, episode steps: 211, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.298048, mae: 10.010129, mean_q: -13.114193, mean_eps: 0.531909\n",
      "  624493/1200000: episode: 2569, duration: 5.734s, episode steps: 266, steps per second:  46, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.259670, mae: 9.927130, mean_q: -13.002260, mean_eps: 0.531730\n",
      "  624722/1200000: episode: 2570, duration: 5.076s, episode steps: 229, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.257890, mae: 9.898205, mean_q: -12.978779, mean_eps: 0.531545\n",
      "  624938/1200000: episode: 2571, duration: 5.497s, episode steps: 216, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.296 [0.000, 3.000],  loss: 0.263230, mae: 10.067251, mean_q: -13.208852, mean_eps: 0.531378\n",
      "  625100/1200000: episode: 2572, duration: 3.504s, episode steps: 162, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.716 [0.000, 3.000],  loss: 0.328841, mae: 9.982527, mean_q: -13.076344, mean_eps: 0.531236\n",
      "  625273/1200000: episode: 2573, duration: 3.783s, episode steps: 173, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.301197, mae: 10.046117, mean_q: -13.158893, mean_eps: 0.531110\n",
      "  625510/1200000: episode: 2574, duration: 5.271s, episode steps: 237, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.351711, mae: 9.964621, mean_q: -13.062144, mean_eps: 0.530957\n",
      "  625683/1200000: episode: 2575, duration: 3.859s, episode steps: 173, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.270444, mae: 10.015976, mean_q: -13.130673, mean_eps: 0.530803\n",
      "  625845/1200000: episode: 2576, duration: 3.646s, episode steps: 162, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.383 [0.000, 3.000],  loss: 0.429450, mae: 10.072563, mean_q: -13.181533, mean_eps: 0.530677\n",
      "  626020/1200000: episode: 2577, duration: 3.754s, episode steps: 175, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: 0.298539, mae: 9.998191, mean_q: -13.099995, mean_eps: 0.530551\n",
      "  626193/1200000: episode: 2578, duration: 3.767s, episode steps: 173, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.723 [0.000, 3.000],  loss: 0.242700, mae: 10.111779, mean_q: -13.277958, mean_eps: 0.530420\n",
      "  626463/1200000: episode: 2579, duration: 5.838s, episode steps: 270, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.314166, mae: 9.991642, mean_q: -13.083371, mean_eps: 0.530254\n",
      "  626701/1200000: episode: 2580, duration: 5.023s, episode steps: 238, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.247134, mae: 10.087170, mean_q: -13.252665, mean_eps: 0.530064\n",
      "  626876/1200000: episode: 2581, duration: 3.786s, episode steps: 175, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.302579, mae: 9.978005, mean_q: -13.057352, mean_eps: 0.529909\n",
      "  627077/1200000: episode: 2582, duration: 4.416s, episode steps: 201, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.275009, mae: 9.966814, mean_q: -13.054475, mean_eps: 0.529768\n",
      "  627245/1200000: episode: 2583, duration: 3.645s, episode steps: 168, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.320885, mae: 10.011326, mean_q: -13.105125, mean_eps: 0.529630\n",
      "  627539/1200000: episode: 2584, duration: 6.232s, episode steps: 294, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.346776, mae: 10.050173, mean_q: -13.167160, mean_eps: 0.529456\n",
      "  627715/1200000: episode: 2585, duration: 3.795s, episode steps: 176, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.288466, mae: 9.970884, mean_q: -13.061977, mean_eps: 0.529280\n",
      "  627955/1200000: episode: 2586, duration: 5.316s, episode steps: 240, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.350546, mae: 9.956392, mean_q: -13.051923, mean_eps: 0.529124\n",
      "  628251/1200000: episode: 2587, duration: 6.299s, episode steps: 296, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.344794, mae: 10.002035, mean_q: -13.105220, mean_eps: 0.528923\n",
      "  628486/1200000: episode: 2588, duration: 4.949s, episode steps: 235, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.269402, mae: 9.930917, mean_q: -13.027444, mean_eps: 0.528724\n",
      "  628724/1200000: episode: 2589, duration: 5.135s, episode steps: 238, steps per second:  46, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.332154, mae: 10.036749, mean_q: -13.155784, mean_eps: 0.528547\n",
      "  629085/1200000: episode: 2590, duration: 7.711s, episode steps: 361, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.244402, mae: 10.031500, mean_q: -13.140253, mean_eps: 0.528322\n",
      "  629371/1200000: episode: 2591, duration: 6.067s, episode steps: 286, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.326859, mae: 9.999859, mean_q: -13.100687, mean_eps: 0.528079\n",
      "  629537/1200000: episode: 2592, duration: 3.521s, episode steps: 166, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.271558, mae: 10.024250, mean_q: -13.120906, mean_eps: 0.527910\n",
      "  629751/1200000: episode: 2593, duration: 4.686s, episode steps: 214, steps per second:  46, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.300780, mae: 9.925109, mean_q: -13.001077, mean_eps: 0.527767\n",
      "  629922/1200000: episode: 2594, duration: 3.719s, episode steps: 171, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.334208, mae: 9.896653, mean_q: -12.963413, mean_eps: 0.527623\n",
      "  630177/1200000: episode: 2595, duration: 5.480s, episode steps: 255, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.294948, mae: 9.779075, mean_q: -12.797312, mean_eps: 0.527463\n",
      "  630385/1200000: episode: 2596, duration: 4.773s, episode steps: 208, steps per second:  44, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.282400, mae: 9.825362, mean_q: -12.853327, mean_eps: 0.527290\n",
      "  630596/1200000: episode: 2597, duration: 4.474s, episode steps: 211, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.285527, mae: 9.644000, mean_q: -12.622791, mean_eps: 0.527132\n",
      "  630781/1200000: episode: 2598, duration: 4.164s, episode steps: 185, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.681 [0.000, 3.000],  loss: 0.363755, mae: 9.781669, mean_q: -12.838291, mean_eps: 0.526984\n",
      "  631104/1200000: episode: 2599, duration: 6.926s, episode steps: 323, steps per second:  47, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.336034, mae: 9.735590, mean_q: -12.750638, mean_eps: 0.526794\n",
      "  631324/1200000: episode: 2600, duration: 4.668s, episode steps: 220, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.316820, mae: 9.810131, mean_q: -12.856864, mean_eps: 0.526590\n",
      "  631505/1200000: episode: 2601, duration: 4.002s, episode steps: 181, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.801 [0.000, 3.000],  loss: 0.441975, mae: 9.747319, mean_q: -12.733240, mean_eps: 0.526440\n",
      "  631672/1200000: episode: 2602, duration: 3.802s, episode steps: 167, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.329 [0.000, 3.000],  loss: 0.248598, mae: 9.799906, mean_q: -12.842241, mean_eps: 0.526309\n",
      "  631844/1200000: episode: 2603, duration: 3.730s, episode steps: 172, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.318327, mae: 9.689438, mean_q: -12.674714, mean_eps: 0.526182\n",
      "  632041/1200000: episode: 2604, duration: 4.557s, episode steps: 197, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: 0.260268, mae: 9.705134, mean_q: -12.734702, mean_eps: 0.526043\n",
      "  632285/1200000: episode: 2605, duration: 5.290s, episode steps: 244, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.252651, mae: 9.699023, mean_q: -12.712390, mean_eps: 0.525878\n",
      "  632535/1200000: episode: 2606, duration: 5.582s, episode steps: 250, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.270320, mae: 9.710858, mean_q: -12.719421, mean_eps: 0.525693\n",
      "  632743/1200000: episode: 2607, duration: 5.016s, episode steps: 208, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.255 [0.000, 3.000],  loss: 0.267581, mae: 9.749623, mean_q: -12.794945, mean_eps: 0.525521\n",
      "  632974/1200000: episode: 2608, duration: 5.302s, episode steps: 231, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.290045, mae: 9.557915, mean_q: -12.520265, mean_eps: 0.525357\n",
      "  633188/1200000: episode: 2609, duration: 4.772s, episode steps: 214, steps per second:  45, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.317399, mae: 9.699991, mean_q: -12.712735, mean_eps: 0.525190\n",
      "  633391/1200000: episode: 2610, duration: 5.474s, episode steps: 203, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.268806, mae: 9.756402, mean_q: -12.794219, mean_eps: 0.525033\n",
      "  633602/1200000: episode: 2611, duration: 5.030s, episode steps: 211, steps per second:  42, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.230917, mae: 9.644423, mean_q: -12.658459, mean_eps: 0.524878\n",
      "  633887/1200000: episode: 2612, duration: 7.053s, episode steps: 285, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 0.262186, mae: 9.691408, mean_q: -12.696398, mean_eps: 0.524692\n",
      "  634173/1200000: episode: 2613, duration: 5.893s, episode steps: 286, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.317399, mae: 9.665966, mean_q: -12.652406, mean_eps: 0.524478\n",
      "  634343/1200000: episode: 2614, duration: 3.865s, episode steps: 170, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.316649, mae: 9.717167, mean_q: -12.731480, mean_eps: 0.524307\n",
      "  634511/1200000: episode: 2615, duration: 3.713s, episode steps: 168, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.205234, mae: 9.653978, mean_q: -12.665837, mean_eps: 0.524180\n",
      "  634708/1200000: episode: 2616, duration: 4.557s, episode steps: 197, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.746 [0.000, 3.000],  loss: 0.239113, mae: 9.765661, mean_q: -12.798060, mean_eps: 0.524043\n",
      "  634896/1200000: episode: 2617, duration: 4.371s, episode steps: 188, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 0.316476, mae: 9.680547, mean_q: -12.673202, mean_eps: 0.523899\n",
      "  635077/1200000: episode: 2618, duration: 3.557s, episode steps: 181, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.276949, mae: 9.698955, mean_q: -12.690209, mean_eps: 0.523761\n",
      "  635291/1200000: episode: 2619, duration: 4.466s, episode steps: 214, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.206894, mae: 9.674974, mean_q: -12.708882, mean_eps: 0.523612\n",
      "  635487/1200000: episode: 2620, duration: 3.837s, episode steps: 196, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.355598, mae: 9.773060, mean_q: -12.786367, mean_eps: 0.523459\n",
      "  635663/1200000: episode: 2621, duration: 3.433s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.795 [0.000, 3.000],  loss: 0.202421, mae: 9.744162, mean_q: -12.805315, mean_eps: 0.523319\n",
      "  635885/1200000: episode: 2622, duration: 4.196s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.330261, mae: 9.777296, mean_q: -12.788253, mean_eps: 0.523170\n",
      "  636165/1200000: episode: 2623, duration: 5.426s, episode steps: 280, steps per second:  52, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.256249, mae: 9.719604, mean_q: -12.741687, mean_eps: 0.522982\n",
      "  636334/1200000: episode: 2624, duration: 3.257s, episode steps: 169, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.267033, mae: 9.714111, mean_q: -12.733897, mean_eps: 0.522813\n",
      "  636592/1200000: episode: 2625, duration: 4.901s, episode steps: 258, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.318988, mae: 9.699007, mean_q: -12.712941, mean_eps: 0.522653\n",
      "  636771/1200000: episode: 2626, duration: 3.399s, episode steps: 179, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.328040, mae: 9.709032, mean_q: -12.709936, mean_eps: 0.522489\n",
      "  636942/1200000: episode: 2627, duration: 3.418s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.363 [0.000, 3.000],  loss: 0.316479, mae: 9.741033, mean_q: -12.764502, mean_eps: 0.522358\n",
      "  637131/1200000: episode: 2628, duration: 3.705s, episode steps: 189, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.175 [0.000, 3.000],  loss: 0.262134, mae: 9.589748, mean_q: -12.558625, mean_eps: 0.522223\n",
      "  637338/1200000: episode: 2629, duration: 3.969s, episode steps: 207, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.251769, mae: 9.740242, mean_q: -12.762170, mean_eps: 0.522074\n",
      "  637566/1200000: episode: 2630, duration: 4.351s, episode steps: 228, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.285842, mae: 9.747234, mean_q: -12.782507, mean_eps: 0.521911\n",
      "  637744/1200000: episode: 2631, duration: 3.517s, episode steps: 178, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.311508, mae: 9.721867, mean_q: -12.741664, mean_eps: 0.521759\n",
      "  637930/1200000: episode: 2632, duration: 3.606s, episode steps: 186, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.276187, mae: 9.604067, mean_q: -12.588432, mean_eps: 0.521623\n",
      "  638094/1200000: episode: 2633, duration: 3.164s, episode steps: 164, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.260725, mae: 9.606045, mean_q: -12.578861, mean_eps: 0.521491\n",
      "  638296/1200000: episode: 2634, duration: 4.093s, episode steps: 202, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.678 [0.000, 3.000],  loss: 0.314296, mae: 9.729342, mean_q: -12.757106, mean_eps: 0.521354\n",
      "  638592/1200000: episode: 2635, duration: 5.898s, episode steps: 296, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.361502, mae: 9.797520, mean_q: -12.811109, mean_eps: 0.521167\n",
      "  638813/1200000: episode: 2636, duration: 4.388s, episode steps: 221, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.257906, mae: 9.628327, mean_q: -12.624482, mean_eps: 0.520973\n",
      "  639000/1200000: episode: 2637, duration: 3.602s, episode steps: 187, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.319188, mae: 9.588906, mean_q: -12.528563, mean_eps: 0.520821\n",
      "  639181/1200000: episode: 2638, duration: 3.780s, episode steps: 181, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.331730, mae: 9.626133, mean_q: -12.609751, mean_eps: 0.520683\n",
      "  639393/1200000: episode: 2639, duration: 4.151s, episode steps: 212, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.310874, mae: 9.714357, mean_q: -12.723670, mean_eps: 0.520535\n",
      "  639572/1200000: episode: 2640, duration: 3.435s, episode steps: 179, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.242080, mae: 9.748369, mean_q: -12.781736, mean_eps: 0.520389\n",
      "  639781/1200000: episode: 2641, duration: 4.142s, episode steps: 209, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.290732, mae: 9.619556, mean_q: -12.584276, mean_eps: 0.520243\n",
      "  639948/1200000: episode: 2642, duration: 3.237s, episode steps: 167, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.388692, mae: 9.730071, mean_q: -12.737952, mean_eps: 0.520102\n",
      "  640126/1200000: episode: 2643, duration: 3.586s, episode steps: 178, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.252344, mae: 9.498992, mean_q: -12.448660, mean_eps: 0.519973\n",
      "  640317/1200000: episode: 2644, duration: 3.618s, episode steps: 191, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.770 [0.000, 3.000],  loss: 0.268752, mae: 9.588629, mean_q: -12.573141, mean_eps: 0.519834\n",
      "  640483/1200000: episode: 2645, duration: 3.269s, episode steps: 166, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.208340, mae: 9.534535, mean_q: -12.502695, mean_eps: 0.519700\n",
      "  640664/1200000: episode: 2646, duration: 3.689s, episode steps: 181, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: 0.260106, mae: 9.498974, mean_q: -12.448560, mean_eps: 0.519570\n",
      "  640837/1200000: episode: 2647, duration: 3.481s, episode steps: 173, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.257303, mae: 9.541906, mean_q: -12.515841, mean_eps: 0.519437\n",
      "  641014/1200000: episode: 2648, duration: 3.465s, episode steps: 177, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.263192, mae: 9.527315, mean_q: -12.460713, mean_eps: 0.519306\n",
      "  641247/1200000: episode: 2649, duration: 4.539s, episode steps: 233, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.223043, mae: 9.529827, mean_q: -12.500551, mean_eps: 0.519153\n",
      "  641461/1200000: episode: 2650, duration: 4.098s, episode steps: 214, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.813 [0.000, 3.000],  loss: 0.242741, mae: 9.496444, mean_q: -12.449658, mean_eps: 0.518985\n",
      "  641679/1200000: episode: 2651, duration: 4.302s, episode steps: 218, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.101 [0.000, 3.000],  loss: 0.345269, mae: 9.542957, mean_q: -12.502961, mean_eps: 0.518823\n",
      "  641897/1200000: episode: 2652, duration: 4.321s, episode steps: 218, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.213861, mae: 9.535673, mean_q: -12.524259, mean_eps: 0.518659\n",
      "  642163/1200000: episode: 2653, duration: 5.284s, episode steps: 266, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.238339, mae: 9.488231, mean_q: -12.450337, mean_eps: 0.518478\n",
      "  642372/1200000: episode: 2654, duration: 4.125s, episode steps: 209, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 0.230470, mae: 9.562130, mean_q: -12.530399, mean_eps: 0.518300\n",
      "  642580/1200000: episode: 2655, duration: 5.663s, episode steps: 208, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.227991, mae: 9.480190, mean_q: -12.429057, mean_eps: 0.518143\n",
      "  642751/1200000: episode: 2656, duration: 3.389s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.784 [0.000, 3.000],  loss: 0.342371, mae: 9.630375, mean_q: -12.612223, mean_eps: 0.518001\n",
      "  642958/1200000: episode: 2657, duration: 4.094s, episode steps: 207, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.195767, mae: 9.555323, mean_q: -12.538039, mean_eps: 0.517860\n",
      "  643126/1200000: episode: 2658, duration: 4.005s, episode steps: 168, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.246652, mae: 9.582141, mean_q: -12.552719, mean_eps: 0.517719\n",
      "  643317/1200000: episode: 2659, duration: 4.301s, episode steps: 191, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.764 [0.000, 3.000],  loss: 0.215683, mae: 9.645968, mean_q: -12.633518, mean_eps: 0.517584\n",
      "  643653/1200000: episode: 2660, duration: 8.889s, episode steps: 336, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.277012, mae: 9.546132, mean_q: -12.503273, mean_eps: 0.517387\n",
      "  643827/1200000: episode: 2661, duration: 5.225s, episode steps: 174, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 0.221584, mae: 9.472672, mean_q: -12.415459, mean_eps: 0.517195\n",
      "  644046/1200000: episode: 2662, duration: 4.703s, episode steps: 219, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.282631, mae: 9.608683, mean_q: -12.601691, mean_eps: 0.517048\n",
      "  644395/1200000: episode: 2663, duration: 7.218s, episode steps: 349, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.228149, mae: 9.441803, mean_q: -12.378569, mean_eps: 0.516835\n",
      "  644586/1200000: episode: 2664, duration: 3.735s, episode steps: 191, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.245133, mae: 9.502791, mean_q: -12.446404, mean_eps: 0.516632\n",
      "  644811/1200000: episode: 2665, duration: 4.519s, episode steps: 225, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.221964, mae: 9.509896, mean_q: -12.449164, mean_eps: 0.516477\n",
      "  644980/1200000: episode: 2666, duration: 4.056s, episode steps: 169, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.339257, mae: 9.520265, mean_q: -12.463689, mean_eps: 0.516329\n",
      "  645217/1200000: episode: 2667, duration: 5.017s, episode steps: 237, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 0.231659, mae: 9.497026, mean_q: -12.468818, mean_eps: 0.516177\n",
      "  645417/1200000: episode: 2668, duration: 3.983s, episode steps: 200, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.425 [0.000, 3.000],  loss: 0.289071, mae: 9.447507, mean_q: -12.370423, mean_eps: 0.516013\n",
      "  645588/1200000: episode: 2669, duration: 3.509s, episode steps: 171, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.260366, mae: 9.457452, mean_q: -12.388211, mean_eps: 0.515873\n",
      "  645767/1200000: episode: 2670, duration: 3.714s, episode steps: 179, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.253171, mae: 9.470820, mean_q: -12.410348, mean_eps: 0.515742\n",
      "  646041/1200000: episode: 2671, duration: 5.824s, episode steps: 274, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.246760, mae: 9.628462, mean_q: -12.614854, mean_eps: 0.515572\n",
      "  646214/1200000: episode: 2672, duration: 3.702s, episode steps: 173, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.775 [0.000, 3.000],  loss: 0.253219, mae: 9.542398, mean_q: -12.517008, mean_eps: 0.515405\n",
      "  646444/1200000: episode: 2673, duration: 4.527s, episode steps: 230, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.258091, mae: 9.565723, mean_q: -12.547622, mean_eps: 0.515254\n",
      "  646627/1200000: episode: 2674, duration: 3.641s, episode steps: 183, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.230054, mae: 9.553772, mean_q: -12.540430, mean_eps: 0.515099\n",
      "  646801/1200000: episode: 2675, duration: 3.585s, episode steps: 174, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.276619, mae: 9.463917, mean_q: -12.411926, mean_eps: 0.514965\n",
      "  646971/1200000: episode: 2676, duration: 3.782s, episode steps: 170, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.841 [0.000, 3.000],  loss: 0.293637, mae: 9.547876, mean_q: -12.482719, mean_eps: 0.514836\n",
      "  647132/1200000: episode: 2677, duration: 3.522s, episode steps: 161, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.271101, mae: 9.483270, mean_q: -12.428578, mean_eps: 0.514712\n",
      "  647298/1200000: episode: 2678, duration: 3.248s, episode steps: 166, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.349 [0.000, 3.000],  loss: 0.208992, mae: 9.485732, mean_q: -12.447535, mean_eps: 0.514589\n",
      "  647583/1200000: episode: 2679, duration: 5.878s, episode steps: 285, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.347779, mae: 9.551581, mean_q: -12.520852, mean_eps: 0.514420\n",
      "  647786/1200000: episode: 2680, duration: 4.477s, episode steps: 203, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.320524, mae: 9.552873, mean_q: -12.513671, mean_eps: 0.514237\n",
      "  648000/1200000: episode: 2681, duration: 4.523s, episode steps: 214, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.283441, mae: 9.474719, mean_q: -12.419920, mean_eps: 0.514081\n",
      "  648315/1200000: episode: 2682, duration: 6.536s, episode steps: 315, steps per second:  48, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.267309, mae: 9.477132, mean_q: -12.412485, mean_eps: 0.513882\n",
      "  648519/1200000: episode: 2683, duration: 4.598s, episode steps: 204, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.299673, mae: 9.452570, mean_q: -12.371933, mean_eps: 0.513688\n",
      "  648729/1200000: episode: 2684, duration: 4.063s, episode steps: 210, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.246891, mae: 9.459595, mean_q: -12.410243, mean_eps: 0.513532\n",
      "  648977/1200000: episode: 2685, duration: 5.532s, episode steps: 248, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.274148, mae: 9.582086, mean_q: -12.562748, mean_eps: 0.513361\n",
      "  649304/1200000: episode: 2686, duration: 6.590s, episode steps: 327, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.202533, mae: 9.403576, mean_q: -12.336748, mean_eps: 0.513145\n",
      "  649703/1200000: episode: 2687, duration: 7.909s, episode steps: 399, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.255840, mae: 9.455297, mean_q: -12.398384, mean_eps: 0.512873\n",
      "  649938/1200000: episode: 2688, duration: 4.827s, episode steps: 235, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.261150, mae: 9.574770, mean_q: -12.579524, mean_eps: 0.512635\n",
      "  650238/1200000: episode: 2689, duration: 5.890s, episode steps: 300, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.254621, mae: 9.411419, mean_q: -12.350677, mean_eps: 0.512434\n",
      "  650463/1200000: episode: 2690, duration: 4.452s, episode steps: 225, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.338 [0.000, 3.000],  loss: 0.226331, mae: 9.377989, mean_q: -12.300829, mean_eps: 0.512238\n",
      "  650684/1200000: episode: 2691, duration: 4.403s, episode steps: 221, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.248467, mae: 9.437033, mean_q: -12.390409, mean_eps: 0.512070\n",
      "  650899/1200000: episode: 2692, duration: 4.151s, episode steps: 215, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.292557, mae: 9.522642, mean_q: -12.497067, mean_eps: 0.511907\n",
      "  651120/1200000: episode: 2693, duration: 4.415s, episode steps: 221, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.300745, mae: 9.454990, mean_q: -12.406800, mean_eps: 0.511743\n",
      "  651312/1200000: episode: 2694, duration: 3.811s, episode steps: 192, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.261618, mae: 9.451083, mean_q: -12.383560, mean_eps: 0.511588\n",
      "  651487/1200000: episode: 2695, duration: 3.625s, episode steps: 175, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.258093, mae: 9.388746, mean_q: -12.306355, mean_eps: 0.511451\n",
      "  651654/1200000: episode: 2696, duration: 3.329s, episode steps: 167, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.347 [0.000, 3.000],  loss: 0.294625, mae: 9.457578, mean_q: -12.406365, mean_eps: 0.511323\n",
      "  651823/1200000: episode: 2697, duration: 3.423s, episode steps: 169, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.308247, mae: 9.406300, mean_q: -12.328573, mean_eps: 0.511197\n",
      "  651990/1200000: episode: 2698, duration: 3.728s, episode steps: 167, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.252562, mae: 9.389254, mean_q: -12.315572, mean_eps: 0.511070\n",
      "  652168/1200000: episode: 2699, duration: 3.689s, episode steps: 178, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.250769, mae: 9.412397, mean_q: -12.324412, mean_eps: 0.510941\n",
      "  652348/1200000: episode: 2700, duration: 3.704s, episode steps: 180, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.220610, mae: 9.511435, mean_q: -12.486270, mean_eps: 0.510807\n",
      "  652764/1200000: episode: 2701, duration: 8.352s, episode steps: 416, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.256997, mae: 9.398059, mean_q: -12.330952, mean_eps: 0.510583\n",
      "  652948/1200000: episode: 2702, duration: 3.972s, episode steps: 184, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 0.267912, mae: 9.440786, mean_q: -12.392730, mean_eps: 0.510358\n",
      "  653145/1200000: episode: 2703, duration: 4.061s, episode steps: 197, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.670 [0.000, 3.000],  loss: 0.245547, mae: 9.440669, mean_q: -12.387227, mean_eps: 0.510216\n",
      "  653342/1200000: episode: 2704, duration: 3.883s, episode steps: 197, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.231453, mae: 9.454348, mean_q: -12.402785, mean_eps: 0.510068\n",
      "  653613/1200000: episode: 2705, duration: 5.662s, episode steps: 271, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.299 [0.000, 3.000],  loss: 0.246189, mae: 9.396302, mean_q: -12.333548, mean_eps: 0.509892\n",
      "  653853/1200000: episode: 2706, duration: 4.945s, episode steps: 240, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.197863, mae: 9.422181, mean_q: -12.391522, mean_eps: 0.509701\n",
      "  654020/1200000: episode: 2707, duration: 3.466s, episode steps: 167, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.191579, mae: 9.354740, mean_q: -12.290666, mean_eps: 0.509548\n",
      "  654194/1200000: episode: 2708, duration: 3.597s, episode steps: 174, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.730 [0.000, 3.000],  loss: 0.188449, mae: 9.333686, mean_q: -12.258519, mean_eps: 0.509420\n",
      "  654445/1200000: episode: 2709, duration: 5.269s, episode steps: 251, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.242581, mae: 9.472910, mean_q: -12.423697, mean_eps: 0.509261\n",
      "  654671/1200000: episode: 2710, duration: 4.559s, episode steps: 226, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.258466, mae: 9.392567, mean_q: -12.321924, mean_eps: 0.509082\n",
      "  654904/1200000: episode: 2711, duration: 4.543s, episode steps: 233, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.243404, mae: 9.390302, mean_q: -12.315993, mean_eps: 0.508910\n",
      "  655222/1200000: episode: 2712, duration: 6.320s, episode steps: 318, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.197722, mae: 9.360207, mean_q: -12.293131, mean_eps: 0.508703\n",
      "  655452/1200000: episode: 2713, duration: 4.614s, episode steps: 230, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.274380, mae: 9.431699, mean_q: -12.357597, mean_eps: 0.508498\n",
      "  655677/1200000: episode: 2714, duration: 4.481s, episode steps: 225, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.269736, mae: 9.435178, mean_q: -12.362349, mean_eps: 0.508327\n",
      "  655853/1200000: episode: 2715, duration: 3.639s, episode steps: 176, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.224115, mae: 9.478139, mean_q: -12.445919, mean_eps: 0.508177\n",
      "  656063/1200000: episode: 2716, duration: 4.119s, episode steps: 210, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.276589, mae: 9.339959, mean_q: -12.246154, mean_eps: 0.508032\n",
      "  656262/1200000: episode: 2717, duration: 3.882s, episode steps: 199, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.307 [0.000, 3.000],  loss: 0.207146, mae: 9.420540, mean_q: -12.367799, mean_eps: 0.507879\n",
      "  656447/1200000: episode: 2718, duration: 3.677s, episode steps: 185, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.378 [0.000, 3.000],  loss: 0.238155, mae: 9.463118, mean_q: -12.425147, mean_eps: 0.507734\n",
      "  656635/1200000: episode: 2719, duration: 3.779s, episode steps: 188, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.325645, mae: 9.384614, mean_q: -12.285903, mean_eps: 0.507595\n",
      "  656927/1200000: episode: 2720, duration: 5.741s, episode steps: 292, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.270362, mae: 9.360232, mean_q: -12.277486, mean_eps: 0.507415\n",
      "  657095/1200000: episode: 2721, duration: 3.375s, episode steps: 168, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.260306, mae: 9.529680, mean_q: -12.491792, mean_eps: 0.507242\n",
      "  657262/1200000: episode: 2722, duration: 3.392s, episode steps: 167, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.275 [0.000, 3.000],  loss: 0.255852, mae: 9.319042, mean_q: -12.236444, mean_eps: 0.507116\n",
      "  657457/1200000: episode: 2723, duration: 3.887s, episode steps: 195, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.969 [0.000, 3.000],  loss: 0.266505, mae: 9.382473, mean_q: -12.290426, mean_eps: 0.506981\n",
      "  657634/1200000: episode: 2724, duration: 3.652s, episode steps: 177, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.257739, mae: 9.379175, mean_q: -12.291581, mean_eps: 0.506841\n",
      "  657847/1200000: episode: 2725, duration: 4.274s, episode steps: 213, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.190063, mae: 9.386577, mean_q: -12.343153, mean_eps: 0.506695\n",
      "  658019/1200000: episode: 2726, duration: 4.182s, episode steps: 172, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.356138, mae: 9.446634, mean_q: -12.369109, mean_eps: 0.506551\n",
      "  658319/1200000: episode: 2727, duration: 6.727s, episode steps: 300, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.250804, mae: 9.455269, mean_q: -12.414346, mean_eps: 0.506374\n",
      "  658510/1200000: episode: 2728, duration: 4.396s, episode steps: 191, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.205302, mae: 9.435702, mean_q: -12.392227, mean_eps: 0.506189\n",
      "  658710/1200000: episode: 2729, duration: 4.602s, episode steps: 200, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.243608, mae: 9.364072, mean_q: -12.275197, mean_eps: 0.506043\n",
      "  658880/1200000: episode: 2730, duration: 3.915s, episode steps: 170, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.258893, mae: 9.340582, mean_q: -12.249376, mean_eps: 0.505904\n",
      "  659096/1200000: episode: 2731, duration: 4.885s, episode steps: 216, steps per second:  44, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.307036, mae: 9.363207, mean_q: -12.242915, mean_eps: 0.505759\n",
      "  659355/1200000: episode: 2732, duration: 5.867s, episode steps: 259, steps per second:  44, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.277698, mae: 9.377608, mean_q: -12.296669, mean_eps: 0.505581\n",
      "  659586/1200000: episode: 2733, duration: 5.143s, episode steps: 231, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.260 [0.000, 3.000],  loss: 0.220012, mae: 9.464915, mean_q: -12.431883, mean_eps: 0.505398\n",
      "  659763/1200000: episode: 2734, duration: 4.044s, episode steps: 177, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.180544, mae: 9.352387, mean_q: -12.287272, mean_eps: 0.505244\n",
      "  659970/1200000: episode: 2735, duration: 4.788s, episode steps: 207, steps per second:  43, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.218836, mae: 9.431714, mean_q: -12.368858, mean_eps: 0.505100\n",
      "  660268/1200000: episode: 2736, duration: 6.583s, episode steps: 298, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.213773, mae: 8.928898, mean_q: -11.703101, mean_eps: 0.504911\n",
      "  660531/1200000: episode: 2737, duration: 6.039s, episode steps: 263, steps per second:  44, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.361 [0.000, 3.000],  loss: 0.173680, mae: 8.872535, mean_q: -11.654105, mean_eps: 0.504701\n",
      "  660713/1200000: episode: 2738, duration: 4.112s, episode steps: 182, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.248207, mae: 8.973925, mean_q: -11.770203, mean_eps: 0.504534\n",
      "  661003/1200000: episode: 2739, duration: 6.480s, episode steps: 290, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.251710, mae: 8.868568, mean_q: -11.617061, mean_eps: 0.504357\n",
      "  661263/1200000: episode: 2740, duration: 6.284s, episode steps: 260, steps per second:  41, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: 0.227906, mae: 8.914557, mean_q: -11.678112, mean_eps: 0.504151\n",
      "  661612/1200000: episode: 2741, duration: 8.390s, episode steps: 349, steps per second:  42, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.226761, mae: 8.884736, mean_q: -11.647973, mean_eps: 0.503922\n",
      "  661770/1200000: episode: 2742, duration: 3.597s, episode steps: 158, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.248410, mae: 8.861285, mean_q: -11.627930, mean_eps: 0.503732\n",
      "  661932/1200000: episode: 2743, duration: 3.638s, episode steps: 162, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.154857, mae: 8.817175, mean_q: -11.576626, mean_eps: 0.503612\n",
      "  662106/1200000: episode: 2744, duration: 3.933s, episode steps: 174, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.266755, mae: 8.939727, mean_q: -11.722518, mean_eps: 0.503486\n",
      "  662280/1200000: episode: 2745, duration: 3.966s, episode steps: 174, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.246952, mae: 8.853282, mean_q: -11.608316, mean_eps: 0.503356\n",
      "  662446/1200000: episode: 2746, duration: 3.853s, episode steps: 166, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.300361, mae: 8.847038, mean_q: -11.599575, mean_eps: 0.503228\n",
      "  662634/1200000: episode: 2747, duration: 4.145s, episode steps: 188, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 0.188533, mae: 8.800996, mean_q: -11.541062, mean_eps: 0.503095\n",
      "  662825/1200000: episode: 2748, duration: 4.249s, episode steps: 191, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: 0.194478, mae: 8.905515, mean_q: -11.682554, mean_eps: 0.502953\n",
      "  663008/1200000: episode: 2749, duration: 4.157s, episode steps: 183, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.181597, mae: 8.839765, mean_q: -11.605307, mean_eps: 0.502813\n",
      "  663237/1200000: episode: 2750, duration: 5.320s, episode steps: 229, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.226821, mae: 8.794076, mean_q: -11.530826, mean_eps: 0.502659\n",
      "  663414/1200000: episode: 2751, duration: 4.093s, episode steps: 177, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.201813, mae: 9.018843, mean_q: -11.848593, mean_eps: 0.502506\n",
      "  663595/1200000: episode: 2752, duration: 4.170s, episode steps: 181, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.624 [0.000, 3.000],  loss: 0.175088, mae: 8.857163, mean_q: -11.618852, mean_eps: 0.502372\n",
      "  663882/1200000: episode: 2753, duration: 6.561s, episode steps: 287, steps per second:  44, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.216827, mae: 8.867044, mean_q: -11.640164, mean_eps: 0.502197\n",
      "  664164/1200000: episode: 2754, duration: 6.604s, episode steps: 282, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.213582, mae: 8.901542, mean_q: -11.675342, mean_eps: 0.501983\n",
      "  664337/1200000: episode: 2755, duration: 3.944s, episode steps: 173, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.204428, mae: 8.848370, mean_q: -11.610874, mean_eps: 0.501812\n",
      "  664557/1200000: episode: 2756, duration: 4.979s, episode steps: 220, steps per second:  44, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.248761, mae: 8.782412, mean_q: -11.503756, mean_eps: 0.501665\n",
      "  664773/1200000: episode: 2757, duration: 5.218s, episode steps: 216, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.287 [0.000, 3.000],  loss: 0.244838, mae: 8.859396, mean_q: -11.605857, mean_eps: 0.501502\n",
      "  664951/1200000: episode: 2758, duration: 4.123s, episode steps: 178, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.612 [0.000, 3.000],  loss: 0.265894, mae: 8.839327, mean_q: -11.582298, mean_eps: 0.501354\n",
      "  665136/1200000: episode: 2759, duration: 4.332s, episode steps: 185, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.686 [0.000, 3.000],  loss: 0.227507, mae: 8.870650, mean_q: -11.625818, mean_eps: 0.501218\n",
      "  665326/1200000: episode: 2760, duration: 4.206s, episode steps: 190, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 0.245691, mae: 8.777559, mean_q: -11.481996, mean_eps: 0.501077\n",
      "  665509/1200000: episode: 2761, duration: 4.068s, episode steps: 183, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.190322, mae: 8.907948, mean_q: -11.676182, mean_eps: 0.500937\n",
      "  665691/1200000: episode: 2762, duration: 4.191s, episode steps: 182, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.209900, mae: 8.868039, mean_q: -11.627138, mean_eps: 0.500800\n",
      "  665875/1200000: episode: 2763, duration: 4.314s, episode steps: 184, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.231341, mae: 8.879700, mean_q: -11.643367, mean_eps: 0.500663\n",
      "  666116/1200000: episode: 2764, duration: 5.422s, episode steps: 241, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.780 [0.000, 3.000],  loss: 0.257913, mae: 8.867920, mean_q: -11.618071, mean_eps: 0.500504\n",
      "  666317/1200000: episode: 2765, duration: 4.600s, episode steps: 201, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.177484, mae: 8.793944, mean_q: -11.541551, mean_eps: 0.500338\n",
      "  666545/1200000: episode: 2766, duration: 5.361s, episode steps: 228, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.211906, mae: 8.888426, mean_q: -11.657247, mean_eps: 0.500177\n",
      "  666853/1200000: episode: 2767, duration: 7.112s, episode steps: 308, steps per second:  43, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.222180, mae: 8.868180, mean_q: -11.642282, mean_eps: 0.499976\n",
      "  667128/1200000: episode: 2768, duration: 6.245s, episode steps: 275, steps per second:  44, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.231163, mae: 8.829718, mean_q: -11.587957, mean_eps: 0.499758\n",
      "  667312/1200000: episode: 2769, duration: 4.199s, episode steps: 184, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.180821, mae: 8.868363, mean_q: -11.669369, mean_eps: 0.499585\n",
      "  667721/1200000: episode: 2770, duration: 9.280s, episode steps: 409, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.192281, mae: 8.849876, mean_q: -11.613319, mean_eps: 0.499363\n",
      "  667998/1200000: episode: 2771, duration: 6.378s, episode steps: 277, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.239644, mae: 8.903058, mean_q: -11.674897, mean_eps: 0.499106\n",
      "  668193/1200000: episode: 2772, duration: 4.475s, episode steps: 195, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.272 [0.000, 3.000],  loss: 0.233183, mae: 8.921039, mean_q: -11.704708, mean_eps: 0.498929\n",
      "  668362/1200000: episode: 2773, duration: 3.937s, episode steps: 169, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.197739, mae: 8.857655, mean_q: -11.611122, mean_eps: 0.498792\n",
      "  668551/1200000: episode: 2774, duration: 4.290s, episode steps: 189, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.259980, mae: 8.853687, mean_q: -11.602595, mean_eps: 0.498658\n",
      "  668740/1200000: episode: 2775, duration: 4.373s, episode steps: 189, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.196 [0.000, 3.000],  loss: 0.316851, mae: 8.843453, mean_q: -11.585624, mean_eps: 0.498516\n",
      "  668949/1200000: episode: 2776, duration: 4.907s, episode steps: 209, steps per second:  43, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.182793, mae: 8.807659, mean_q: -11.566927, mean_eps: 0.498367\n",
      "  669214/1200000: episode: 2777, duration: 6.101s, episode steps: 265, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.222785, mae: 8.857808, mean_q: -11.627401, mean_eps: 0.498189\n",
      "  669447/1200000: episode: 2778, duration: 5.319s, episode steps: 233, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.191406, mae: 8.820440, mean_q: -11.579603, mean_eps: 0.498003\n",
      "  669791/1200000: episode: 2779, duration: 7.786s, episode steps: 344, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.210140, mae: 8.838440, mean_q: -11.588913, mean_eps: 0.497786\n",
      "  670048/1200000: episode: 2780, duration: 5.827s, episode steps: 257, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.303825, mae: 8.815079, mean_q: -11.559818, mean_eps: 0.497561\n",
      "  670220/1200000: episode: 2781, duration: 4.129s, episode steps: 172, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.070 [0.000, 3.000],  loss: 0.231540, mae: 8.519705, mean_q: -11.169267, mean_eps: 0.497400\n",
      "  670403/1200000: episode: 2782, duration: 4.170s, episode steps: 183, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.249275, mae: 8.504832, mean_q: -11.154550, mean_eps: 0.497267\n",
      "  670686/1200000: episode: 2783, duration: 6.459s, episode steps: 283, steps per second:  44, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.237362, mae: 8.521431, mean_q: -11.184571, mean_eps: 0.497092\n",
      "  670903/1200000: episode: 2784, duration: 5.169s, episode steps: 217, steps per second:  42, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.323400, mae: 8.572134, mean_q: -11.217423, mean_eps: 0.496904\n",
      "  671145/1200000: episode: 2785, duration: 5.806s, episode steps: 242, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.172999, mae: 8.505829, mean_q: -11.151013, mean_eps: 0.496732\n",
      "  671388/1200000: episode: 2786, duration: 5.734s, episode steps: 243, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.186187, mae: 8.595042, mean_q: -11.275800, mean_eps: 0.496551\n",
      "  671883/1200000: episode: 2787, duration: 11.319s, episode steps: 495, steps per second:  44, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.191167, mae: 8.518865, mean_q: -11.175542, mean_eps: 0.496274\n",
      "  672129/1200000: episode: 2788, duration: 5.667s, episode steps: 246, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.166725, mae: 8.533375, mean_q: -11.198489, mean_eps: 0.495996\n",
      "  672409/1200000: episode: 2789, duration: 6.479s, episode steps: 280, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.217189, mae: 8.537101, mean_q: -11.189064, mean_eps: 0.495799\n",
      "  672694/1200000: episode: 2790, duration: 6.756s, episode steps: 285, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.218461, mae: 8.519256, mean_q: -11.172289, mean_eps: 0.495587\n",
      "  672944/1200000: episode: 2791, duration: 5.849s, episode steps: 250, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.244496, mae: 8.492620, mean_q: -11.129003, mean_eps: 0.495386\n",
      "  673111/1200000: episode: 2792, duration: 3.909s, episode steps: 167, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.137812, mae: 8.525370, mean_q: -11.209676, mean_eps: 0.495230\n",
      "  673338/1200000: episode: 2793, duration: 5.228s, episode steps: 227, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.201341, mae: 8.563128, mean_q: -11.226224, mean_eps: 0.495082\n",
      "  673557/1200000: episode: 2794, duration: 5.217s, episode steps: 219, steps per second:  42, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.249436, mae: 8.570133, mean_q: -11.214159, mean_eps: 0.494915\n",
      "  673777/1200000: episode: 2795, duration: 5.394s, episode steps: 220, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.841 [0.000, 3.000],  loss: 0.222025, mae: 8.514175, mean_q: -11.147180, mean_eps: 0.494750\n",
      "  673971/1200000: episode: 2796, duration: 4.619s, episode steps: 194, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.301789, mae: 8.555272, mean_q: -11.181387, mean_eps: 0.494595\n",
      "  674133/1200000: episode: 2797, duration: 3.940s, episode steps: 162, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.173658, mae: 8.584591, mean_q: -11.269447, mean_eps: 0.494461\n",
      "  674418/1200000: episode: 2798, duration: 6.980s, episode steps: 285, steps per second:  41, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.241544, mae: 8.570920, mean_q: -11.227312, mean_eps: 0.494294\n",
      "  674690/1200000: episode: 2799, duration: 6.330s, episode steps: 272, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.172066, mae: 8.495197, mean_q: -11.139140, mean_eps: 0.494085\n",
      "  674936/1200000: episode: 2800, duration: 6.221s, episode steps: 246, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.228474, mae: 8.537400, mean_q: -11.193102, mean_eps: 0.493891\n",
      "  675118/1200000: episode: 2801, duration: 4.131s, episode steps: 182, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.198201, mae: 8.504877, mean_q: -11.140158, mean_eps: 0.493730\n",
      "  675298/1200000: episode: 2802, duration: 4.353s, episode steps: 180, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.182675, mae: 8.553391, mean_q: -11.227264, mean_eps: 0.493594\n",
      "  675510/1200000: episode: 2803, duration: 4.978s, episode steps: 212, steps per second:  43, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.228755, mae: 8.575593, mean_q: -11.247383, mean_eps: 0.493447\n",
      "  675708/1200000: episode: 2804, duration: 4.554s, episode steps: 198, steps per second:  43, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.164514, mae: 8.469535, mean_q: -11.131358, mean_eps: 0.493294\n",
      "  675989/1200000: episode: 2805, duration: 6.549s, episode steps: 281, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.240587, mae: 8.522161, mean_q: -11.174307, mean_eps: 0.493114\n",
      "  676221/1200000: episode: 2806, duration: 5.437s, episode steps: 232, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.229054, mae: 8.600141, mean_q: -11.285865, mean_eps: 0.492922\n",
      "  676424/1200000: episode: 2807, duration: 4.707s, episode steps: 203, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.211141, mae: 8.458357, mean_q: -11.074554, mean_eps: 0.492758\n",
      "  676608/1200000: episode: 2808, duration: 4.281s, episode steps: 184, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.179474, mae: 8.567652, mean_q: -11.245346, mean_eps: 0.492613\n",
      "  676778/1200000: episode: 2809, duration: 4.037s, episode steps: 170, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.240940, mae: 8.557886, mean_q: -11.221714, mean_eps: 0.492481\n",
      "  677009/1200000: episode: 2810, duration: 5.488s, episode steps: 231, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.217340, mae: 8.556991, mean_q: -11.208416, mean_eps: 0.492330\n",
      "  677246/1200000: episode: 2811, duration: 5.486s, episode steps: 237, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.198391, mae: 8.549763, mean_q: -11.210496, mean_eps: 0.492155\n",
      "  677438/1200000: episode: 2812, duration: 4.461s, episode steps: 192, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.719 [0.000, 3.000],  loss: 0.245218, mae: 8.493923, mean_q: -11.113835, mean_eps: 0.491994\n",
      "  677667/1200000: episode: 2813, duration: 5.315s, episode steps: 229, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.213884, mae: 8.549676, mean_q: -11.191980, mean_eps: 0.491836\n",
      "  677856/1200000: episode: 2814, duration: 4.524s, episode steps: 189, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.290807, mae: 8.499891, mean_q: -11.119252, mean_eps: 0.491679\n",
      "  678096/1200000: episode: 2815, duration: 5.501s, episode steps: 240, steps per second:  44, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.199990, mae: 8.413920, mean_q: -11.024000, mean_eps: 0.491518\n",
      "  678441/1200000: episode: 2816, duration: 7.835s, episode steps: 345, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.212690, mae: 8.508755, mean_q: -11.151207, mean_eps: 0.491299\n",
      "  678603/1200000: episode: 2817, duration: 3.853s, episode steps: 162, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.827 [0.000, 3.000],  loss: 0.174596, mae: 8.539596, mean_q: -11.209255, mean_eps: 0.491109\n",
      "  678849/1200000: episode: 2818, duration: 5.867s, episode steps: 246, steps per second:  42, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.197228, mae: 8.586965, mean_q: -11.270846, mean_eps: 0.490956\n",
      "  679088/1200000: episode: 2819, duration: 5.571s, episode steps: 239, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.225284, mae: 8.568528, mean_q: -11.232165, mean_eps: 0.490774\n",
      "  679253/1200000: episode: 2820, duration: 3.835s, episode steps: 165, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: 0.213793, mae: 8.549906, mean_q: -11.195877, mean_eps: 0.490623\n",
      "  679418/1200000: episode: 2821, duration: 3.939s, episode steps: 165, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.267326, mae: 8.589817, mean_q: -11.245652, mean_eps: 0.490499\n",
      "  679634/1200000: episode: 2822, duration: 5.335s, episode steps: 216, steps per second:  40, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.219949, mae: 8.483531, mean_q: -11.109111, mean_eps: 0.490356\n",
      "  679827/1200000: episode: 2823, duration: 4.622s, episode steps: 193, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.202307, mae: 8.517105, mean_q: -11.178967, mean_eps: 0.490202\n",
      "  680016/1200000: episode: 2824, duration: 4.348s, episode steps: 189, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.211248, mae: 8.551727, mean_q: -11.208424, mean_eps: 0.490059\n",
      "  680253/1200000: episode: 2825, duration: 5.518s, episode steps: 237, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.211201, mae: 8.317931, mean_q: -10.903288, mean_eps: 0.489899\n",
      "  680491/1200000: episode: 2826, duration: 5.762s, episode steps: 238, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.223605, mae: 8.336190, mean_q: -10.911887, mean_eps: 0.489721\n",
      "  680658/1200000: episode: 2827, duration: 4.051s, episode steps: 167, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.158916, mae: 8.251774, mean_q: -10.812652, mean_eps: 0.489569\n",
      "  680822/1200000: episode: 2828, duration: 3.939s, episode steps: 164, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.211885, mae: 8.452115, mean_q: -11.071632, mean_eps: 0.489445\n",
      "  681000/1200000: episode: 2829, duration: 4.153s, episode steps: 178, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.191803, mae: 8.254899, mean_q: -10.826180, mean_eps: 0.489317\n",
      "  681181/1200000: episode: 2830, duration: 4.302s, episode steps: 181, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.148596, mae: 8.351271, mean_q: -10.958904, mean_eps: 0.489183\n",
      "  681418/1200000: episode: 2831, duration: 5.595s, episode steps: 237, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 0.190635, mae: 8.337672, mean_q: -10.930416, mean_eps: 0.489026\n",
      "  681778/1200000: episode: 2832, duration: 8.356s, episode steps: 360, steps per second:  43, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: 0.198309, mae: 8.355146, mean_q: -10.947301, mean_eps: 0.488802\n",
      "  682019/1200000: episode: 2833, duration: 5.714s, episode steps: 241, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.179215, mae: 8.340238, mean_q: -10.939524, mean_eps: 0.488577\n",
      "  682243/1200000: episode: 2834, duration: 5.213s, episode steps: 224, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.330 [0.000, 3.000],  loss: 0.164032, mae: 8.303527, mean_q: -10.882430, mean_eps: 0.488402\n",
      "  682420/1200000: episode: 2835, duration: 4.349s, episode steps: 177, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: 0.207147, mae: 8.328424, mean_q: -10.906785, mean_eps: 0.488252\n",
      "  682689/1200000: episode: 2836, duration: 6.320s, episode steps: 269, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.222959, mae: 8.345059, mean_q: -10.916589, mean_eps: 0.488084\n",
      "  682866/1200000: episode: 2837, duration: 4.221s, episode steps: 177, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.216388, mae: 8.357245, mean_q: -10.944218, mean_eps: 0.487917\n",
      "  683149/1200000: episode: 2838, duration: 6.717s, episode steps: 283, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.217785, mae: 8.349672, mean_q: -10.920990, mean_eps: 0.487745\n",
      "  683333/1200000: episode: 2839, duration: 4.369s, episode steps: 184, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.216368, mae: 8.387162, mean_q: -10.985077, mean_eps: 0.487570\n",
      "  683526/1200000: episode: 2840, duration: 4.476s, episode steps: 193, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 0.200522, mae: 8.432892, mean_q: -11.052847, mean_eps: 0.487428\n",
      "  683703/1200000: episode: 2841, duration: 4.235s, episode steps: 177, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.140614, mae: 8.299743, mean_q: -10.903447, mean_eps: 0.487289\n",
      "  683880/1200000: episode: 2842, duration: 4.271s, episode steps: 177, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.261315, mae: 8.337241, mean_q: -10.890004, mean_eps: 0.487157\n",
      "  684055/1200000: episode: 2843, duration: 4.218s, episode steps: 175, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.223191, mae: 8.272437, mean_q: -10.843881, mean_eps: 0.487025\n",
      "  684233/1200000: episode: 2844, duration: 4.237s, episode steps: 178, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.279574, mae: 8.396574, mean_q: -11.000962, mean_eps: 0.486892\n",
      "  684459/1200000: episode: 2845, duration: 5.353s, episode steps: 226, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.314 [0.000, 3.000],  loss: 0.238610, mae: 8.439657, mean_q: -11.066434, mean_eps: 0.486741\n",
      "  684627/1200000: episode: 2846, duration: 4.190s, episode steps: 168, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.242304, mae: 8.354442, mean_q: -10.945500, mean_eps: 0.486593\n",
      "  684803/1200000: episode: 2847, duration: 4.338s, episode steps: 176, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.193684, mae: 8.370873, mean_q: -10.966912, mean_eps: 0.486464\n",
      "  684969/1200000: episode: 2848, duration: 3.959s, episode steps: 166, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 0.183744, mae: 8.344674, mean_q: -10.940874, mean_eps: 0.486336\n",
      "  685241/1200000: episode: 2849, duration: 6.325s, episode steps: 272, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.229323, mae: 8.330581, mean_q: -10.919455, mean_eps: 0.486172\n",
      "  685409/1200000: episode: 2850, duration: 4.062s, episode steps: 168, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.845 [0.000, 3.000],  loss: 0.240580, mae: 8.347472, mean_q: -10.922036, mean_eps: 0.486007\n",
      "  685576/1200000: episode: 2851, duration: 3.960s, episode steps: 167, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.219479, mae: 8.254317, mean_q: -10.804753, mean_eps: 0.485881\n",
      "  685757/1200000: episode: 2852, duration: 4.405s, episode steps: 181, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.198667, mae: 8.317218, mean_q: -10.925362, mean_eps: 0.485750\n",
      "  685941/1200000: episode: 2853, duration: 4.362s, episode steps: 184, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.826 [0.000, 3.000],  loss: 0.281390, mae: 8.257195, mean_q: -10.804887, mean_eps: 0.485614\n",
      "  686133/1200000: episode: 2854, duration: 4.527s, episode steps: 192, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.222087, mae: 8.384253, mean_q: -10.976227, mean_eps: 0.485473\n",
      "  686409/1200000: episode: 2855, duration: 6.543s, episode steps: 276, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.308 [0.000, 3.000],  loss: 0.223560, mae: 8.296695, mean_q: -10.868134, mean_eps: 0.485297\n",
      "  686600/1200000: episode: 2856, duration: 4.782s, episode steps: 191, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.206051, mae: 8.292792, mean_q: -10.874877, mean_eps: 0.485122\n",
      "  686963/1200000: episode: 2857, duration: 8.686s, episode steps: 363, steps per second:  42, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.181176, mae: 8.333770, mean_q: -10.925051, mean_eps: 0.484914\n",
      "  687143/1200000: episode: 2858, duration: 4.455s, episode steps: 180, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.233585, mae: 8.413795, mean_q: -11.020159, mean_eps: 0.484711\n",
      "  687312/1200000: episode: 2859, duration: 4.069s, episode steps: 169, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.325 [0.000, 3.000],  loss: 0.220037, mae: 8.328595, mean_q: -10.908790, mean_eps: 0.484580\n",
      "  687486/1200000: episode: 2860, duration: 4.241s, episode steps: 174, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.374 [0.000, 3.000],  loss: 0.203759, mae: 8.291821, mean_q: -10.858382, mean_eps: 0.484451\n",
      "  687749/1200000: episode: 2861, duration: 6.258s, episode steps: 263, steps per second:  42, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.168297, mae: 8.384479, mean_q: -10.991947, mean_eps: 0.484287\n",
      "  687928/1200000: episode: 2862, duration: 4.342s, episode steps: 179, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.089 [0.000, 3.000],  loss: 0.292950, mae: 8.361136, mean_q: -10.922286, mean_eps: 0.484121\n",
      "  688110/1200000: episode: 2863, duration: 4.329s, episode steps: 182, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.188072, mae: 8.339177, mean_q: -10.933027, mean_eps: 0.483986\n",
      "  688281/1200000: episode: 2864, duration: 4.064s, episode steps: 171, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.227502, mae: 8.281250, mean_q: -10.836651, mean_eps: 0.483854\n",
      "  688529/1200000: episode: 2865, duration: 5.715s, episode steps: 248, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.214726, mae: 8.328784, mean_q: -10.901879, mean_eps: 0.483697\n",
      "  688692/1200000: episode: 2866, duration: 3.962s, episode steps: 163, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.190534, mae: 8.282622, mean_q: -10.854256, mean_eps: 0.483543\n",
      "  688937/1200000: episode: 2867, duration: 5.887s, episode steps: 245, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.219476, mae: 8.343273, mean_q: -10.926897, mean_eps: 0.483390\n",
      "  689118/1200000: episode: 2868, duration: 4.347s, episode steps: 181, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.183489, mae: 8.338985, mean_q: -10.940339, mean_eps: 0.483230\n",
      "  689369/1200000: episode: 2869, duration: 5.828s, episode steps: 251, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.207668, mae: 8.351058, mean_q: -10.944797, mean_eps: 0.483068\n",
      "  689605/1200000: episode: 2870, duration: 5.788s, episode steps: 236, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.194618, mae: 8.326012, mean_q: -10.937718, mean_eps: 0.482885\n",
      "  689902/1200000: episode: 2871, duration: 6.996s, episode steps: 297, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.266 [0.000, 3.000],  loss: 0.193755, mae: 8.359614, mean_q: -10.954557, mean_eps: 0.482685\n",
      "  690118/1200000: episode: 2872, duration: 5.168s, episode steps: 216, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.293755, mae: 8.247335, mean_q: -10.757508, mean_eps: 0.482493\n",
      "  690370/1200000: episode: 2873, duration: 5.954s, episode steps: 252, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.172511, mae: 8.059740, mean_q: -10.547895, mean_eps: 0.482317\n",
      "  690671/1200000: episode: 2874, duration: 7.143s, episode steps: 301, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.199327, mae: 8.103028, mean_q: -10.598379, mean_eps: 0.482110\n",
      "  690985/1200000: episode: 2875, duration: 7.368s, episode steps: 314, steps per second:  43, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.203949, mae: 8.029529, mean_q: -10.490760, mean_eps: 0.481879\n",
      "  691323/1200000: episode: 2876, duration: 7.974s, episode steps: 338, steps per second:  42, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.198810, mae: 8.039792, mean_q: -10.523468, mean_eps: 0.481635\n",
      "  691492/1200000: episode: 2877, duration: 4.080s, episode steps: 169, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.236377, mae: 8.031899, mean_q: -10.501837, mean_eps: 0.481445\n",
      "  691713/1200000: episode: 2878, duration: 5.275s, episode steps: 221, steps per second:  42, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.206606, mae: 8.057293, mean_q: -10.534102, mean_eps: 0.481299\n",
      "  692073/1200000: episode: 2879, duration: 8.471s, episode steps: 360, steps per second:  43, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.331 [0.000, 3.000],  loss: 0.174129, mae: 7.986518, mean_q: -10.457937, mean_eps: 0.481081\n",
      "  692240/1200000: episode: 2880, duration: 4.263s, episode steps: 167, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.171853, mae: 8.081857, mean_q: -10.563209, mean_eps: 0.480883\n",
      "  692408/1200000: episode: 2881, duration: 4.182s, episode steps: 168, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.167166, mae: 7.911734, mean_q: -10.357229, mean_eps: 0.480757\n",
      "  692634/1200000: episode: 2882, duration: 5.440s, episode steps: 226, steps per second:  42, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: 0.189954, mae: 8.013469, mean_q: -10.482760, mean_eps: 0.480610\n",
      "  692928/1200000: episode: 2883, duration: 7.026s, episode steps: 294, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.204425, mae: 8.037899, mean_q: -10.523503, mean_eps: 0.480415\n",
      "  693460/1200000: episode: 2884, duration: 12.713s, episode steps: 532, steps per second:  42, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.184054, mae: 8.067987, mean_q: -10.566080, mean_eps: 0.480105\n",
      "  693639/1200000: episode: 2885, duration: 4.304s, episode steps: 179, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.648 [0.000, 3.000],  loss: 0.182862, mae: 8.095719, mean_q: -10.590318, mean_eps: 0.479838\n",
      "  693860/1200000: episode: 2886, duration: 5.331s, episode steps: 221, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.188953, mae: 8.014161, mean_q: -10.495191, mean_eps: 0.479688\n",
      "  694032/1200000: episode: 2887, duration: 4.236s, episode steps: 172, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.523 [0.000, 3.000],  loss: 0.168347, mae: 8.006708, mean_q: -10.475555, mean_eps: 0.479541\n",
      "  694210/1200000: episode: 2888, duration: 4.323s, episode steps: 178, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.309 [0.000, 3.000],  loss: 0.158842, mae: 8.060170, mean_q: -10.564947, mean_eps: 0.479410\n",
      "  694379/1200000: episode: 2889, duration: 4.066s, episode steps: 169, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.211922, mae: 8.072831, mean_q: -10.572640, mean_eps: 0.479279\n",
      "  694717/1200000: episode: 2890, duration: 8.319s, episode steps: 338, steps per second:  41, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.176986, mae: 7.997284, mean_q: -10.465669, mean_eps: 0.479089\n",
      "  694899/1200000: episode: 2891, duration: 4.447s, episode steps: 182, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.648 [0.000, 3.000],  loss: 0.248176, mae: 8.053235, mean_q: -10.514898, mean_eps: 0.478894\n",
      "  695110/1200000: episode: 2892, duration: 5.208s, episode steps: 211, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.228414, mae: 8.101060, mean_q: -10.584797, mean_eps: 0.478747\n",
      "  695287/1200000: episode: 2893, duration: 4.245s, episode steps: 177, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.740 [0.000, 3.000],  loss: 0.183448, mae: 8.123083, mean_q: -10.629750, mean_eps: 0.478602\n",
      "  695455/1200000: episode: 2894, duration: 4.375s, episode steps: 168, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.144767, mae: 8.021476, mean_q: -10.501323, mean_eps: 0.478472\n",
      "  695628/1200000: episode: 2895, duration: 4.305s, episode steps: 173, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.191550, mae: 8.074284, mean_q: -10.542428, mean_eps: 0.478344\n",
      "  695807/1200000: episode: 2896, duration: 4.399s, episode steps: 179, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.174435, mae: 7.993855, mean_q: -10.463159, mean_eps: 0.478212\n",
      "  695989/1200000: episode: 2897, duration: 4.512s, episode steps: 182, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.224590, mae: 8.112380, mean_q: -10.603037, mean_eps: 0.478077\n",
      "  696165/1200000: episode: 2898, duration: 4.234s, episode steps: 176, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.210 [0.000, 3.000],  loss: 0.223758, mae: 7.958462, mean_q: -10.424820, mean_eps: 0.477943\n",
      "  696403/1200000: episode: 2899, duration: 5.738s, episode steps: 238, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.213214, mae: 8.044116, mean_q: -10.533554, mean_eps: 0.477787\n",
      "  696710/1200000: episode: 2900, duration: 7.403s, episode steps: 307, steps per second:  41, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.183717, mae: 8.057387, mean_q: -10.549942, mean_eps: 0.477583\n",
      "  696884/1200000: episode: 2901, duration: 4.184s, episode steps: 174, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.157874, mae: 7.987895, mean_q: -10.467772, mean_eps: 0.477403\n",
      "  697122/1200000: episode: 2902, duration: 5.981s, episode steps: 238, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.194643, mae: 8.043319, mean_q: -10.517229, mean_eps: 0.477248\n",
      "  697482/1200000: episode: 2903, duration: 8.688s, episode steps: 360, steps per second:  41, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.239347, mae: 7.999563, mean_q: -10.449886, mean_eps: 0.477024\n",
      "  697708/1200000: episode: 2904, duration: 5.485s, episode steps: 226, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: 0.242222, mae: 8.094072, mean_q: -10.588752, mean_eps: 0.476804\n",
      "  697915/1200000: episode: 2905, duration: 5.396s, episode steps: 207, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.197570, mae: 8.003556, mean_q: -10.471902, mean_eps: 0.476642\n",
      "  698336/1200000: episode: 2906, duration: 10.318s, episode steps: 421, steps per second:  41, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.160421, mae: 8.056791, mean_q: -10.555375, mean_eps: 0.476406\n",
      "  698601/1200000: episode: 2907, duration: 6.536s, episode steps: 265, steps per second:  41, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.214649, mae: 8.009399, mean_q: -10.463323, mean_eps: 0.476149\n",
      "  698841/1200000: episode: 2908, duration: 5.822s, episode steps: 240, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.225993, mae: 8.041384, mean_q: -10.510118, mean_eps: 0.475960\n",
      "  699087/1200000: episode: 2909, duration: 6.009s, episode steps: 246, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.233409, mae: 8.025328, mean_q: -10.494341, mean_eps: 0.475777\n",
      "  699263/1200000: episode: 2910, duration: 4.327s, episode steps: 176, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.190742, mae: 8.081959, mean_q: -10.571556, mean_eps: 0.475619\n",
      "  699439/1200000: episode: 2911, duration: 4.290s, episode steps: 176, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.358 [0.000, 3.000],  loss: 0.202273, mae: 8.034037, mean_q: -10.505033, mean_eps: 0.475487\n",
      "  699725/1200000: episode: 2912, duration: 7.223s, episode steps: 286, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.245219, mae: 8.001098, mean_q: -10.463895, mean_eps: 0.475314\n",
      "  699890/1200000: episode: 2913, duration: 4.134s, episode steps: 165, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.195775, mae: 8.123018, mean_q: -10.634654, mean_eps: 0.475145\n",
      "  700128/1200000: episode: 2914, duration: 5.804s, episode steps: 238, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: 0.230522, mae: 8.038151, mean_q: -10.511888, mean_eps: 0.474994\n",
      "  700300/1200000: episode: 2915, duration: 4.332s, episode steps: 172, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.161712, mae: 7.876316, mean_q: -10.330621, mean_eps: 0.474840\n",
      "  700479/1200000: episode: 2916, duration: 4.429s, episode steps: 179, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.570 [0.000, 3.000],  loss: 0.168095, mae: 8.042231, mean_q: -10.548334, mean_eps: 0.474708\n",
      "  700677/1200000: episode: 2917, duration: 4.819s, episode steps: 198, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.187532, mae: 7.923793, mean_q: -10.372117, mean_eps: 0.474567\n",
      "  700906/1200000: episode: 2918, duration: 5.595s, episode steps: 229, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.217689, mae: 8.047696, mean_q: -10.542062, mean_eps: 0.474407\n",
      "  701093/1200000: episode: 2919, duration: 4.499s, episode steps: 187, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.156777, mae: 8.032382, mean_q: -10.533970, mean_eps: 0.474251\n",
      "  701317/1200000: episode: 2920, duration: 5.462s, episode steps: 224, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.226005, mae: 7.973751, mean_q: -10.430030, mean_eps: 0.474097\n",
      "  701622/1200000: episode: 2921, duration: 7.234s, episode steps: 305, steps per second:  42, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.175134, mae: 7.950881, mean_q: -10.411140, mean_eps: 0.473898\n",
      "  701792/1200000: episode: 2922, duration: 4.308s, episode steps: 170, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.151734, mae: 7.986441, mean_q: -10.468598, mean_eps: 0.473720\n",
      "  701992/1200000: episode: 2923, duration: 5.048s, episode steps: 200, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.163342, mae: 7.976781, mean_q: -10.453325, mean_eps: 0.473581\n",
      "  702229/1200000: episode: 2924, duration: 5.770s, episode steps: 237, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: 0.184738, mae: 7.935084, mean_q: -10.389097, mean_eps: 0.473417\n",
      "  702539/1200000: episode: 2925, duration: 7.496s, episode steps: 310, steps per second:  41, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.192238, mae: 7.967521, mean_q: -10.431950, mean_eps: 0.473212\n",
      "  702743/1200000: episode: 2926, duration: 4.999s, episode steps: 204, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.166827, mae: 7.944803, mean_q: -10.415476, mean_eps: 0.473020\n",
      "  702918/1200000: episode: 2927, duration: 4.478s, episode steps: 175, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.206657, mae: 7.995786, mean_q: -10.467563, mean_eps: 0.472877\n",
      "  703239/1200000: episode: 2928, duration: 7.778s, episode steps: 321, steps per second:  41, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.153770, mae: 7.973144, mean_q: -10.433451, mean_eps: 0.472691\n",
      "  703580/1200000: episode: 2929, duration: 8.210s, episode steps: 341, steps per second:  42, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.198288, mae: 7.951170, mean_q: -10.406467, mean_eps: 0.472443\n",
      "  703821/1200000: episode: 2930, duration: 6.005s, episode steps: 241, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.201497, mae: 7.965937, mean_q: -10.420515, mean_eps: 0.472225\n",
      "  704073/1200000: episode: 2931, duration: 6.174s, episode steps: 252, steps per second:  41, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.178095, mae: 7.929912, mean_q: -10.365611, mean_eps: 0.472040\n",
      "  704273/1200000: episode: 2932, duration: 4.894s, episode steps: 200, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.182066, mae: 7.937578, mean_q: -10.383488, mean_eps: 0.471871\n",
      "  704446/1200000: episode: 2933, duration: 4.404s, episode steps: 173, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.220534, mae: 7.976874, mean_q: -10.436227, mean_eps: 0.471731\n",
      "  704916/1200000: episode: 2934, duration: 11.842s, episode steps: 470, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.198719, mae: 7.971737, mean_q: -10.443429, mean_eps: 0.471490\n",
      "  705096/1200000: episode: 2935, duration: 4.487s, episode steps: 180, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.261135, mae: 8.055393, mean_q: -10.530730, mean_eps: 0.471246\n",
      "  705279/1200000: episode: 2936, duration: 4.711s, episode steps: 183, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.251 [0.000, 3.000],  loss: 0.178145, mae: 7.963392, mean_q: -10.436430, mean_eps: 0.471110\n",
      "  705473/1200000: episode: 2937, duration: 4.962s, episode steps: 194, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.163066, mae: 7.992039, mean_q: -10.483325, mean_eps: 0.470968\n",
      "  705680/1200000: episode: 2938, duration: 5.071s, episode steps: 207, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.185701, mae: 7.942381, mean_q: -10.406914, mean_eps: 0.470818\n",
      "  705893/1200000: episode: 2939, duration: 5.152s, episode steps: 213, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.153591, mae: 7.987796, mean_q: -10.469912, mean_eps: 0.470660\n",
      "  706134/1200000: episode: 2940, duration: 5.978s, episode steps: 241, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.171990, mae: 7.910411, mean_q: -10.370259, mean_eps: 0.470490\n",
      "  706309/1200000: episode: 2941, duration: 4.357s, episode steps: 175, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.137 [0.000, 3.000],  loss: 0.188569, mae: 8.004792, mean_q: -10.489140, mean_eps: 0.470334\n",
      "  706488/1200000: episode: 2942, duration: 4.456s, episode steps: 179, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.257 [0.000, 3.000],  loss: 0.173317, mae: 7.933730, mean_q: -10.393903, mean_eps: 0.470201\n",
      "  706782/1200000: episode: 2943, duration: 7.189s, episode steps: 294, steps per second:  41, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.198530, mae: 8.021286, mean_q: -10.495836, mean_eps: 0.470024\n",
      "  707045/1200000: episode: 2944, duration: 6.636s, episode steps: 263, steps per second:  40, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.151144, mae: 7.888055, mean_q: -10.336068, mean_eps: 0.469815\n",
      "  707217/1200000: episode: 2945, duration: 4.537s, episode steps: 172, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.139585, mae: 7.897643, mean_q: -10.345781, mean_eps: 0.469652\n",
      "  707418/1200000: episode: 2946, duration: 5.118s, episode steps: 201, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.192568, mae: 7.926313, mean_q: -10.368887, mean_eps: 0.469512\n",
      "  707657/1200000: episode: 2947, duration: 6.010s, episode steps: 239, steps per second:  40, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.191069, mae: 7.882060, mean_q: -10.316134, mean_eps: 0.469347\n",
      "  707832/1200000: episode: 2948, duration: 4.405s, episode steps: 175, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.125504, mae: 7.977425, mean_q: -10.460575, mean_eps: 0.469192\n",
      "  707999/1200000: episode: 2949, duration: 4.179s, episode steps: 167, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.192557, mae: 7.980288, mean_q: -10.444568, mean_eps: 0.469064\n",
      "  708246/1200000: episode: 2950, duration: 6.016s, episode steps: 247, steps per second:  41, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.178965, mae: 7.984641, mean_q: -10.465120, mean_eps: 0.468909\n",
      "  708552/1200000: episode: 2951, duration: 7.548s, episode steps: 306, steps per second:  41, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.169032, mae: 7.920587, mean_q: -10.378008, mean_eps: 0.468701\n",
      "  708901/1200000: episode: 2952, duration: 8.592s, episode steps: 349, steps per second:  41, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.211967, mae: 7.950191, mean_q: -10.415169, mean_eps: 0.468456\n",
      "  709093/1200000: episode: 2953, duration: 4.805s, episode steps: 192, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.206037, mae: 7.998068, mean_q: -10.478825, mean_eps: 0.468253\n",
      "  709286/1200000: episode: 2954, duration: 4.912s, episode steps: 193, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.181642, mae: 7.984559, mean_q: -10.455051, mean_eps: 0.468108\n",
      "  709570/1200000: episode: 2955, duration: 7.057s, episode steps: 284, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.170895, mae: 7.976629, mean_q: -10.441725, mean_eps: 0.467929\n",
      "  709752/1200000: episode: 2956, duration: 4.491s, episode steps: 182, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.140370, mae: 8.089978, mean_q: -10.607607, mean_eps: 0.467755\n",
      "  709955/1200000: episode: 2957, duration: 5.180s, episode steps: 203, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.174021, mae: 7.952962, mean_q: -10.418895, mean_eps: 0.467610\n",
      "  710146/1200000: episode: 2958, duration: 4.926s, episode steps: 191, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.192363, mae: 7.861767, mean_q: -10.289497, mean_eps: 0.467463\n",
      "  710489/1200000: episode: 2959, duration: 8.591s, episode steps: 343, steps per second:  40, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.149838, mae: 7.752926, mean_q: -10.164921, mean_eps: 0.467262\n",
      "  710665/1200000: episode: 2960, duration: 4.389s, episode steps: 176, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.191335, mae: 7.781045, mean_q: -10.187956, mean_eps: 0.467068\n",
      "  710832/1200000: episode: 2961, duration: 4.184s, episode steps: 167, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.198 [0.000, 3.000],  loss: 0.201610, mae: 7.812814, mean_q: -10.228728, mean_eps: 0.466939\n",
      "  711005/1200000: episode: 2962, duration: 4.420s, episode steps: 173, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.237 [0.000, 3.000],  loss: 0.180726, mae: 7.784888, mean_q: -10.198557, mean_eps: 0.466811\n",
      "  711177/1200000: episode: 2963, duration: 4.318s, episode steps: 172, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.156779, mae: 7.708949, mean_q: -10.112974, mean_eps: 0.466682\n",
      "  711535/1200000: episode: 2964, duration: 8.563s, episode steps: 358, steps per second:  42, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.176511, mae: 7.752472, mean_q: -10.145959, mean_eps: 0.466483\n",
      "  711699/1200000: episode: 2965, duration: 4.764s, episode steps: 164, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.305 [0.000, 3.000],  loss: 0.136854, mae: 7.723061, mean_q: -10.110467, mean_eps: 0.466288\n",
      "  711858/1200000: episode: 2966, duration: 4.796s, episode steps: 159, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.173233, mae: 7.752247, mean_q: -10.146373, mean_eps: 0.466167\n",
      "  712169/1200000: episode: 2967, duration: 7.980s, episode steps: 311, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.166524, mae: 7.740457, mean_q: -10.143016, mean_eps: 0.465990\n",
      "  712341/1200000: episode: 2968, duration: 4.067s, episode steps: 172, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.175438, mae: 7.780345, mean_q: -10.201452, mean_eps: 0.465809\n",
      "  712633/1200000: episode: 2969, duration: 7.070s, episode steps: 292, steps per second:  41, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.194925, mae: 7.786289, mean_q: -10.209371, mean_eps: 0.465635\n",
      "  712800/1200000: episode: 2970, duration: 4.234s, episode steps: 167, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.317 [0.000, 3.000],  loss: 0.117397, mae: 7.714959, mean_q: -10.129444, mean_eps: 0.465463\n",
      "  713075/1200000: episode: 2971, duration: 7.109s, episode steps: 275, steps per second:  39, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.143881, mae: 7.761074, mean_q: -10.179971, mean_eps: 0.465297\n",
      "  713313/1200000: episode: 2972, duration: 7.116s, episode steps: 238, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.135273, mae: 7.649549, mean_q: -10.026189, mean_eps: 0.465105\n",
      "  713535/1200000: episode: 2973, duration: 7.269s, episode steps: 222, steps per second:  31, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.186433, mae: 7.788314, mean_q: -10.186240, mean_eps: 0.464932\n",
      "  713845/1200000: episode: 2974, duration: 10.653s, episode steps: 310, steps per second:  29, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.151026, mae: 7.758558, mean_q: -10.151814, mean_eps: 0.464733\n",
      "  714022/1200000: episode: 2975, duration: 5.571s, episode steps: 177, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.169618, mae: 7.683366, mean_q: -10.059838, mean_eps: 0.464550\n",
      "  714188/1200000: episode: 2976, duration: 5.432s, episode steps: 166, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.135599, mae: 7.700481, mean_q: -10.095515, mean_eps: 0.464422\n",
      "  714394/1200000: episode: 2977, duration: 5.389s, episode steps: 206, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.194056, mae: 7.746312, mean_q: -10.145227, mean_eps: 0.464282\n",
      "  714603/1200000: episode: 2978, duration: 5.962s, episode steps: 209, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.155085, mae: 7.747429, mean_q: -10.135542, mean_eps: 0.464127\n",
      "  714770/1200000: episode: 2979, duration: 5.031s, episode steps: 167, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.485 [0.000, 3.000],  loss: 0.197957, mae: 7.722693, mean_q: -10.110570, mean_eps: 0.463985\n",
      "  715359/1200000: episode: 2980, duration: 15.078s, episode steps: 589, steps per second:  39, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.167987, mae: 7.777803, mean_q: -10.190036, mean_eps: 0.463702\n",
      "  715603/1200000: episode: 2981, duration: 6.255s, episode steps: 244, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.177416, mae: 7.746767, mean_q: -10.149145, mean_eps: 0.463390\n",
      "  715936/1200000: episode: 2982, duration: 8.492s, episode steps: 333, steps per second:  39, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.161857, mae: 7.721545, mean_q: -10.113877, mean_eps: 0.463173\n",
      "  716156/1200000: episode: 2983, duration: 5.788s, episode steps: 220, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.197711, mae: 7.698683, mean_q: -10.077534, mean_eps: 0.462966\n",
      "  716317/1200000: episode: 2984, duration: 4.198s, episode steps: 161, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.224 [0.000, 3.000],  loss: 0.153319, mae: 7.771487, mean_q: -10.174078, mean_eps: 0.462823\n",
      "  716501/1200000: episode: 2985, duration: 4.647s, episode steps: 184, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.158181, mae: 7.729057, mean_q: -10.122101, mean_eps: 0.462694\n",
      "  716674/1200000: episode: 2986, duration: 4.480s, episode steps: 173, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: 0.173852, mae: 7.785159, mean_q: -10.192026, mean_eps: 0.462560\n",
      "  716986/1200000: episode: 2987, duration: 8.086s, episode steps: 312, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.154708, mae: 7.729844, mean_q: -10.136013, mean_eps: 0.462378\n",
      "  717265/1200000: episode: 2988, duration: 6.988s, episode steps: 279, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.156180, mae: 7.818912, mean_q: -10.251782, mean_eps: 0.462156\n",
      "  717470/1200000: episode: 2989, duration: 5.227s, episode steps: 205, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.179668, mae: 7.769392, mean_q: -10.176424, mean_eps: 0.461975\n",
      "  717650/1200000: episode: 2990, duration: 4.590s, episode steps: 180, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.184755, mae: 7.721923, mean_q: -10.106415, mean_eps: 0.461830\n",
      "  717826/1200000: episode: 2991, duration: 4.399s, episode steps: 176, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.295 [0.000, 3.000],  loss: 0.184288, mae: 7.707249, mean_q: -10.093072, mean_eps: 0.461697\n",
      "  718003/1200000: episode: 2992, duration: 4.529s, episode steps: 177, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.140141, mae: 7.736044, mean_q: -10.151253, mean_eps: 0.461564\n",
      "  718326/1200000: episode: 2993, duration: 8.101s, episode steps: 323, steps per second:  40, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.155479, mae: 7.789921, mean_q: -10.216650, mean_eps: 0.461377\n",
      "  718500/1200000: episode: 2994, duration: 4.512s, episode steps: 174, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.328 [0.000, 3.000],  loss: 0.183270, mae: 7.758474, mean_q: -10.156611, mean_eps: 0.461191\n",
      "  718753/1200000: episode: 2995, duration: 6.424s, episode steps: 253, steps per second:  39, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.148142, mae: 7.708709, mean_q: -10.111349, mean_eps: 0.461031\n",
      "  719032/1200000: episode: 2996, duration: 6.999s, episode steps: 279, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.160352, mae: 7.739321, mean_q: -10.149162, mean_eps: 0.460831\n",
      "  719213/1200000: episode: 2997, duration: 4.872s, episode steps: 181, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.167656, mae: 7.794220, mean_q: -10.208216, mean_eps: 0.460658\n",
      "  719547/1200000: episode: 2998, duration: 8.729s, episode steps: 334, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.147937, mae: 7.751741, mean_q: -10.158621, mean_eps: 0.460465\n",
      "  719751/1200000: episode: 2999, duration: 5.802s, episode steps: 204, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.177607, mae: 7.764280, mean_q: -10.160507, mean_eps: 0.460264\n",
      "  719963/1200000: episode: 3000, duration: 5.417s, episode steps: 212, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.151181, mae: 7.627668, mean_q: -9.990717, mean_eps: 0.460108\n",
      "  720250/1200000: episode: 3001, duration: 7.879s, episode steps: 287, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: 0.186527, mae: 7.529523, mean_q: -9.847630, mean_eps: 0.459921\n",
      "  720454/1200000: episode: 3002, duration: 6.114s, episode steps: 204, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.161513, mae: 7.450222, mean_q: -9.749565, mean_eps: 0.459736\n",
      "  720639/1200000: episode: 3003, duration: 4.883s, episode steps: 185, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 0.161764, mae: 7.522566, mean_q: -9.832867, mean_eps: 0.459591\n",
      "  720892/1200000: episode: 3004, duration: 6.410s, episode steps: 253, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.151829, mae: 7.511718, mean_q: -9.827838, mean_eps: 0.459426\n",
      "  721131/1200000: episode: 3005, duration: 6.175s, episode steps: 239, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.190345, mae: 7.535958, mean_q: -9.857035, mean_eps: 0.459242\n",
      "  721424/1200000: episode: 3006, duration: 7.411s, episode steps: 293, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.169553, mae: 7.523997, mean_q: -9.845205, mean_eps: 0.459042\n",
      "  721660/1200000: episode: 3007, duration: 6.167s, episode steps: 236, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.141027, mae: 7.511038, mean_q: -9.833659, mean_eps: 0.458844\n",
      "  721966/1200000: episode: 3008, duration: 12.300s, episode steps: 306, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.193716, mae: 7.512473, mean_q: -9.836108, mean_eps: 0.458641\n",
      "  722127/1200000: episode: 3009, duration: 6.026s, episode steps: 161, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.185077, mae: 7.542059, mean_q: -9.865570, mean_eps: 0.458465\n",
      "  722330/1200000: episode: 3010, duration: 6.905s, episode steps: 203, steps per second:  29, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.189565, mae: 7.471642, mean_q: -9.785310, mean_eps: 0.458329\n",
      "  722513/1200000: episode: 3011, duration: 6.073s, episode steps: 183, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.150549, mae: 7.571629, mean_q: -9.924722, mean_eps: 0.458184\n",
      "  722708/1200000: episode: 3012, duration: 7.143s, episode steps: 195, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.194824, mae: 7.453116, mean_q: -9.751149, mean_eps: 0.458042\n",
      "  723070/1200000: episode: 3013, duration: 12.684s, episode steps: 362, steps per second:  29, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.157524, mae: 7.490130, mean_q: -9.810838, mean_eps: 0.457834\n",
      "  723372/1200000: episode: 3014, duration: 9.597s, episode steps: 302, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.196796, mae: 7.481313, mean_q: -9.778872, mean_eps: 0.457585\n",
      "  723546/1200000: episode: 3015, duration: 5.744s, episode steps: 174, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.148439, mae: 7.528959, mean_q: -9.864226, mean_eps: 0.457406\n",
      "  723712/1200000: episode: 3016, duration: 5.622s, episode steps: 166, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.265 [0.000, 3.000],  loss: 0.182699, mae: 7.573719, mean_q: -9.912949, mean_eps: 0.457279\n",
      "  723891/1200000: episode: 3017, duration: 5.139s, episode steps: 179, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.216120, mae: 7.468949, mean_q: -9.773584, mean_eps: 0.457149\n",
      "  724083/1200000: episode: 3018, duration: 6.324s, episode steps: 192, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.698 [0.000, 3.000],  loss: 0.199345, mae: 7.574828, mean_q: -9.919272, mean_eps: 0.457010\n",
      "  724268/1200000: episode: 3019, duration: 5.436s, episode steps: 185, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.166569, mae: 7.486296, mean_q: -9.809192, mean_eps: 0.456869\n",
      "  724493/1200000: episode: 3020, duration: 5.974s, episode steps: 225, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.183015, mae: 7.533786, mean_q: -9.862068, mean_eps: 0.456715\n",
      "  724711/1200000: episode: 3021, duration: 5.474s, episode steps: 218, steps per second:  40, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.145748, mae: 7.476130, mean_q: -9.797620, mean_eps: 0.456549\n",
      "  724961/1200000: episode: 3022, duration: 6.224s, episode steps: 250, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.143252, mae: 7.497057, mean_q: -9.827939, mean_eps: 0.456373\n",
      "  725238/1200000: episode: 3023, duration: 6.695s, episode steps: 277, steps per second:  41, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.159891, mae: 7.553337, mean_q: -9.881953, mean_eps: 0.456176\n",
      "  725422/1200000: episode: 3024, duration: 4.561s, episode steps: 184, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.146002, mae: 7.483639, mean_q: -9.806005, mean_eps: 0.456003\n",
      "  725709/1200000: episode: 3025, duration: 7.067s, episode steps: 287, steps per second:  41, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.166052, mae: 7.492641, mean_q: -9.807552, mean_eps: 0.455826\n",
      "  725999/1200000: episode: 3026, duration: 7.351s, episode steps: 290, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.228 [0.000, 3.000],  loss: 0.154915, mae: 7.447631, mean_q: -9.752398, mean_eps: 0.455610\n",
      "  726185/1200000: episode: 3027, duration: 4.981s, episode steps: 186, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.715 [0.000, 3.000],  loss: 0.152130, mae: 7.538252, mean_q: -9.856516, mean_eps: 0.455431\n",
      "  726460/1200000: episode: 3028, duration: 9.847s, episode steps: 275, steps per second:  28, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.152793, mae: 7.505970, mean_q: -9.835414, mean_eps: 0.455259\n",
      "  726777/1200000: episode: 3029, duration: 10.005s, episode steps: 317, steps per second:  32, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.180310, mae: 7.496494, mean_q: -9.822034, mean_eps: 0.455037\n",
      "  727008/1200000: episode: 3030, duration: 6.636s, episode steps: 231, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.176342, mae: 7.507331, mean_q: -9.825723, mean_eps: 0.454831\n",
      "  727178/1200000: episode: 3031, duration: 5.019s, episode steps: 170, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.133944, mae: 7.461894, mean_q: -9.768635, mean_eps: 0.454681\n",
      "  727428/1200000: episode: 3032, duration: 6.784s, episode steps: 250, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.142929, mae: 7.566903, mean_q: -9.931452, mean_eps: 0.454523\n",
      "  727836/1200000: episode: 3033, duration: 10.515s, episode steps: 408, steps per second:  39, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.161032, mae: 7.473884, mean_q: -9.787866, mean_eps: 0.454276\n",
      "  728066/1200000: episode: 3034, duration: 5.954s, episode steps: 230, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.152109, mae: 7.456844, mean_q: -9.766112, mean_eps: 0.454037\n",
      "  728261/1200000: episode: 3035, duration: 5.639s, episode steps: 195, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.487 [0.000, 3.000],  loss: 0.135718, mae: 7.469032, mean_q: -9.788692, mean_eps: 0.453878\n",
      "  728556/1200000: episode: 3036, duration: 10.254s, episode steps: 295, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.143751, mae: 7.470666, mean_q: -9.790095, mean_eps: 0.453694\n",
      "  728719/1200000: episode: 3037, duration: 6.712s, episode steps: 163, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.199935, mae: 7.431538, mean_q: -9.729983, mean_eps: 0.453522\n",
      "  728977/1200000: episode: 3038, duration: 7.087s, episode steps: 258, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.155285, mae: 7.517846, mean_q: -9.850569, mean_eps: 0.453364\n",
      "  729166/1200000: episode: 3039, duration: 4.990s, episode steps: 189, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.154294, mae: 7.459326, mean_q: -9.779118, mean_eps: 0.453197\n",
      "  729340/1200000: episode: 3040, duration: 4.261s, episode steps: 174, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 0.160279, mae: 7.458126, mean_q: -9.767389, mean_eps: 0.453061\n",
      "  729525/1200000: episode: 3041, duration: 4.789s, episode steps: 185, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.849 [0.000, 3.000],  loss: 0.145918, mae: 7.465313, mean_q: -9.781294, mean_eps: 0.452926\n",
      "  729851/1200000: episode: 3042, duration: 8.046s, episode steps: 326, steps per second:  41, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.160696, mae: 7.486534, mean_q: -9.802722, mean_eps: 0.452734\n",
      "  730156/1200000: episode: 3043, duration: 7.536s, episode steps: 305, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.148085, mae: 7.452816, mean_q: -9.757809, mean_eps: 0.452498\n",
      "  730404/1200000: episode: 3044, duration: 6.124s, episode steps: 248, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 0.160181, mae: 7.369935, mean_q: -9.641809, mean_eps: 0.452290\n",
      "  730579/1200000: episode: 3045, duration: 4.297s, episode steps: 175, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.150504, mae: 7.370551, mean_q: -9.645547, mean_eps: 0.452132\n",
      "  730748/1200000: episode: 3046, duration: 4.418s, episode steps: 169, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.137724, mae: 7.445330, mean_q: -9.752870, mean_eps: 0.452003\n",
      "  730995/1200000: episode: 3047, duration: 6.350s, episode steps: 247, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.161907, mae: 7.363311, mean_q: -9.621752, mean_eps: 0.451847\n",
      "  731305/1200000: episode: 3048, duration: 7.654s, episode steps: 310, steps per second:  40, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.190688, mae: 7.382387, mean_q: -9.665843, mean_eps: 0.451638\n",
      "  731482/1200000: episode: 3049, duration: 4.422s, episode steps: 177, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.173184, mae: 7.387282, mean_q: -9.681217, mean_eps: 0.451455\n",
      "  731652/1200000: episode: 3050, duration: 4.232s, episode steps: 170, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.178431, mae: 7.376058, mean_q: -9.654221, mean_eps: 0.451325\n",
      "  731853/1200000: episode: 3051, duration: 4.990s, episode steps: 201, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 0.131400, mae: 7.352665, mean_q: -9.645859, mean_eps: 0.451186\n",
      "  732195/1200000: episode: 3052, duration: 8.286s, episode steps: 342, steps per second:  41, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.298 [0.000, 3.000],  loss: 0.149257, mae: 7.384898, mean_q: -9.688774, mean_eps: 0.450982\n",
      "  732379/1200000: episode: 3053, duration: 4.617s, episode steps: 184, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.685 [0.000, 3.000],  loss: 0.143397, mae: 7.379139, mean_q: -9.662220, mean_eps: 0.450785\n",
      "  732555/1200000: episode: 3054, duration: 4.385s, episode steps: 176, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.119 [0.000, 3.000],  loss: 0.169467, mae: 7.401925, mean_q: -9.668635, mean_eps: 0.450650\n",
      "  732771/1200000: episode: 3055, duration: 5.408s, episode steps: 216, steps per second:  40, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.143035, mae: 7.425780, mean_q: -9.726777, mean_eps: 0.450503\n",
      "  733144/1200000: episode: 3056, duration: 9.144s, episode steps: 373, steps per second:  41, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 0.142398, mae: 7.418690, mean_q: -9.710603, mean_eps: 0.450282\n",
      "  733314/1200000: episode: 3057, duration: 4.235s, episode steps: 170, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.122473, mae: 7.376986, mean_q: -9.659828, mean_eps: 0.450079\n",
      "  733502/1200000: episode: 3058, duration: 4.782s, episode steps: 188, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.204310, mae: 7.383332, mean_q: -9.662491, mean_eps: 0.449944\n",
      "  733674/1200000: episode: 3059, duration: 4.865s, episode steps: 172, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 0.184772, mae: 7.346802, mean_q: -9.608595, mean_eps: 0.449809\n",
      "  733853/1200000: episode: 3060, duration: 4.501s, episode steps: 179, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.160356, mae: 7.329642, mean_q: -9.585958, mean_eps: 0.449678\n",
      "  734058/1200000: episode: 3061, duration: 4.978s, episode steps: 205, steps per second:  41, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.141104, mae: 7.444822, mean_q: -9.761150, mean_eps: 0.449534\n",
      "  734234/1200000: episode: 3062, duration: 4.384s, episode steps: 176, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.147167, mae: 7.357693, mean_q: -9.626697, mean_eps: 0.449391\n",
      "  734397/1200000: episode: 3063, duration: 4.090s, episode steps: 163, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.188925, mae: 7.416046, mean_q: -9.709238, mean_eps: 0.449264\n",
      "  734610/1200000: episode: 3064, duration: 5.467s, episode steps: 213, steps per second:  39, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.164510, mae: 7.505283, mean_q: -9.831742, mean_eps: 0.449123\n",
      "  734858/1200000: episode: 3065, duration: 6.224s, episode steps: 248, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.186156, mae: 7.379644, mean_q: -9.657069, mean_eps: 0.448950\n",
      "  735048/1200000: episode: 3066, duration: 4.636s, episode steps: 190, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.199037, mae: 7.417586, mean_q: -9.687901, mean_eps: 0.448786\n",
      "  735218/1200000: episode: 3067, duration: 4.287s, episode steps: 170, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.147430, mae: 7.374774, mean_q: -9.663618, mean_eps: 0.448651\n",
      "  735517/1200000: episode: 3068, duration: 7.477s, episode steps: 299, steps per second:  40, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.288 [0.000, 3.000],  loss: 0.139793, mae: 7.338539, mean_q: -9.609057, mean_eps: 0.448475\n",
      "  735699/1200000: episode: 3069, duration: 5.017s, episode steps: 182, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.167723, mae: 7.390250, mean_q: -9.664929, mean_eps: 0.448294\n",
      "  735886/1200000: episode: 3070, duration: 4.631s, episode steps: 187, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 0.168261, mae: 7.377332, mean_q: -9.654967, mean_eps: 0.448156\n",
      "  736109/1200000: episode: 3071, duration: 8.332s, episode steps: 223, steps per second:  27, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.162156, mae: 7.359213, mean_q: -9.642922, mean_eps: 0.448002\n",
      "  736281/1200000: episode: 3072, duration: 5.387s, episode steps: 172, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: 0.162553, mae: 7.355718, mean_q: -9.628065, mean_eps: 0.447854\n",
      "  736448/1200000: episode: 3073, duration: 5.027s, episode steps: 167, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.281 [0.000, 3.000],  loss: 0.153397, mae: 7.401113, mean_q: -9.700991, mean_eps: 0.447727\n",
      "  736834/1200000: episode: 3074, duration: 10.179s, episode steps: 386, steps per second:  38, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: 0.203159, mae: 7.404345, mean_q: -9.682553, mean_eps: 0.447520\n",
      "  737088/1200000: episode: 3075, duration: 6.542s, episode steps: 254, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.164591, mae: 7.321227, mean_q: -9.588405, mean_eps: 0.447280\n",
      "  737270/1200000: episode: 3076, duration: 4.716s, episode steps: 182, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.173371, mae: 7.426829, mean_q: -9.716444, mean_eps: 0.447116\n",
      "  737528/1200000: episode: 3077, duration: 6.526s, episode steps: 258, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.129306, mae: 7.428750, mean_q: -9.740679, mean_eps: 0.446951\n",
      "  737788/1200000: episode: 3078, duration: 6.499s, episode steps: 260, steps per second:  40, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.190665, mae: 7.370209, mean_q: -9.645483, mean_eps: 0.446757\n",
      "  737955/1200000: episode: 3079, duration: 4.319s, episode steps: 167, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.365 [0.000, 3.000],  loss: 0.206716, mae: 7.421048, mean_q: -9.718144, mean_eps: 0.446597\n",
      "  738195/1200000: episode: 3080, duration: 6.239s, episode steps: 240, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.134137, mae: 7.381052, mean_q: -9.685051, mean_eps: 0.446444\n",
      "  738361/1200000: episode: 3081, duration: 4.486s, episode steps: 166, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 0.121425, mae: 7.384161, mean_q: -9.692280, mean_eps: 0.446292\n",
      "  738543/1200000: episode: 3082, duration: 4.803s, episode steps: 182, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.225 [0.000, 3.000],  loss: 0.150262, mae: 7.474417, mean_q: -9.801820, mean_eps: 0.446161\n",
      "  738757/1200000: episode: 3083, duration: 5.474s, episode steps: 214, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.181567, mae: 7.336131, mean_q: -9.590874, mean_eps: 0.446013\n",
      "  739051/1200000: episode: 3084, duration: 7.478s, episode steps: 294, steps per second:  39, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.158148, mae: 7.365066, mean_q: -9.632767, mean_eps: 0.445822\n",
      "  739286/1200000: episode: 3085, duration: 6.023s, episode steps: 235, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.161944, mae: 7.398736, mean_q: -9.688833, mean_eps: 0.445624\n",
      "  739586/1200000: episode: 3086, duration: 7.740s, episode steps: 300, steps per second:  39, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.141922, mae: 7.403321, mean_q: -9.707739, mean_eps: 0.445423\n",
      "  739816/1200000: episode: 3087, duration: 5.922s, episode steps: 230, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.145854, mae: 7.444583, mean_q: -9.760888, mean_eps: 0.445225\n",
      "  739976/1200000: episode: 3088, duration: 4.100s, episode steps: 160, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.161954, mae: 7.282475, mean_q: -9.533830, mean_eps: 0.445078\n",
      "  740172/1200000: episode: 3089, duration: 5.130s, episode steps: 196, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 0.151758, mae: 7.081229, mean_q: -9.274733, mean_eps: 0.444945\n",
      "  740342/1200000: episode: 3090, duration: 4.435s, episode steps: 170, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.182974, mae: 7.030980, mean_q: -9.179423, mean_eps: 0.444808\n",
      "  740655/1200000: episode: 3091, duration: 8.100s, episode steps: 313, steps per second:  39, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.119005, mae: 7.120848, mean_q: -9.343091, mean_eps: 0.444626\n",
      "  740839/1200000: episode: 3092, duration: 4.930s, episode steps: 184, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.130723, mae: 7.046200, mean_q: -9.221860, mean_eps: 0.444440\n",
      "  741014/1200000: episode: 3093, duration: 4.668s, episode steps: 175, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.126750, mae: 7.084887, mean_q: -9.265100, mean_eps: 0.444306\n",
      "  741196/1200000: episode: 3094, duration: 4.672s, episode steps: 182, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.313 [0.000, 3.000],  loss: 0.154413, mae: 7.182971, mean_q: -9.413741, mean_eps: 0.444172\n",
      "  741449/1200000: episode: 3095, duration: 6.504s, episode steps: 253, steps per second:  39, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.134735, mae: 7.110460, mean_q: -9.302424, mean_eps: 0.444008\n",
      "  741623/1200000: episode: 3096, duration: 4.437s, episode steps: 174, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 0.153872, mae: 7.183039, mean_q: -9.411216, mean_eps: 0.443848\n",
      "  741817/1200000: episode: 3097, duration: 5.564s, episode steps: 194, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.294 [0.000, 3.000],  loss: 0.119971, mae: 7.044391, mean_q: -9.234522, mean_eps: 0.443710\n",
      "  741996/1200000: episode: 3098, duration: 5.246s, episode steps: 179, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.142621, mae: 7.111693, mean_q: -9.321178, mean_eps: 0.443570\n",
      "  742205/1200000: episode: 3099, duration: 5.455s, episode steps: 209, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.129152, mae: 7.125034, mean_q: -9.314346, mean_eps: 0.443425\n",
      "  742442/1200000: episode: 3100, duration: 6.007s, episode steps: 237, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.183006, mae: 7.059656, mean_q: -9.228959, mean_eps: 0.443258\n",
      "  742612/1200000: episode: 3101, duration: 4.448s, episode steps: 170, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.157679, mae: 7.106116, mean_q: -9.301944, mean_eps: 0.443105\n",
      "  742771/1200000: episode: 3102, duration: 4.104s, episode steps: 159, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.428 [0.000, 3.000],  loss: 0.142248, mae: 7.035552, mean_q: -9.229693, mean_eps: 0.442982\n",
      "  742973/1200000: episode: 3103, duration: 5.224s, episode steps: 202, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.342 [0.000, 3.000],  loss: 0.160603, mae: 7.159065, mean_q: -9.373725, mean_eps: 0.442846\n",
      "  743304/1200000: episode: 3104, duration: 8.555s, episode steps: 331, steps per second:  39, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.195530, mae: 7.085967, mean_q: -9.259932, mean_eps: 0.442646\n",
      "  743614/1200000: episode: 3105, duration: 7.893s, episode steps: 310, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.147394, mae: 7.048771, mean_q: -9.237341, mean_eps: 0.442406\n",
      "  743985/1200000: episode: 3106, duration: 9.518s, episode steps: 371, steps per second:  39, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.149620, mae: 7.160620, mean_q: -9.380608, mean_eps: 0.442151\n",
      "  744174/1200000: episode: 3107, duration: 4.953s, episode steps: 189, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.153303, mae: 7.087479, mean_q: -9.276156, mean_eps: 0.441941\n",
      "  744371/1200000: episode: 3108, duration: 5.046s, episode steps: 197, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 0.151303, mae: 7.123656, mean_q: -9.326424, mean_eps: 0.441796\n",
      "  744565/1200000: episode: 3109, duration: 5.063s, episode steps: 194, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.158484, mae: 7.186672, mean_q: -9.416976, mean_eps: 0.441649\n",
      "  744810/1200000: episode: 3110, duration: 6.284s, episode steps: 245, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.138349, mae: 7.095756, mean_q: -9.289087, mean_eps: 0.441485\n",
      "  745181/1200000: episode: 3111, duration: 10.525s, episode steps: 371, steps per second:  35, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.146769, mae: 7.106970, mean_q: -9.312668, mean_eps: 0.441254\n",
      "  745390/1200000: episode: 3112, duration: 5.674s, episode steps: 209, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: 0.122064, mae: 7.155900, mean_q: -9.375255, mean_eps: 0.441036\n",
      "  745615/1200000: episode: 3113, duration: 6.228s, episode steps: 225, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.114827, mae: 7.114615, mean_q: -9.328563, mean_eps: 0.440874\n",
      "  745864/1200000: episode: 3114, duration: 6.776s, episode steps: 249, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.163446, mae: 7.136611, mean_q: -9.336522, mean_eps: 0.440696\n",
      "  746076/1200000: episode: 3115, duration: 5.562s, episode steps: 212, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.166856, mae: 7.159695, mean_q: -9.370689, mean_eps: 0.440523\n",
      "  746446/1200000: episode: 3116, duration: 9.308s, episode steps: 370, steps per second:  40, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.150656, mae: 7.088750, mean_q: -9.283558, mean_eps: 0.440305\n",
      "  746755/1200000: episode: 3117, duration: 7.850s, episode steps: 309, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.152680, mae: 7.132798, mean_q: -9.332524, mean_eps: 0.440050\n",
      "  747100/1200000: episode: 3118, duration: 8.848s, episode steps: 345, steps per second:  39, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.144035, mae: 7.081494, mean_q: -9.280771, mean_eps: 0.439805\n",
      "  747286/1200000: episode: 3119, duration: 4.849s, episode steps: 186, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: 0.126677, mae: 7.114084, mean_q: -9.330467, mean_eps: 0.439606\n",
      "  747477/1200000: episode: 3120, duration: 5.128s, episode steps: 191, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.168125, mae: 7.188247, mean_q: -9.422200, mean_eps: 0.439464\n",
      "  747649/1200000: episode: 3121, duration: 4.464s, episode steps: 172, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.133998, mae: 7.066283, mean_q: -9.249780, mean_eps: 0.439328\n",
      "  747912/1200000: episode: 3122, duration: 6.759s, episode steps: 263, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.157777, mae: 7.133204, mean_q: -9.327831, mean_eps: 0.439165\n",
      "  748153/1200000: episode: 3123, duration: 6.253s, episode steps: 241, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.170295, mae: 7.146554, mean_q: -9.362142, mean_eps: 0.438976\n",
      "  748498/1200000: episode: 3124, duration: 8.990s, episode steps: 345, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.154441, mae: 7.131028, mean_q: -9.348649, mean_eps: 0.438756\n",
      "  748701/1200000: episode: 3125, duration: 5.152s, episode steps: 203, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.116067, mae: 7.147053, mean_q: -9.369875, mean_eps: 0.438551\n",
      "  748884/1200000: episode: 3126, duration: 4.769s, episode steps: 183, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.612 [0.000, 3.000],  loss: 0.137252, mae: 7.109538, mean_q: -9.322728, mean_eps: 0.438406\n",
      "  749089/1200000: episode: 3127, duration: 5.208s, episode steps: 205, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.136133, mae: 7.072211, mean_q: -9.276146, mean_eps: 0.438260\n",
      "  749273/1200000: episode: 3128, duration: 4.862s, episode steps: 184, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.359 [0.000, 3.000],  loss: 0.156332, mae: 7.042902, mean_q: -9.224301, mean_eps: 0.438115\n",
      "  749593/1200000: episode: 3129, duration: 8.165s, episode steps: 320, steps per second:  39, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.151897, mae: 7.126257, mean_q: -9.334224, mean_eps: 0.437926\n",
      "  749782/1200000: episode: 3130, duration: 4.966s, episode steps: 189, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.149933, mae: 7.087954, mean_q: -9.283712, mean_eps: 0.437735\n",
      "  750049/1200000: episode: 3131, duration: 7.006s, episode steps: 267, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.168567, mae: 7.104986, mean_q: -9.284528, mean_eps: 0.437564\n",
      "  750220/1200000: episode: 3132, duration: 4.636s, episode steps: 171, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.138298, mae: 6.817735, mean_q: -8.927262, mean_eps: 0.437399\n",
      "  750400/1200000: episode: 3133, duration: 4.804s, episode steps: 180, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.186165, mae: 6.847538, mean_q: -8.950450, mean_eps: 0.437268\n",
      "  750605/1200000: episode: 3134, duration: 5.387s, episode steps: 205, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.190954, mae: 6.782003, mean_q: -8.844584, mean_eps: 0.437124\n",
      "  750786/1200000: episode: 3135, duration: 4.778s, episode steps: 181, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.159939, mae: 6.883482, mean_q: -9.004831, mean_eps: 0.436979\n",
      "  751173/1200000: episode: 3136, duration: 10.073s, episode steps: 387, steps per second:  38, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.142525, mae: 6.883597, mean_q: -9.000795, mean_eps: 0.436766\n",
      "  751348/1200000: episode: 3137, duration: 4.858s, episode steps: 175, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.093114, mae: 6.794043, mean_q: -8.904645, mean_eps: 0.436555\n",
      "  751544/1200000: episode: 3138, duration: 5.109s, episode steps: 196, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 0.129824, mae: 6.806636, mean_q: -8.915358, mean_eps: 0.436416\n",
      "  751722/1200000: episode: 3139, duration: 4.799s, episode steps: 178, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.142386, mae: 6.732923, mean_q: -8.799919, mean_eps: 0.436276\n",
      "  751979/1200000: episode: 3140, duration: 6.619s, episode steps: 257, steps per second:  39, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.142793, mae: 6.819263, mean_q: -8.922317, mean_eps: 0.436113\n",
      "  752155/1200000: episode: 3141, duration: 4.677s, episode steps: 176, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.144385, mae: 6.845237, mean_q: -8.959388, mean_eps: 0.435950\n",
      "  752335/1200000: episode: 3142, duration: 4.734s, episode steps: 180, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.617 [0.000, 3.000],  loss: 0.100776, mae: 6.774279, mean_q: -8.869487, mean_eps: 0.435817\n",
      "  752516/1200000: episode: 3143, duration: 4.769s, episode steps: 181, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: 0.105099, mae: 6.802493, mean_q: -8.916871, mean_eps: 0.435681\n",
      "  752806/1200000: episode: 3144, duration: 7.464s, episode steps: 290, steps per second:  39, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.123966, mae: 6.870725, mean_q: -8.986868, mean_eps: 0.435505\n",
      "  753226/1200000: episode: 3145, duration: 12.332s, episode steps: 420, steps per second:  34, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.155659, mae: 6.833763, mean_q: -8.929793, mean_eps: 0.435238\n",
      "  753456/1200000: episode: 3146, duration: 5.924s, episode steps: 230, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.156924, mae: 6.931310, mean_q: -9.074945, mean_eps: 0.434995\n",
      "  753699/1200000: episode: 3147, duration: 6.282s, episode steps: 243, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.121836, mae: 6.797748, mean_q: -8.891557, mean_eps: 0.434817\n",
      "  754007/1200000: episode: 3148, duration: 8.481s, episode steps: 308, steps per second:  36, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.108575, mae: 6.827917, mean_q: -8.935922, mean_eps: 0.434611\n",
      "  754251/1200000: episode: 3149, duration: 7.137s, episode steps: 244, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.151979, mae: 6.836102, mean_q: -8.939183, mean_eps: 0.434404\n",
      "  754464/1200000: episode: 3150, duration: 5.748s, episode steps: 213, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.131161, mae: 6.865397, mean_q: -8.976696, mean_eps: 0.434232\n",
      "  754656/1200000: episode: 3151, duration: 5.070s, episode steps: 192, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.354 [0.000, 3.000],  loss: 0.167693, mae: 6.797629, mean_q: -8.864042, mean_eps: 0.434080\n",
      "  754847/1200000: episode: 3152, duration: 5.105s, episode steps: 191, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.117296, mae: 6.837498, mean_q: -8.944929, mean_eps: 0.433937\n",
      "  755056/1200000: episode: 3153, duration: 5.453s, episode steps: 209, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.152561, mae: 6.833799, mean_q: -8.931584, mean_eps: 0.433787\n",
      "  755235/1200000: episode: 3154, duration: 4.882s, episode steps: 179, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.816 [0.000, 3.000],  loss: 0.116615, mae: 6.807951, mean_q: -8.896283, mean_eps: 0.433641\n",
      "  755430/1200000: episode: 3155, duration: 5.212s, episode steps: 195, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.164641, mae: 6.816744, mean_q: -8.904774, mean_eps: 0.433501\n",
      "  755637/1200000: episode: 3156, duration: 5.442s, episode steps: 207, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.122504, mae: 6.860700, mean_q: -8.973387, mean_eps: 0.433350\n",
      "  755872/1200000: episode: 3157, duration: 6.170s, episode steps: 235, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.306 [0.000, 3.000],  loss: 0.143772, mae: 6.874705, mean_q: -8.995597, mean_eps: 0.433185\n",
      "  756124/1200000: episode: 3158, duration: 6.753s, episode steps: 252, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.154427, mae: 6.894402, mean_q: -9.012933, mean_eps: 0.433002\n",
      "  756289/1200000: episode: 3159, duration: 4.339s, episode steps: 165, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.148842, mae: 6.754153, mean_q: -8.825133, mean_eps: 0.432845\n",
      "  756494/1200000: episode: 3160, duration: 5.589s, episode steps: 205, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.234 [0.000, 3.000],  loss: 0.134676, mae: 6.762279, mean_q: -8.847562, mean_eps: 0.432707\n",
      "  756682/1200000: episode: 3161, duration: 5.084s, episode steps: 188, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.809 [0.000, 3.000],  loss: 0.163199, mae: 6.821915, mean_q: -8.905292, mean_eps: 0.432559\n",
      "  756960/1200000: episode: 3162, duration: 7.050s, episode steps: 278, steps per second:  39, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.135992, mae: 6.824113, mean_q: -8.920833, mean_eps: 0.432385\n",
      "  757131/1200000: episode: 3163, duration: 4.524s, episode steps: 171, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.209826, mae: 6.836909, mean_q: -8.935181, mean_eps: 0.432216\n",
      "  757385/1200000: episode: 3164, duration: 6.733s, episode steps: 254, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.131954, mae: 6.822021, mean_q: -8.935132, mean_eps: 0.432057\n",
      "  757559/1200000: episode: 3165, duration: 4.632s, episode steps: 174, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.626 [0.000, 3.000],  loss: 0.214769, mae: 6.840781, mean_q: -8.927226, mean_eps: 0.431896\n",
      "  757830/1200000: episode: 3166, duration: 7.247s, episode steps: 271, steps per second:  37, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.138027, mae: 6.866528, mean_q: -8.981667, mean_eps: 0.431729\n",
      "  758123/1200000: episode: 3167, duration: 8.194s, episode steps: 293, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.163999, mae: 6.805352, mean_q: -8.901001, mean_eps: 0.431518\n",
      "  758368/1200000: episode: 3168, duration: 6.509s, episode steps: 245, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.124319, mae: 6.821594, mean_q: -8.915482, mean_eps: 0.431316\n",
      "  758565/1200000: episode: 3169, duration: 5.220s, episode steps: 197, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 0.134189, mae: 6.839243, mean_q: -8.945617, mean_eps: 0.431151\n",
      "  758759/1200000: episode: 3170, duration: 5.123s, episode steps: 194, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.138964, mae: 6.933461, mean_q: -9.083557, mean_eps: 0.431004\n",
      "  759070/1200000: episode: 3171, duration: 8.278s, episode steps: 311, steps per second:  38, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.137700, mae: 6.846445, mean_q: -8.962568, mean_eps: 0.430814\n",
      "  759303/1200000: episode: 3172, duration: 6.142s, episode steps: 233, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.137417, mae: 6.792665, mean_q: -8.888213, mean_eps: 0.430611\n",
      "  759528/1200000: episode: 3173, duration: 5.985s, episode steps: 225, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.100598, mae: 6.808237, mean_q: -8.911838, mean_eps: 0.430439\n",
      "  759706/1200000: episode: 3174, duration: 4.818s, episode steps: 178, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.135660, mae: 6.857648, mean_q: -8.962244, mean_eps: 0.430288\n",
      "  759888/1200000: episode: 3175, duration: 4.899s, episode steps: 182, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.139431, mae: 6.837636, mean_q: -8.927148, mean_eps: 0.430153\n",
      "  760104/1200000: episode: 3176, duration: 5.822s, episode steps: 216, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 0.140855, mae: 6.699123, mean_q: -8.757159, mean_eps: 0.430003\n",
      "  760298/1200000: episode: 3177, duration: 5.336s, episode steps: 194, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 0.111262, mae: 6.786928, mean_q: -8.888937, mean_eps: 0.429850\n",
      "  760488/1200000: episode: 3178, duration: 5.182s, episode steps: 190, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.164051, mae: 6.798731, mean_q: -8.889519, mean_eps: 0.429706\n",
      "  760658/1200000: episode: 3179, duration: 4.558s, episode steps: 170, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.147735, mae: 6.744354, mean_q: -8.821023, mean_eps: 0.429571\n",
      "  760919/1200000: episode: 3180, duration: 7.766s, episode steps: 261, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.130835, mae: 6.806347, mean_q: -8.901243, mean_eps: 0.429409\n",
      "  761220/1200000: episode: 3181, duration: 7.968s, episode steps: 301, steps per second:  38, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.362 [0.000, 3.000],  loss: 0.106630, mae: 6.786214, mean_q: -8.890220, mean_eps: 0.429198\n",
      "  761507/1200000: episode: 3182, duration: 7.664s, episode steps: 287, steps per second:  37, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.164846, mae: 6.693427, mean_q: -8.754309, mean_eps: 0.428978\n",
      "  761874/1200000: episode: 3183, duration: 9.683s, episode steps: 367, steps per second:  38, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: 0.119619, mae: 6.782017, mean_q: -8.874826, mean_eps: 0.428733\n",
      "  762200/1200000: episode: 3184, duration: 8.692s, episode steps: 326, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.148447, mae: 6.725638, mean_q: -8.793683, mean_eps: 0.428473\n",
      "  762536/1200000: episode: 3185, duration: 8.829s, episode steps: 336, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.208 [0.000, 3.000],  loss: 0.130632, mae: 6.791630, mean_q: -8.879209, mean_eps: 0.428224\n",
      "  762716/1200000: episode: 3186, duration: 4.917s, episode steps: 180, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.145632, mae: 6.712521, mean_q: -8.790326, mean_eps: 0.428031\n",
      "  762950/1200000: episode: 3187, duration: 6.889s, episode steps: 234, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.122459, mae: 6.762741, mean_q: -8.849478, mean_eps: 0.427876\n",
      "  763116/1200000: episode: 3188, duration: 6.452s, episode steps: 166, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.191799, mae: 6.803376, mean_q: -8.889098, mean_eps: 0.427726\n",
      "  763286/1200000: episode: 3189, duration: 4.950s, episode steps: 170, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.241 [0.000, 3.000],  loss: 0.103165, mae: 6.726558, mean_q: -8.799223, mean_eps: 0.427600\n",
      "  763477/1200000: episode: 3190, duration: 5.158s, episode steps: 191, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.140022, mae: 6.742233, mean_q: -8.814004, mean_eps: 0.427464\n",
      "  763648/1200000: episode: 3191, duration: 4.554s, episode steps: 171, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.155711, mae: 6.749125, mean_q: -8.826397, mean_eps: 0.427329\n",
      "  763908/1200000: episode: 3192, duration: 6.824s, episode steps: 260, steps per second:  38, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.127500, mae: 6.716735, mean_q: -8.789613, mean_eps: 0.427167\n",
      "  764092/1200000: episode: 3193, duration: 4.880s, episode steps: 184, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.148057, mae: 6.748577, mean_q: -8.822100, mean_eps: 0.427000\n",
      "  764294/1200000: episode: 3194, duration: 5.301s, episode steps: 202, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.127528, mae: 6.653703, mean_q: -8.703177, mean_eps: 0.426856\n",
      "  764577/1200000: episode: 3195, duration: 7.236s, episode steps: 283, steps per second:  39, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.147609, mae: 6.708679, mean_q: -8.766853, mean_eps: 0.426674\n",
      "  764762/1200000: episode: 3196, duration: 5.176s, episode steps: 185, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.120425, mae: 6.762938, mean_q: -8.847961, mean_eps: 0.426498\n",
      "  765018/1200000: episode: 3197, duration: 6.814s, episode steps: 256, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.122018, mae: 6.725733, mean_q: -8.803488, mean_eps: 0.426333\n",
      "  765348/1200000: episode: 3198, duration: 8.549s, episode steps: 330, steps per second:  39, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.147472, mae: 6.747179, mean_q: -8.831386, mean_eps: 0.426113\n",
      "  765544/1200000: episode: 3199, duration: 5.317s, episode steps: 196, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: 0.128498, mae: 6.705659, mean_q: -8.784966, mean_eps: 0.425916\n",
      "  765737/1200000: episode: 3200, duration: 5.125s, episode steps: 193, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.116642, mae: 6.841028, mean_q: -8.965525, mean_eps: 0.425770\n",
      "  765923/1200000: episode: 3201, duration: 4.919s, episode steps: 186, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.495 [0.000, 3.000],  loss: 0.122423, mae: 6.755597, mean_q: -8.850539, mean_eps: 0.425628\n",
      "  766159/1200000: episode: 3202, duration: 6.250s, episode steps: 236, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.169785, mae: 6.705113, mean_q: -8.773122, mean_eps: 0.425470\n",
      "  766349/1200000: episode: 3203, duration: 4.962s, episode steps: 190, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 0.135284, mae: 6.729735, mean_q: -8.795571, mean_eps: 0.425310\n",
      "  766509/1200000: episode: 3204, duration: 4.191s, episode steps: 160, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.319 [0.000, 3.000],  loss: 0.164295, mae: 6.779023, mean_q: -8.853720, mean_eps: 0.425179\n",
      "  766833/1200000: episode: 3205, duration: 8.580s, episode steps: 324, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.125119, mae: 6.754445, mean_q: -8.838538, mean_eps: 0.424997\n",
      "  767052/1200000: episode: 3206, duration: 5.838s, episode steps: 219, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.717 [0.000, 3.000],  loss: 0.137519, mae: 6.729560, mean_q: -8.806659, mean_eps: 0.424793\n",
      "  767332/1200000: episode: 3207, duration: 7.466s, episode steps: 280, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.121759, mae: 6.769606, mean_q: -8.873133, mean_eps: 0.424606\n",
      "  767559/1200000: episode: 3208, duration: 6.118s, episode steps: 227, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.183021, mae: 6.762205, mean_q: -8.832542, mean_eps: 0.424416\n",
      "  767957/1200000: episode: 3209, duration: 10.708s, episode steps: 398, steps per second:  37, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: 0.147964, mae: 6.794990, mean_q: -8.880792, mean_eps: 0.424182\n",
      "  768218/1200000: episode: 3210, duration: 6.822s, episode steps: 261, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.117219, mae: 6.739223, mean_q: -8.830008, mean_eps: 0.423935\n",
      "  768388/1200000: episode: 3211, duration: 4.500s, episode steps: 170, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.641 [0.000, 3.000],  loss: 0.149921, mae: 6.733855, mean_q: -8.796823, mean_eps: 0.423773\n",
      "  768710/1200000: episode: 3212, duration: 9.245s, episode steps: 322, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.141952, mae: 6.757782, mean_q: -8.842901, mean_eps: 0.423589\n",
      "  768892/1200000: episode: 3213, duration: 5.229s, episode steps: 182, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.148300, mae: 6.817496, mean_q: -8.924784, mean_eps: 0.423400\n",
      "  769062/1200000: episode: 3214, duration: 4.718s, episode steps: 170, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.365 [0.000, 3.000],  loss: 0.121523, mae: 6.763061, mean_q: -8.861478, mean_eps: 0.423268\n",
      "  769235/1200000: episode: 3215, duration: 4.569s, episode steps: 173, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.665 [0.000, 3.000],  loss: 0.174600, mae: 6.785959, mean_q: -8.856888, mean_eps: 0.423139\n",
      "  769409/1200000: episode: 3216, duration: 4.726s, episode steps: 174, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 0.127838, mae: 6.832525, mean_q: -8.956045, mean_eps: 0.423009\n",
      "  769576/1200000: episode: 3217, duration: 4.630s, episode steps: 167, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.178708, mae: 6.644373, mean_q: -8.672374, mean_eps: 0.422881\n",
      "  769947/1200000: episode: 3218, duration: 9.953s, episode steps: 371, steps per second:  37, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.135114, mae: 6.775805, mean_q: -8.877372, mean_eps: 0.422679\n",
      "  770260/1200000: episode: 3219, duration: 8.200s, episode steps: 313, steps per second:  38, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.316 [0.000, 3.000],  loss: 0.108259, mae: 6.619919, mean_q: -8.673133, mean_eps: 0.422423\n",
      "  770439/1200000: episode: 3220, duration: 4.864s, episode steps: 179, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.132307, mae: 6.647134, mean_q: -8.695814, mean_eps: 0.422238\n",
      "  770764/1200000: episode: 3221, duration: 8.495s, episode steps: 325, steps per second:  38, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.128818, mae: 6.599069, mean_q: -8.628695, mean_eps: 0.422049\n",
      "  770990/1200000: episode: 3222, duration: 5.978s, episode steps: 226, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.163244, mae: 6.595716, mean_q: -8.599835, mean_eps: 0.421843\n",
      "  771196/1200000: episode: 3223, duration: 5.534s, episode steps: 206, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.131446, mae: 6.575891, mean_q: -8.603902, mean_eps: 0.421681\n",
      "  771400/1200000: episode: 3224, duration: 5.428s, episode steps: 204, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 0.113401, mae: 6.552147, mean_q: -8.570066, mean_eps: 0.421527\n",
      "  771564/1200000: episode: 3225, duration: 4.400s, episode steps: 164, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.144149, mae: 6.580088, mean_q: -8.598384, mean_eps: 0.421389\n",
      "  771826/1200000: episode: 3226, duration: 7.349s, episode steps: 262, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.147088, mae: 6.516918, mean_q: -8.517166, mean_eps: 0.421229\n",
      "  772073/1200000: episode: 3227, duration: 7.403s, episode steps: 247, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.145667, mae: 6.504387, mean_q: -8.500952, mean_eps: 0.421038\n",
      "  772252/1200000: episode: 3228, duration: 5.533s, episode steps: 179, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.251 [0.000, 3.000],  loss: 0.133131, mae: 6.547764, mean_q: -8.568527, mean_eps: 0.420879\n",
      "  772460/1200000: episode: 3229, duration: 5.986s, episode steps: 208, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.159599, mae: 6.593021, mean_q: -8.607975, mean_eps: 0.420733\n",
      "  772779/1200000: episode: 3230, duration: 11.204s, episode steps: 319, steps per second:  28, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.133584, mae: 6.567564, mean_q: -8.578092, mean_eps: 0.420536\n",
      "  773070/1200000: episode: 3231, duration: 8.606s, episode steps: 291, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.119803, mae: 6.653456, mean_q: -8.704968, mean_eps: 0.420307\n",
      "  773270/1200000: episode: 3232, duration: 6.118s, episode steps: 200, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.125090, mae: 6.611134, mean_q: -8.631490, mean_eps: 0.420123\n",
      "  773440/1200000: episode: 3233, duration: 4.900s, episode steps: 170, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.694 [0.000, 3.000],  loss: 0.132632, mae: 6.592205, mean_q: -8.613355, mean_eps: 0.419984\n",
      "  773729/1200000: episode: 3234, duration: 8.122s, episode steps: 289, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.154164, mae: 6.601843, mean_q: -8.607646, mean_eps: 0.419812\n",
      "  774010/1200000: episode: 3235, duration: 8.754s, episode steps: 281, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.143168, mae: 6.565409, mean_q: -8.571979, mean_eps: 0.419598\n",
      "  774185/1200000: episode: 3236, duration: 5.759s, episode steps: 175, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.097503, mae: 6.555320, mean_q: -8.585150, mean_eps: 0.419427\n",
      "  774366/1200000: episode: 3237, duration: 6.521s, episode steps: 181, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.147733, mae: 6.626251, mean_q: -8.667138, mean_eps: 0.419294\n",
      "  774826/1200000: episode: 3238, duration: 15.233s, episode steps: 460, steps per second:  30, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.124553, mae: 6.551624, mean_q: -8.568308, mean_eps: 0.419053\n",
      "  775131/1200000: episode: 3239, duration: 8.251s, episode steps: 305, steps per second:  37, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.121485, mae: 6.555931, mean_q: -8.589724, mean_eps: 0.418766\n",
      "  775493/1200000: episode: 3240, duration: 12.197s, episode steps: 362, steps per second:  30, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.108127, mae: 6.580861, mean_q: -8.617492, mean_eps: 0.418516\n",
      "  775763/1200000: episode: 3241, duration: 9.698s, episode steps: 270, steps per second:  28, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.145457, mae: 6.577077, mean_q: -8.599697, mean_eps: 0.418279\n",
      "  775924/1200000: episode: 3242, duration: 5.379s, episode steps: 161, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.137421, mae: 6.533005, mean_q: -8.543781, mean_eps: 0.418118\n",
      "  776098/1200000: episode: 3243, duration: 7.140s, episode steps: 174, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.160693, mae: 6.618276, mean_q: -8.659967, mean_eps: 0.417992\n",
      "  776276/1200000: episode: 3244, duration: 6.195s, episode steps: 178, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.099830, mae: 6.637517, mean_q: -8.703280, mean_eps: 0.417860\n",
      "  776615/1200000: episode: 3245, duration: 9.620s, episode steps: 339, steps per second:  35, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.128009, mae: 6.610421, mean_q: -8.625540, mean_eps: 0.417666\n",
      "  776813/1200000: episode: 3246, duration: 5.532s, episode steps: 198, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.646 [0.000, 3.000],  loss: 0.135115, mae: 6.558128, mean_q: -8.561255, mean_eps: 0.417465\n",
      "  776970/1200000: episode: 3247, duration: 4.394s, episode steps: 157, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.160590, mae: 6.566671, mean_q: -8.569921, mean_eps: 0.417332\n",
      "  777182/1200000: episode: 3248, duration: 5.721s, episode steps: 212, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.344 [0.000, 3.000],  loss: 0.125795, mae: 6.662578, mean_q: -8.711257, mean_eps: 0.417193\n",
      "  777464/1200000: episode: 3249, duration: 7.678s, episode steps: 282, steps per second:  37, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.155617, mae: 6.568534, mean_q: -8.587911, mean_eps: 0.417008\n",
      "  777680/1200000: episode: 3250, duration: 6.015s, episode steps: 216, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.319 [0.000, 3.000],  loss: 0.138001, mae: 6.536499, mean_q: -8.546111, mean_eps: 0.416821\n",
      "  777929/1200000: episode: 3251, duration: 6.865s, episode steps: 249, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.151000, mae: 6.572862, mean_q: -8.593347, mean_eps: 0.416647\n",
      "  778163/1200000: episode: 3252, duration: 6.392s, episode steps: 234, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.125570, mae: 6.611485, mean_q: -8.660486, mean_eps: 0.416466\n",
      "  778368/1200000: episode: 3253, duration: 5.851s, episode steps: 205, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 0.127582, mae: 6.600293, mean_q: -8.628939, mean_eps: 0.416301\n",
      "  778540/1200000: episode: 3254, duration: 4.709s, episode steps: 172, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 0.163158, mae: 6.577960, mean_q: -8.603151, mean_eps: 0.416160\n",
      "  778865/1200000: episode: 3255, duration: 8.783s, episode steps: 325, steps per second:  37, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.138563, mae: 6.601466, mean_q: -8.631357, mean_eps: 0.415973\n",
      "  779081/1200000: episode: 3256, duration: 6.068s, episode steps: 216, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.150836, mae: 6.527074, mean_q: -8.519215, mean_eps: 0.415771\n",
      "  779369/1200000: episode: 3257, duration: 8.937s, episode steps: 288, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.148515, mae: 6.543391, mean_q: -8.550066, mean_eps: 0.415582\n",
      "  779626/1200000: episode: 3258, duration: 7.396s, episode steps: 257, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.125419, mae: 6.608574, mean_q: -8.644072, mean_eps: 0.415377\n",
      "  779915/1200000: episode: 3259, duration: 8.028s, episode steps: 289, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.140620, mae: 6.601026, mean_q: -8.632617, mean_eps: 0.415172\n",
      "  780106/1200000: episode: 3260, duration: 5.846s, episode steps: 191, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.283 [0.000, 3.000],  loss: 0.129502, mae: 6.555829, mean_q: -8.575548, mean_eps: 0.414992\n",
      "  780356/1200000: episode: 3261, duration: 6.942s, episode steps: 250, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.155780, mae: 6.434545, mean_q: -8.416392, mean_eps: 0.414827\n",
      "  780560/1200000: episode: 3262, duration: 5.828s, episode steps: 204, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.106708, mae: 6.420829, mean_q: -8.391707, mean_eps: 0.414657\n",
      "  780842/1200000: episode: 3263, duration: 8.445s, episode steps: 282, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.148999, mae: 6.394166, mean_q: -8.349889, mean_eps: 0.414475\n",
      "  781049/1200000: episode: 3264, duration: 5.976s, episode steps: 207, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.112159, mae: 6.476961, mean_q: -8.475206, mean_eps: 0.414291\n",
      "  781290/1200000: episode: 3265, duration: 6.673s, episode steps: 241, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.173666, mae: 6.448434, mean_q: -8.399812, mean_eps: 0.414123\n",
      "  781597/1200000: episode: 3266, duration: 8.455s, episode steps: 307, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.147782, mae: 6.441856, mean_q: -8.420495, mean_eps: 0.413918\n",
      "  781919/1200000: episode: 3267, duration: 8.968s, episode steps: 322, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.119967, mae: 6.420307, mean_q: -8.400853, mean_eps: 0.413682\n",
      "  782256/1200000: episode: 3268, duration: 9.561s, episode steps: 337, steps per second:  35, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.122271, mae: 6.467040, mean_q: -8.450055, mean_eps: 0.413435\n",
      "  782456/1200000: episode: 3269, duration: 5.623s, episode steps: 200, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.380 [0.000, 3.000],  loss: 0.155595, mae: 6.436793, mean_q: -8.404545, mean_eps: 0.413233\n",
      "  782630/1200000: episode: 3270, duration: 4.879s, episode steps: 174, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.787 [0.000, 3.000],  loss: 0.140837, mae: 6.513822, mean_q: -8.510775, mean_eps: 0.413093\n",
      "  782822/1200000: episode: 3271, duration: 5.409s, episode steps: 192, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.443 [0.000, 3.000],  loss: 0.135736, mae: 6.421295, mean_q: -8.390797, mean_eps: 0.412956\n",
      "  783133/1200000: episode: 3272, duration: 8.587s, episode steps: 311, steps per second:  36, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.149342, mae: 6.443194, mean_q: -8.416504, mean_eps: 0.412767\n",
      "  783325/1200000: episode: 3273, duration: 5.523s, episode steps: 192, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.185141, mae: 6.523322, mean_q: -8.509811, mean_eps: 0.412579\n",
      "  783634/1200000: episode: 3274, duration: 8.629s, episode steps: 309, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.154408, mae: 6.509320, mean_q: -8.511968, mean_eps: 0.412391\n",
      "  783815/1200000: episode: 3275, duration: 4.997s, episode steps: 181, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.155333, mae: 6.469350, mean_q: -8.459980, mean_eps: 0.412207\n",
      "  784044/1200000: episode: 3276, duration: 6.415s, episode steps: 229, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.118976, mae: 6.450164, mean_q: -8.436531, mean_eps: 0.412053\n",
      "  784250/1200000: episode: 3277, duration: 5.727s, episode steps: 206, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.116168, mae: 6.514302, mean_q: -8.531694, mean_eps: 0.411890\n",
      "  784575/1200000: episode: 3278, duration: 9.073s, episode steps: 325, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.138447, mae: 6.507520, mean_q: -8.498594, mean_eps: 0.411691\n",
      "  784833/1200000: episode: 3279, duration: 7.210s, episode steps: 258, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.864 [0.000, 3.000],  loss: 0.120405, mae: 6.506884, mean_q: -8.500427, mean_eps: 0.411472\n",
      "  785053/1200000: episode: 3280, duration: 6.159s, episode steps: 220, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.138491, mae: 6.505749, mean_q: -8.487937, mean_eps: 0.411293\n",
      "  785283/1200000: episode: 3281, duration: 6.499s, episode steps: 230, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.143711, mae: 6.462083, mean_q: -8.445939, mean_eps: 0.411124\n",
      "  785472/1200000: episode: 3282, duration: 5.319s, episode steps: 189, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.265 [0.000, 3.000],  loss: 0.127704, mae: 6.460722, mean_q: -8.432764, mean_eps: 0.410967\n",
      "  785652/1200000: episode: 3283, duration: 5.085s, episode steps: 180, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.131397, mae: 6.510496, mean_q: -8.504050, mean_eps: 0.410829\n",
      "  786120/1200000: episode: 3284, duration: 12.885s, episode steps: 468, steps per second:  36, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.145832, mae: 6.536058, mean_q: -8.553516, mean_eps: 0.410586\n",
      "  786415/1200000: episode: 3285, duration: 8.093s, episode steps: 295, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.203 [0.000, 3.000],  loss: 0.166865, mae: 6.497443, mean_q: -8.495911, mean_eps: 0.410300\n",
      "  786614/1200000: episode: 3286, duration: 5.773s, episode steps: 199, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.157654, mae: 6.440676, mean_q: -8.400575, mean_eps: 0.410114\n",
      "  787013/1200000: episode: 3287, duration: 10.963s, episode steps: 399, steps per second:  36, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.134211, mae: 6.423998, mean_q: -8.390006, mean_eps: 0.409890\n",
      "  787204/1200000: episode: 3288, duration: 5.262s, episode steps: 191, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.796 [0.000, 3.000],  loss: 0.142053, mae: 6.442498, mean_q: -8.409990, mean_eps: 0.409669\n",
      "  787388/1200000: episode: 3289, duration: 5.236s, episode steps: 184, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.185547, mae: 6.456058, mean_q: -8.395457, mean_eps: 0.409528\n",
      "  787593/1200000: episode: 3290, duration: 5.905s, episode steps: 205, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.693 [0.000, 3.000],  loss: 0.114656, mae: 6.496505, mean_q: -8.498473, mean_eps: 0.409382\n",
      "  787773/1200000: episode: 3291, duration: 5.217s, episode steps: 180, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.113060, mae: 6.481135, mean_q: -8.470514, mean_eps: 0.409238\n",
      "  788114/1200000: episode: 3292, duration: 9.530s, episode steps: 341, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.147686, mae: 6.488659, mean_q: -8.477473, mean_eps: 0.409043\n",
      "  788453/1200000: episode: 3293, duration: 9.495s, episode steps: 339, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 0.099531, mae: 6.517430, mean_q: -8.525640, mean_eps: 0.408788\n",
      "  788639/1200000: episode: 3294, duration: 5.187s, episode steps: 186, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.127339, mae: 6.421571, mean_q: -8.383783, mean_eps: 0.408591\n",
      "  788829/1200000: episode: 3295, duration: 6.004s, episode steps: 190, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.347 [0.000, 3.000],  loss: 0.126551, mae: 6.513668, mean_q: -8.496786, mean_eps: 0.408450\n",
      "  789041/1200000: episode: 3296, duration: 6.190s, episode steps: 212, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.166766, mae: 6.449406, mean_q: -8.414573, mean_eps: 0.408299\n",
      "  789346/1200000: episode: 3297, duration: 12.388s, episode steps: 305, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.119707, mae: 6.465368, mean_q: -8.446163, mean_eps: 0.408105\n",
      "  789572/1200000: episode: 3298, duration: 6.783s, episode steps: 226, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.130592, mae: 6.456813, mean_q: -8.430561, mean_eps: 0.407906\n",
      "  789767/1200000: episode: 3299, duration: 5.789s, episode steps: 195, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.134104, mae: 6.528434, mean_q: -8.528390, mean_eps: 0.407748\n",
      "  789957/1200000: episode: 3300, duration: 5.528s, episode steps: 190, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.705 [0.000, 3.000],  loss: 0.126659, mae: 6.442824, mean_q: -8.422265, mean_eps: 0.407604\n",
      "  790155/1200000: episode: 3301, duration: 5.564s, episode steps: 198, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.359 [0.000, 3.000],  loss: 0.129150, mae: 6.404445, mean_q: -8.368051, mean_eps: 0.407458\n",
      "  790361/1200000: episode: 3302, duration: 5.918s, episode steps: 206, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.129630, mae: 6.410301, mean_q: -8.375420, mean_eps: 0.407307\n",
      "  790553/1200000: episode: 3303, duration: 5.636s, episode steps: 192, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.116955, mae: 6.458787, mean_q: -8.449530, mean_eps: 0.407158\n",
      "  790790/1200000: episode: 3304, duration: 6.899s, episode steps: 237, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.129218, mae: 6.459985, mean_q: -8.439364, mean_eps: 0.406997\n",
      "  791018/1200000: episode: 3305, duration: 6.407s, episode steps: 228, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.125194, mae: 6.439587, mean_q: -8.416753, mean_eps: 0.406822\n",
      "  791378/1200000: episode: 3306, duration: 10.991s, episode steps: 360, steps per second:  33, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.124227, mae: 6.438211, mean_q: -8.437331, mean_eps: 0.406602\n",
      "  791610/1200000: episode: 3307, duration: 7.825s, episode steps: 232, steps per second:  30, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.088779, mae: 6.431880, mean_q: -8.424088, mean_eps: 0.406380\n",
      "  791839/1200000: episode: 3308, duration: 7.059s, episode steps: 229, steps per second:  32, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.149052, mae: 6.408440, mean_q: -8.354844, mean_eps: 0.406207\n",
      "  792042/1200000: episode: 3309, duration: 6.162s, episode steps: 203, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.340 [0.000, 3.000],  loss: 0.134641, mae: 6.453774, mean_q: -8.434431, mean_eps: 0.406045\n",
      "  792385/1200000: episode: 3310, duration: 10.078s, episode steps: 343, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.138603, mae: 6.447616, mean_q: -8.425383, mean_eps: 0.405840\n",
      "  792559/1200000: episode: 3311, duration: 5.561s, episode steps: 174, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.108263, mae: 6.400904, mean_q: -8.369993, mean_eps: 0.405646\n",
      "  792783/1200000: episode: 3312, duration: 7.745s, episode steps: 224, steps per second:  29, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.123840, mae: 6.452054, mean_q: -8.442329, mean_eps: 0.405497\n",
      "  792957/1200000: episode: 3313, duration: 5.329s, episode steps: 174, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.264 [0.000, 3.000],  loss: 0.143270, mae: 6.481335, mean_q: -8.472277, mean_eps: 0.405348\n",
      "  793243/1200000: episode: 3314, duration: 9.426s, episode steps: 286, steps per second:  30, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.137133, mae: 6.370505, mean_q: -8.335988, mean_eps: 0.405175\n",
      "  793477/1200000: episode: 3315, duration: 7.074s, episode steps: 234, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.143803, mae: 6.433308, mean_q: -8.399778, mean_eps: 0.404980\n",
      "  793811/1200000: episode: 3316, duration: 10.065s, episode steps: 334, steps per second:  33, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.129854, mae: 6.408960, mean_q: -8.368761, mean_eps: 0.404767\n",
      "  794030/1200000: episode: 3317, duration: 6.743s, episode steps: 219, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 0.129189, mae: 6.419448, mean_q: -8.390807, mean_eps: 0.404560\n",
      "  794207/1200000: episode: 3318, duration: 5.435s, episode steps: 177, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.746 [0.000, 3.000],  loss: 0.136852, mae: 6.394329, mean_q: -8.363410, mean_eps: 0.404412\n",
      "  794388/1200000: episode: 3319, duration: 5.737s, episode steps: 181, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.624 [0.000, 3.000],  loss: 0.151147, mae: 6.414569, mean_q: -8.376265, mean_eps: 0.404277\n",
      "  794564/1200000: episode: 3320, duration: 7.046s, episode steps: 176, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.171756, mae: 6.377447, mean_q: -8.312833, mean_eps: 0.404143\n",
      "  794797/1200000: episode: 3321, duration: 7.639s, episode steps: 233, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.123700, mae: 6.413880, mean_q: -8.376038, mean_eps: 0.403990\n",
      "  795069/1200000: episode: 3322, duration: 8.573s, episode steps: 272, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.128810, mae: 6.442023, mean_q: -8.427324, mean_eps: 0.403801\n",
      "  795256/1200000: episode: 3323, duration: 5.697s, episode steps: 187, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.513 [0.000, 3.000],  loss: 0.085290, mae: 6.414442, mean_q: -8.398923, mean_eps: 0.403629\n",
      "  795535/1200000: episode: 3324, duration: 8.279s, episode steps: 279, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.155846, mae: 6.450022, mean_q: -8.416374, mean_eps: 0.403454\n",
      "  795720/1200000: episode: 3325, duration: 5.708s, episode steps: 185, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.757 [0.000, 3.000],  loss: 0.087294, mae: 6.323529, mean_q: -8.274027, mean_eps: 0.403280\n",
      "  796038/1200000: episode: 3326, duration: 10.201s, episode steps: 318, steps per second:  31, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.136045, mae: 6.413729, mean_q: -8.383254, mean_eps: 0.403091\n",
      "  796408/1200000: episode: 3327, duration: 12.354s, episode steps: 370, steps per second:  30, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.135178, mae: 6.403286, mean_q: -8.365323, mean_eps: 0.402833\n",
      "  796763/1200000: episode: 3328, duration: 10.679s, episode steps: 355, steps per second:  33, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.126557, mae: 6.437608, mean_q: -8.421343, mean_eps: 0.402561\n",
      "  796969/1200000: episode: 3329, duration: 5.945s, episode steps: 206, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.612 [0.000, 3.000],  loss: 0.128878, mae: 6.488972, mean_q: -8.494838, mean_eps: 0.402351\n",
      "  797262/1200000: episode: 3330, duration: 9.019s, episode steps: 293, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.153933, mae: 6.435073, mean_q: -8.409251, mean_eps: 0.402164\n",
      "  797441/1200000: episode: 3331, duration: 5.234s, episode steps: 179, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 0.125312, mae: 6.468982, mean_q: -8.455772, mean_eps: 0.401987\n",
      "  797813/1200000: episode: 3332, duration: 12.584s, episode steps: 372, steps per second:  30, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.132311, mae: 6.394625, mean_q: -8.352886, mean_eps: 0.401780\n",
      "  798094/1200000: episode: 3333, duration: 10.623s, episode steps: 281, steps per second:  26, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.126151, mae: 6.406799, mean_q: -8.379060, mean_eps: 0.401535\n",
      "  798266/1200000: episode: 3334, duration: 6.197s, episode steps: 172, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.122531, mae: 6.495282, mean_q: -8.497386, mean_eps: 0.401365\n",
      "  798548/1200000: episode: 3335, duration: 8.519s, episode steps: 282, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.124058, mae: 6.419017, mean_q: -8.382151, mean_eps: 0.401195\n",
      "  798728/1200000: episode: 3336, duration: 5.051s, episode steps: 180, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.120655, mae: 6.404253, mean_q: -8.365199, mean_eps: 0.401022\n",
      "  798921/1200000: episode: 3337, duration: 5.629s, episode steps: 193, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.736 [0.000, 3.000],  loss: 0.143459, mae: 6.429943, mean_q: -8.406425, mean_eps: 0.400882\n",
      "  799084/1200000: episode: 3338, duration: 4.319s, episode steps: 163, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.152274, mae: 6.415730, mean_q: -8.367691, mean_eps: 0.400749\n",
      "  799297/1200000: episode: 3339, duration: 5.558s, episode steps: 213, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.132667, mae: 6.436519, mean_q: -8.424038, mean_eps: 0.400608\n",
      "  799476/1200000: episode: 3340, duration: 4.664s, episode steps: 179, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.754 [0.000, 3.000],  loss: 0.129191, mae: 6.403958, mean_q: -8.376434, mean_eps: 0.400460\n",
      "  799658/1200000: episode: 3341, duration: 4.788s, episode steps: 182, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.275 [0.000, 3.000],  loss: 0.159253, mae: 6.423678, mean_q: -8.400895, mean_eps: 0.400325\n",
      "  799836/1200000: episode: 3342, duration: 6.616s, episode steps: 178, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.159435, mae: 6.474985, mean_q: -8.453272, mean_eps: 0.400190\n",
      "  800189/1200000: episode: 3343, duration: 10.450s, episode steps: 353, steps per second:  34, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.105765, mae: 6.328541, mean_q: -8.268497, mean_eps: 0.399991\n",
      "  800365/1200000: episode: 3344, duration: 4.978s, episode steps: 176, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.124663, mae: 6.203746, mean_q: -8.109718, mean_eps: 0.399793\n",
      "  800611/1200000: episode: 3345, duration: 6.432s, episode steps: 246, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.125065, mae: 6.180514, mean_q: -8.075482, mean_eps: 0.399634\n",
      "  800788/1200000: episode: 3346, duration: 4.801s, episode steps: 177, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.135217, mae: 6.250606, mean_q: -8.163713, mean_eps: 0.399476\n",
      "  801152/1200000: episode: 3347, duration: 9.695s, episode steps: 364, steps per second:  38, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.129771, mae: 6.223147, mean_q: -8.139872, mean_eps: 0.399273\n",
      "  801434/1200000: episode: 3348, duration: 7.432s, episode steps: 282, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.129611, mae: 6.262735, mean_q: -8.185791, mean_eps: 0.399031\n",
      "  801609/1200000: episode: 3349, duration: 5.027s, episode steps: 175, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.118776, mae: 6.230242, mean_q: -8.149846, mean_eps: 0.398859\n",
      "  801793/1200000: episode: 3350, duration: 4.858s, episode steps: 184, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.148881, mae: 6.200912, mean_q: -8.086993, mean_eps: 0.398725\n",
      "  801965/1200000: episode: 3351, duration: 4.607s, episode steps: 172, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.118519, mae: 6.230353, mean_q: -8.134540, mean_eps: 0.398591\n",
      "  802210/1200000: episode: 3352, duration: 6.383s, episode steps: 245, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.131698, mae: 6.150657, mean_q: -8.056202, mean_eps: 0.398435\n",
      "  802408/1200000: episode: 3353, duration: 5.165s, episode steps: 198, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.283 [0.000, 3.000],  loss: 0.155386, mae: 6.229369, mean_q: -8.127635, mean_eps: 0.398269\n",
      "  802648/1200000: episode: 3354, duration: 6.487s, episode steps: 240, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.104812, mae: 6.251372, mean_q: -8.186567, mean_eps: 0.398104\n",
      "  802820/1200000: episode: 3355, duration: 4.614s, episode steps: 172, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.157192, mae: 6.217781, mean_q: -8.114886, mean_eps: 0.397950\n",
      "  803028/1200000: episode: 3356, duration: 5.468s, episode steps: 208, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.880 [0.000, 3.000],  loss: 0.107078, mae: 6.290163, mean_q: -8.228424, mean_eps: 0.397807\n",
      "  803208/1200000: episode: 3357, duration: 4.690s, episode steps: 180, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.109056, mae: 6.308671, mean_q: -8.251271, mean_eps: 0.397662\n",
      "  803391/1200000: episode: 3358, duration: 4.883s, episode steps: 183, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.143654, mae: 6.249616, mean_q: -8.161057, mean_eps: 0.397526\n",
      "  803569/1200000: episode: 3359, duration: 4.834s, episode steps: 178, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.102360, mae: 6.259897, mean_q: -8.192773, mean_eps: 0.397390\n",
      "  803769/1200000: episode: 3360, duration: 5.269s, episode steps: 200, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.093046, mae: 6.231310, mean_q: -8.157590, mean_eps: 0.397249\n",
      "  803953/1200000: episode: 3361, duration: 4.889s, episode steps: 184, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.973 [0.000, 3.000],  loss: 0.110442, mae: 6.320341, mean_q: -8.282864, mean_eps: 0.397105\n",
      "  804146/1200000: episode: 3362, duration: 4.984s, episode steps: 193, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.839 [0.000, 3.000],  loss: 0.158491, mae: 6.289962, mean_q: -8.201501, mean_eps: 0.396963\n",
      "  804332/1200000: episode: 3363, duration: 4.878s, episode steps: 186, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.128420, mae: 6.193905, mean_q: -8.079720, mean_eps: 0.396821\n",
      "  804569/1200000: episode: 3364, duration: 6.171s, episode steps: 237, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.122633, mae: 6.173307, mean_q: -8.078120, mean_eps: 0.396662\n",
      "  804750/1200000: episode: 3365, duration: 4.670s, episode steps: 181, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.104838, mae: 6.238826, mean_q: -8.161979, mean_eps: 0.396506\n",
      "  804930/1200000: episode: 3366, duration: 4.868s, episode steps: 180, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.138407, mae: 6.241502, mean_q: -8.152809, mean_eps: 0.396370\n",
      "  805276/1200000: episode: 3367, duration: 9.128s, episode steps: 346, steps per second:  38, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.125807, mae: 6.203674, mean_q: -8.121262, mean_eps: 0.396173\n",
      "  805457/1200000: episode: 3368, duration: 4.707s, episode steps: 181, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.146951, mae: 6.243739, mean_q: -8.167917, mean_eps: 0.395975\n",
      "  805659/1200000: episode: 3369, duration: 5.226s, episode steps: 202, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: 0.100717, mae: 6.265718, mean_q: -8.182886, mean_eps: 0.395832\n",
      "  805955/1200000: episode: 3370, duration: 7.732s, episode steps: 296, steps per second:  38, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.144759, mae: 6.268400, mean_q: -8.192096, mean_eps: 0.395645\n",
      "  806163/1200000: episode: 3371, duration: 5.451s, episode steps: 208, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.128509, mae: 6.162836, mean_q: -8.034994, mean_eps: 0.395456\n",
      "  806367/1200000: episode: 3372, duration: 5.296s, episode steps: 204, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 0.110903, mae: 6.214851, mean_q: -8.134175, mean_eps: 0.395302\n",
      "  806578/1200000: episode: 3373, duration: 5.626s, episode steps: 211, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.096293, mae: 6.185288, mean_q: -8.090990, mean_eps: 0.395146\n",
      "  806762/1200000: episode: 3374, duration: 4.974s, episode steps: 184, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.111842, mae: 6.182342, mean_q: -8.086081, mean_eps: 0.394998\n",
      "  806979/1200000: episode: 3375, duration: 5.755s, episode steps: 217, steps per second:  38, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.146982, mae: 6.230932, mean_q: -8.132530, mean_eps: 0.394847\n",
      "  807160/1200000: episode: 3376, duration: 4.850s, episode steps: 181, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.132070, mae: 6.301992, mean_q: -8.249322, mean_eps: 0.394698\n",
      "  807355/1200000: episode: 3377, duration: 5.177s, episode steps: 195, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.144364, mae: 6.267722, mean_q: -8.198107, mean_eps: 0.394557\n",
      "  807656/1200000: episode: 3378, duration: 7.854s, episode steps: 301, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.111184, mae: 6.276279, mean_q: -8.207122, mean_eps: 0.394371\n",
      "  807908/1200000: episode: 3379, duration: 6.595s, episode steps: 252, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.129432, mae: 6.240844, mean_q: -8.146818, mean_eps: 0.394164\n",
      "  808257/1200000: episode: 3380, duration: 9.452s, episode steps: 349, steps per second:  37, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.138253, mae: 6.286088, mean_q: -8.225101, mean_eps: 0.393938\n",
      "  808486/1200000: episode: 3381, duration: 6.400s, episode steps: 229, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.112254, mae: 6.253541, mean_q: -8.176996, mean_eps: 0.393722\n",
      "  808727/1200000: episode: 3382, duration: 6.526s, episode steps: 241, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.116028, mae: 6.217299, mean_q: -8.122194, mean_eps: 0.393545\n",
      "  808934/1200000: episode: 3383, duration: 5.580s, episode steps: 207, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.122234, mae: 6.190401, mean_q: -8.099221, mean_eps: 0.393377\n",
      "  809210/1200000: episode: 3384, duration: 7.703s, episode steps: 276, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.144557, mae: 6.249964, mean_q: -8.176692, mean_eps: 0.393196\n",
      "  809506/1200000: episode: 3385, duration: 7.871s, episode steps: 296, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.123844, mae: 6.280752, mean_q: -8.210083, mean_eps: 0.392982\n",
      "  809782/1200000: episode: 3386, duration: 7.199s, episode steps: 276, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.113939, mae: 6.279157, mean_q: -8.221068, mean_eps: 0.392767\n",
      "  810094/1200000: episode: 3387, duration: 8.091s, episode steps: 312, steps per second:  39, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.134971, mae: 6.212804, mean_q: -8.112964, mean_eps: 0.392547\n",
      "  810346/1200000: episode: 3388, duration: 6.683s, episode steps: 252, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.104821, mae: 6.115715, mean_q: -8.007988, mean_eps: 0.392335\n",
      "  810525/1200000: episode: 3389, duration: 4.759s, episode steps: 179, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.119696, mae: 6.145399, mean_q: -8.042754, mean_eps: 0.392174\n",
      "  810703/1200000: episode: 3390, duration: 4.817s, episode steps: 178, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.116769, mae: 6.139245, mean_q: -8.014167, mean_eps: 0.392040\n",
      "  811024/1200000: episode: 3391, duration: 8.578s, episode steps: 321, steps per second:  37, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.107106, mae: 6.131189, mean_q: -8.024347, mean_eps: 0.391853\n",
      "  811211/1200000: episode: 3392, duration: 4.987s, episode steps: 187, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.100812, mae: 6.055920, mean_q: -7.924572, mean_eps: 0.391662\n",
      "  811471/1200000: episode: 3393, duration: 6.953s, episode steps: 260, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.096184, mae: 6.156827, mean_q: -8.066506, mean_eps: 0.391495\n",
      "  811690/1200000: episode: 3394, duration: 6.064s, episode steps: 219, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.103092, mae: 6.103077, mean_q: -7.989566, mean_eps: 0.391315\n",
      "  811913/1200000: episode: 3395, duration: 6.017s, episode steps: 223, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.124373, mae: 6.099509, mean_q: -7.980439, mean_eps: 0.391149\n",
      "  812098/1200000: episode: 3396, duration: 5.036s, episode steps: 185, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.139896, mae: 6.096071, mean_q: -7.945671, mean_eps: 0.390996\n",
      "  812292/1200000: episode: 3397, duration: 5.193s, episode steps: 194, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.799 [0.000, 3.000],  loss: 0.107550, mae: 6.117214, mean_q: -8.004505, mean_eps: 0.390854\n",
      "  812600/1200000: episode: 3398, duration: 7.992s, episode steps: 308, steps per second:  39, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.118857, mae: 6.148829, mean_q: -8.040695, mean_eps: 0.390666\n",
      "  812949/1200000: episode: 3399, duration: 9.838s, episode steps: 349, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.104987, mae: 6.044896, mean_q: -7.914510, mean_eps: 0.390419\n",
      "  813247/1200000: episode: 3400, duration: 7.982s, episode steps: 298, steps per second:  37, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.119729, mae: 6.093552, mean_q: -7.957163, mean_eps: 0.390177\n",
      "  813435/1200000: episode: 3401, duration: 5.244s, episode steps: 188, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.079899, mae: 6.142218, mean_q: -8.042462, mean_eps: 0.389995\n",
      "  813659/1200000: episode: 3402, duration: 6.187s, episode steps: 224, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.098819, mae: 6.060101, mean_q: -7.923511, mean_eps: 0.389840\n",
      "  813849/1200000: episode: 3403, duration: 5.414s, episode steps: 190, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.284 [0.000, 3.000],  loss: 0.102766, mae: 6.077452, mean_q: -7.951228, mean_eps: 0.389685\n",
      "  814033/1200000: episode: 3404, duration: 5.348s, episode steps: 184, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.853 [0.000, 3.000],  loss: 0.135953, mae: 6.173011, mean_q: -8.066272, mean_eps: 0.389545\n",
      "  814286/1200000: episode: 3405, duration: 7.198s, episode steps: 253, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.088139, mae: 6.046524, mean_q: -7.900165, mean_eps: 0.389381\n",
      "  814521/1200000: episode: 3406, duration: 6.218s, episode steps: 235, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.114619, mae: 6.134952, mean_q: -8.018192, mean_eps: 0.389198\n",
      "  814713/1200000: episode: 3407, duration: 5.135s, episode steps: 192, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.505 [0.000, 3.000],  loss: 0.099940, mae: 6.123631, mean_q: -8.008949, mean_eps: 0.389038\n",
      "  815006/1200000: episode: 3408, duration: 7.777s, episode steps: 293, steps per second:  38, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.122140, mae: 6.069297, mean_q: -7.921572, mean_eps: 0.388856\n",
      "  815261/1200000: episode: 3409, duration: 6.906s, episode steps: 255, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.121838, mae: 6.182409, mean_q: -8.100750, mean_eps: 0.388650\n",
      "  815446/1200000: episode: 3410, duration: 5.475s, episode steps: 185, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 0.114376, mae: 6.013261, mean_q: -7.856422, mean_eps: 0.388485\n",
      "  815646/1200000: episode: 3411, duration: 5.478s, episode steps: 200, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.112598, mae: 6.078409, mean_q: -7.942371, mean_eps: 0.388341\n",
      "  815837/1200000: episode: 3412, duration: 5.167s, episode steps: 191, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.686 [0.000, 3.000],  loss: 0.125013, mae: 6.124642, mean_q: -7.988781, mean_eps: 0.388194\n",
      "  816092/1200000: episode: 3413, duration: 7.307s, episode steps: 255, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.116679, mae: 6.125367, mean_q: -8.000391, mean_eps: 0.388027\n",
      "  816280/1200000: episode: 3414, duration: 5.065s, episode steps: 188, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: 0.119952, mae: 6.066162, mean_q: -7.925458, mean_eps: 0.387861\n",
      "  816601/1200000: episode: 3415, duration: 8.486s, episode steps: 321, steps per second:  38, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.157550, mae: 6.073874, mean_q: -7.921237, mean_eps: 0.387670\n",
      "  816788/1200000: episode: 3416, duration: 4.984s, episode steps: 187, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.829 [0.000, 3.000],  loss: 0.122810, mae: 6.179407, mean_q: -8.071289, mean_eps: 0.387479\n",
      "  816960/1200000: episode: 3417, duration: 4.703s, episode steps: 172, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.096575, mae: 6.114306, mean_q: -8.002735, mean_eps: 0.387345\n",
      "  817173/1200000: episode: 3418, duration: 5.917s, episode steps: 213, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.690 [0.000, 3.000],  loss: 0.144129, mae: 6.068440, mean_q: -7.910617, mean_eps: 0.387200\n",
      "  817491/1200000: episode: 3419, duration: 8.449s, episode steps: 318, steps per second:  38, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.346 [0.000, 3.000],  loss: 0.106274, mae: 6.078272, mean_q: -7.949188, mean_eps: 0.387001\n",
      "  817667/1200000: episode: 3420, duration: 4.818s, episode steps: 176, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.092062, mae: 6.120617, mean_q: -8.005405, mean_eps: 0.386816\n",
      "  817855/1200000: episode: 3421, duration: 5.231s, episode steps: 188, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.787 [0.000, 3.000],  loss: 0.140868, mae: 6.135149, mean_q: -8.003909, mean_eps: 0.386680\n",
      "  818181/1200000: episode: 3422, duration: 8.753s, episode steps: 326, steps per second:  37, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.124698, mae: 6.087553, mean_q: -7.947891, mean_eps: 0.386487\n",
      "  818403/1200000: episode: 3423, duration: 5.934s, episode steps: 222, steps per second:  37, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.121678, mae: 6.135698, mean_q: -8.029224, mean_eps: 0.386281\n",
      "  818582/1200000: episode: 3424, duration: 4.852s, episode steps: 179, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.313 [0.000, 3.000],  loss: 0.112283, mae: 6.105897, mean_q: -7.972747, mean_eps: 0.386131\n",
      "  818781/1200000: episode: 3425, duration: 5.272s, episode steps: 199, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.115950, mae: 6.118068, mean_q: -7.991190, mean_eps: 0.385989\n",
      "  819059/1200000: episode: 3426, duration: 7.405s, episode steps: 278, steps per second:  38, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.114061, mae: 6.067991, mean_q: -7.922835, mean_eps: 0.385810\n",
      "  819322/1200000: episode: 3427, duration: 7.074s, episode steps: 263, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.121886, mae: 6.095080, mean_q: -7.978993, mean_eps: 0.385607\n",
      "  819538/1200000: episode: 3428, duration: 5.734s, episode steps: 216, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.087814, mae: 6.103435, mean_q: -7.985924, mean_eps: 0.385428\n",
      "  819769/1200000: episode: 3429, duration: 6.233s, episode steps: 231, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.208 [0.000, 3.000],  loss: 0.122879, mae: 6.067035, mean_q: -7.922105, mean_eps: 0.385260\n",
      "  820025/1200000: episode: 3430, duration: 6.912s, episode steps: 256, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.867 [0.000, 3.000],  loss: 0.103091, mae: 6.136087, mean_q: -8.026254, mean_eps: 0.385078\n",
      "  820223/1200000: episode: 3431, duration: 5.373s, episode steps: 198, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.134538, mae: 5.967457, mean_q: -7.796234, mean_eps: 0.384907\n",
      "  820391/1200000: episode: 3432, duration: 4.670s, episode steps: 168, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.093656, mae: 5.983766, mean_q: -7.830225, mean_eps: 0.384770\n",
      "  820573/1200000: episode: 3433, duration: 5.167s, episode steps: 182, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.132174, mae: 6.049812, mean_q: -7.894208, mean_eps: 0.384639\n",
      "  820885/1200000: episode: 3434, duration: 8.317s, episode steps: 312, steps per second:  38, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.125470, mae: 6.012145, mean_q: -7.862017, mean_eps: 0.384454\n",
      "  821126/1200000: episode: 3435, duration: 6.413s, episode steps: 241, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.114542, mae: 6.033508, mean_q: -7.877092, mean_eps: 0.384246\n",
      "  821302/1200000: episode: 3436, duration: 4.829s, episode steps: 176, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.111246, mae: 6.013127, mean_q: -7.873065, mean_eps: 0.384090\n",
      "  821479/1200000: episode: 3437, duration: 5.177s, episode steps: 177, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.143956, mae: 5.970074, mean_q: -7.787825, mean_eps: 0.383957\n",
      "  821685/1200000: episode: 3438, duration: 5.570s, episode steps: 206, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.133880, mae: 6.048270, mean_q: -7.911088, mean_eps: 0.383814\n",
      "  821923/1200000: episode: 3439, duration: 6.420s, episode steps: 238, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.127177, mae: 6.053079, mean_q: -7.910921, mean_eps: 0.383647\n",
      "  822117/1200000: episode: 3440, duration: 5.240s, episode steps: 194, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.130125, mae: 6.051418, mean_q: -7.902311, mean_eps: 0.383485\n",
      "  822357/1200000: episode: 3441, duration: 6.519s, episode steps: 240, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.825 [0.000, 3.000],  loss: 0.107952, mae: 6.091405, mean_q: -7.964714, mean_eps: 0.383323\n",
      "  822531/1200000: episode: 3442, duration: 4.656s, episode steps: 174, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.305 [0.000, 3.000],  loss: 0.125772, mae: 6.018799, mean_q: -7.869419, mean_eps: 0.383167\n",
      "  822769/1200000: episode: 3443, duration: 6.597s, episode steps: 238, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.138409, mae: 5.988140, mean_q: -7.820303, mean_eps: 0.383013\n",
      "  822945/1200000: episode: 3444, duration: 4.776s, episode steps: 176, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.151837, mae: 6.029356, mean_q: -7.877783, mean_eps: 0.382858\n",
      "  823160/1200000: episode: 3445, duration: 5.717s, episode steps: 215, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.089043, mae: 6.031721, mean_q: -7.906752, mean_eps: 0.382711\n",
      "  823343/1200000: episode: 3446, duration: 4.932s, episode steps: 183, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.689 [0.000, 3.000],  loss: 0.131510, mae: 6.090876, mean_q: -7.964815, mean_eps: 0.382562\n",
      "  823538/1200000: episode: 3447, duration: 5.278s, episode steps: 195, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.569 [0.000, 3.000],  loss: 0.103125, mae: 5.970144, mean_q: -7.808016, mean_eps: 0.382420\n",
      "  823936/1200000: episode: 3448, duration: 10.761s, episode steps: 398, steps per second:  37, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.125254, mae: 5.964727, mean_q: -7.784091, mean_eps: 0.382198\n",
      "  824120/1200000: episode: 3449, duration: 5.415s, episode steps: 184, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.745 [0.000, 3.000],  loss: 0.114238, mae: 6.018565, mean_q: -7.873605, mean_eps: 0.381979\n",
      "  824321/1200000: episode: 3450, duration: 5.751s, episode steps: 201, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.886 [0.000, 3.000],  loss: 0.100217, mae: 6.022247, mean_q: -7.889875, mean_eps: 0.381835\n",
      "  824533/1200000: episode: 3451, duration: 5.964s, episode steps: 212, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.092555, mae: 5.960190, mean_q: -7.798768, mean_eps: 0.381680\n",
      "  824730/1200000: episode: 3452, duration: 5.843s, episode steps: 197, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: 0.109179, mae: 6.030089, mean_q: -7.891250, mean_eps: 0.381527\n",
      "  824907/1200000: episode: 3453, duration: 5.246s, episode steps: 177, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.084766, mae: 5.998380, mean_q: -7.849031, mean_eps: 0.381386\n",
      "  825213/1200000: episode: 3454, duration: 9.488s, episode steps: 306, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.114832, mae: 5.995211, mean_q: -7.841426, mean_eps: 0.381205\n",
      "  825470/1200000: episode: 3455, duration: 7.145s, episode steps: 257, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.106482, mae: 5.994033, mean_q: -7.845995, mean_eps: 0.380994\n",
      "  825690/1200000: episode: 3456, duration: 6.173s, episode steps: 220, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.101720, mae: 6.043340, mean_q: -7.909137, mean_eps: 0.380815\n",
      "  826025/1200000: episode: 3457, duration: 8.989s, episode steps: 335, steps per second:  37, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.096208, mae: 5.957255, mean_q: -7.796930, mean_eps: 0.380607\n",
      "  826213/1200000: episode: 3458, duration: 5.067s, episode steps: 188, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.979 [0.000, 3.000],  loss: 0.129175, mae: 6.001858, mean_q: -7.845507, mean_eps: 0.380411\n",
      "  826410/1200000: episode: 3459, duration: 5.463s, episode steps: 197, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.089815, mae: 6.009375, mean_q: -7.870486, mean_eps: 0.380267\n",
      "  826615/1200000: episode: 3460, duration: 5.636s, episode steps: 205, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.502 [0.000, 3.000],  loss: 0.103437, mae: 6.019785, mean_q: -7.872400, mean_eps: 0.380116\n",
      "  826781/1200000: episode: 3461, duration: 4.614s, episode steps: 166, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.301 [0.000, 3.000],  loss: 0.101118, mae: 6.036814, mean_q: -7.893728, mean_eps: 0.379977\n",
      "  826987/1200000: episode: 3462, duration: 5.595s, episode steps: 206, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.243 [0.000, 3.000],  loss: 0.123709, mae: 6.041407, mean_q: -7.889297, mean_eps: 0.379837\n",
      "  827181/1200000: episode: 3463, duration: 5.346s, episode steps: 194, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.147637, mae: 6.021778, mean_q: -7.859955, mean_eps: 0.379687\n",
      "  827365/1200000: episode: 3464, duration: 4.966s, episode steps: 184, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.113520, mae: 5.978614, mean_q: -7.795887, mean_eps: 0.379546\n",
      "  827530/1200000: episode: 3465, duration: 4.788s, episode steps: 165, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.133117, mae: 5.990577, mean_q: -7.831485, mean_eps: 0.379415\n",
      "  827730/1200000: episode: 3466, duration: 5.636s, episode steps: 200, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.580 [0.000, 3.000],  loss: 0.109851, mae: 5.962552, mean_q: -7.806789, mean_eps: 0.379278\n",
      "  827937/1200000: episode: 3467, duration: 5.762s, episode steps: 207, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.154553, mae: 6.022993, mean_q: -7.865685, mean_eps: 0.379125\n",
      "  828135/1200000: episode: 3468, duration: 5.473s, episode steps: 198, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.136836, mae: 6.006994, mean_q: -7.843195, mean_eps: 0.378973\n",
      "  828339/1200000: episode: 3469, duration: 5.680s, episode steps: 204, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.116381, mae: 5.972756, mean_q: -7.817181, mean_eps: 0.378823\n",
      "  828546/1200000: episode: 3470, duration: 5.627s, episode steps: 207, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.136237, mae: 6.009707, mean_q: -7.856021, mean_eps: 0.378668\n",
      "  828727/1200000: episode: 3471, duration: 5.015s, episode steps: 181, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.134214, mae: 6.047779, mean_q: -7.904895, mean_eps: 0.378523\n",
      "  828905/1200000: episode: 3472, duration: 5.122s, episode steps: 178, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 0.108604, mae: 5.984702, mean_q: -7.824003, mean_eps: 0.378388\n",
      "  829091/1200000: episode: 3473, duration: 5.388s, episode steps: 186, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.301 [0.000, 3.000],  loss: 0.100923, mae: 6.020567, mean_q: -7.870974, mean_eps: 0.378252\n",
      "  829319/1200000: episode: 3474, duration: 6.566s, episode steps: 228, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.137056, mae: 6.032394, mean_q: -7.882176, mean_eps: 0.378097\n",
      "  829558/1200000: episode: 3475, duration: 6.774s, episode steps: 239, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.331 [0.000, 3.000],  loss: 0.119405, mae: 5.980206, mean_q: -7.807233, mean_eps: 0.377922\n",
      "  829744/1200000: episode: 3476, duration: 5.075s, episode steps: 186, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.103775, mae: 5.971366, mean_q: -7.801515, mean_eps: 0.377762\n",
      "  829956/1200000: episode: 3477, duration: 5.962s, episode steps: 212, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.102908, mae: 6.055472, mean_q: -7.924444, mean_eps: 0.377613\n",
      "  830133/1200000: episode: 3478, duration: 5.028s, episode steps: 177, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.734 [0.000, 3.000],  loss: 0.137464, mae: 5.930049, mean_q: -7.736518, mean_eps: 0.377467\n",
      "  830331/1200000: episode: 3479, duration: 5.426s, episode steps: 198, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.081667, mae: 5.880434, mean_q: -7.708065, mean_eps: 0.377326\n",
      "  830509/1200000: episode: 3480, duration: 4.896s, episode steps: 178, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.102970, mae: 5.907809, mean_q: -7.727701, mean_eps: 0.377185\n",
      "  830736/1200000: episode: 3481, duration: 6.316s, episode steps: 227, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.085977, mae: 5.952693, mean_q: -7.801472, mean_eps: 0.377033\n",
      "  830964/1200000: episode: 3482, duration: 6.243s, episode steps: 228, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.122403, mae: 5.886482, mean_q: -7.703011, mean_eps: 0.376863\n",
      "  831167/1200000: episode: 3483, duration: 6.040s, episode steps: 203, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.094260, mae: 5.885337, mean_q: -7.705901, mean_eps: 0.376701\n",
      "  831335/1200000: episode: 3484, duration: 4.793s, episode steps: 168, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.084216, mae: 5.939196, mean_q: -7.777586, mean_eps: 0.376562\n",
      "  831527/1200000: episode: 3485, duration: 5.787s, episode steps: 192, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.094872, mae: 5.937792, mean_q: -7.765533, mean_eps: 0.376427\n",
      "  831716/1200000: episode: 3486, duration: 5.225s, episode steps: 189, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.513 [0.000, 3.000],  loss: 0.104876, mae: 5.937525, mean_q: -7.783252, mean_eps: 0.376284\n",
      "  831921/1200000: episode: 3487, duration: 5.677s, episode steps: 205, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 0.118015, mae: 5.904452, mean_q: -7.717490, mean_eps: 0.376136\n",
      "  832105/1200000: episode: 3488, duration: 5.082s, episode steps: 184, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.082044, mae: 5.871796, mean_q: -7.691469, mean_eps: 0.375991\n",
      "  832315/1200000: episode: 3489, duration: 5.791s, episode steps: 210, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.098838, mae: 5.913531, mean_q: -7.731989, mean_eps: 0.375843\n",
      "  832521/1200000: episode: 3490, duration: 5.919s, episode steps: 206, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.728 [0.000, 3.000],  loss: 0.103201, mae: 5.968404, mean_q: -7.807082, mean_eps: 0.375687\n",
      "  832744/1200000: episode: 3491, duration: 6.023s, episode steps: 223, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 0.097281, mae: 5.887712, mean_q: -7.701301, mean_eps: 0.375526\n",
      "  832937/1200000: episode: 3492, duration: 5.381s, episode steps: 193, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.106697, mae: 5.947677, mean_q: -7.774335, mean_eps: 0.375370\n",
      "  833118/1200000: episode: 3493, duration: 4.940s, episode steps: 181, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.271 [0.000, 3.000],  loss: 0.081454, mae: 5.933013, mean_q: -7.755732, mean_eps: 0.375230\n",
      "  833291/1200000: episode: 3494, duration: 4.803s, episode steps: 173, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.097597, mae: 5.909477, mean_q: -7.714545, mean_eps: 0.375097\n",
      "  833463/1200000: episode: 3495, duration: 4.709s, episode steps: 172, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.091512, mae: 5.940600, mean_q: -7.770801, mean_eps: 0.374968\n",
      "  833667/1200000: episode: 3496, duration: 5.724s, episode steps: 204, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.093435, mae: 5.926960, mean_q: -7.769196, mean_eps: 0.374827\n",
      "  833954/1200000: episode: 3497, duration: 7.838s, episode steps: 287, steps per second:  37, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.099953, mae: 5.889961, mean_q: -7.715035, mean_eps: 0.374642\n",
      "  834215/1200000: episode: 3498, duration: 7.121s, episode steps: 261, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.090803, mae: 5.890095, mean_q: -7.714202, mean_eps: 0.374437\n",
      "  834427/1200000: episode: 3499, duration: 5.922s, episode steps: 212, steps per second:  36, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.325 [0.000, 3.000],  loss: 0.114454, mae: 5.955939, mean_q: -7.796331, mean_eps: 0.374260\n",
      "  834625/1200000: episode: 3500, duration: 5.582s, episode steps: 198, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.092507, mae: 5.891153, mean_q: -7.705369, mean_eps: 0.374106\n",
      "  834812/1200000: episode: 3501, duration: 5.521s, episode steps: 187, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.075746, mae: 5.911676, mean_q: -7.753741, mean_eps: 0.373962\n",
      "  835044/1200000: episode: 3502, duration: 6.648s, episode steps: 232, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.098536, mae: 5.908318, mean_q: -7.727429, mean_eps: 0.373804\n",
      "  835228/1200000: episode: 3503, duration: 5.134s, episode steps: 184, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.117383, mae: 5.940066, mean_q: -7.768915, mean_eps: 0.373648\n",
      "  835424/1200000: episode: 3504, duration: 5.425s, episode steps: 196, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.092470, mae: 5.887389, mean_q: -7.711978, mean_eps: 0.373506\n",
      "  835600/1200000: episode: 3505, duration: 5.095s, episode steps: 176, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.152219, mae: 5.902638, mean_q: -7.703890, mean_eps: 0.373366\n",
      "  835777/1200000: episode: 3506, duration: 5.298s, episode steps: 177, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.096175, mae: 5.890184, mean_q: -7.685151, mean_eps: 0.373234\n",
      "  835960/1200000: episode: 3507, duration: 5.169s, episode steps: 183, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.721 [0.000, 3.000],  loss: 0.098781, mae: 5.814870, mean_q: -7.597538, mean_eps: 0.373099\n",
      "  836153/1200000: episode: 3508, duration: 5.368s, episode steps: 193, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.095540, mae: 5.966420, mean_q: -7.818496, mean_eps: 0.372958\n",
      "  836343/1200000: episode: 3509, duration: 5.616s, episode steps: 190, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.747 [0.000, 3.000],  loss: 0.111990, mae: 5.928406, mean_q: -7.747737, mean_eps: 0.372814\n",
      "  836561/1200000: episode: 3510, duration: 6.155s, episode steps: 218, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.298 [0.000, 3.000],  loss: 0.110023, mae: 5.920959, mean_q: -7.744656, mean_eps: 0.372661\n",
      "  836777/1200000: episode: 3511, duration: 6.089s, episode steps: 216, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.095825, mae: 5.916961, mean_q: -7.736373, mean_eps: 0.372499\n",
      "  836955/1200000: episode: 3512, duration: 4.930s, episode steps: 178, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.281 [0.000, 3.000],  loss: 0.116473, mae: 5.874639, mean_q: -7.678369, mean_eps: 0.372351\n",
      "  837165/1200000: episode: 3513, duration: 6.013s, episode steps: 210, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.112447, mae: 5.930010, mean_q: -7.747650, mean_eps: 0.372205\n",
      "  837504/1200000: episode: 3514, duration: 9.340s, episode steps: 339, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.097171, mae: 5.954731, mean_q: -7.801018, mean_eps: 0.372000\n",
      "  837836/1200000: episode: 3515, duration: 9.245s, episode steps: 332, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.103574, mae: 5.945417, mean_q: -7.780480, mean_eps: 0.371748\n",
      "  838133/1200000: episode: 3516, duration: 8.485s, episode steps: 297, steps per second:  35, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.101504, mae: 5.921404, mean_q: -7.748670, mean_eps: 0.371512\n",
      "  838391/1200000: episode: 3517, duration: 7.107s, episode steps: 258, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.136277, mae: 5.944982, mean_q: -7.761923, mean_eps: 0.371304\n",
      "  838556/1200000: episode: 3518, duration: 4.632s, episode steps: 165, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.079119, mae: 5.880898, mean_q: -7.695032, mean_eps: 0.371145\n",
      "  838817/1200000: episode: 3519, duration: 7.191s, episode steps: 261, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.109386, mae: 5.969706, mean_q: -7.806437, mean_eps: 0.370985\n",
      "  839005/1200000: episode: 3520, duration: 5.139s, episode steps: 188, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.131614, mae: 5.945979, mean_q: -7.764844, mean_eps: 0.370817\n",
      "  839306/1200000: episode: 3521, duration: 8.302s, episode steps: 301, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.115134, mae: 5.872825, mean_q: -7.681888, mean_eps: 0.370634\n",
      "  839479/1200000: episode: 3522, duration: 4.864s, episode steps: 173, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.075376, mae: 5.926472, mean_q: -7.766786, mean_eps: 0.370456\n",
      "  839706/1200000: episode: 3523, duration: 6.431s, episode steps: 227, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.093828, mae: 5.890009, mean_q: -7.713019, mean_eps: 0.370306\n",
      "  839883/1200000: episode: 3524, duration: 4.894s, episode steps: 177, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.132853, mae: 5.845874, mean_q: -7.642957, mean_eps: 0.370154\n",
      "  840157/1200000: episode: 3525, duration: 8.112s, episode steps: 274, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.084083, mae: 5.817268, mean_q: -7.634442, mean_eps: 0.369985\n",
      "  840334/1200000: episode: 3526, duration: 5.463s, episode steps: 177, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.091174, mae: 5.774132, mean_q: -7.561401, mean_eps: 0.369816\n",
      "  840507/1200000: episode: 3527, duration: 4.944s, episode steps: 173, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.107637, mae: 5.756381, mean_q: -7.546984, mean_eps: 0.369685\n",
      "  840811/1200000: episode: 3528, duration: 9.034s, episode steps: 304, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.078124, mae: 5.740060, mean_q: -7.516821, mean_eps: 0.369506\n",
      "  841070/1200000: episode: 3529, duration: 7.326s, episode steps: 259, steps per second:  35, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.094432, mae: 5.754742, mean_q: -7.521694, mean_eps: 0.369295\n",
      "  841270/1200000: episode: 3530, duration: 5.668s, episode steps: 200, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.490 [0.000, 3.000],  loss: 0.096796, mae: 5.797645, mean_q: -7.591928, mean_eps: 0.369123\n",
      "  841489/1200000: episode: 3531, duration: 6.209s, episode steps: 219, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.106168, mae: 5.803324, mean_q: -7.596553, mean_eps: 0.368966\n",
      "  841751/1200000: episode: 3532, duration: 7.245s, episode steps: 262, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.097658, mae: 5.785256, mean_q: -7.574286, mean_eps: 0.368785\n",
      "  841948/1200000: episode: 3533, duration: 5.471s, episode steps: 197, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.095656, mae: 5.805661, mean_q: -7.589998, mean_eps: 0.368613\n",
      "  842135/1200000: episode: 3534, duration: 5.271s, episode steps: 187, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.105137, mae: 5.746022, mean_q: -7.512470, mean_eps: 0.368469\n",
      "  842314/1200000: episode: 3535, duration: 5.118s, episode steps: 179, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.087550, mae: 5.824131, mean_q: -7.611092, mean_eps: 0.368332\n",
      "  842616/1200000: episode: 3536, duration: 8.446s, episode steps: 302, steps per second:  36, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.092577, mae: 5.743086, mean_q: -7.509203, mean_eps: 0.368152\n",
      "  842798/1200000: episode: 3537, duration: 5.123s, episode steps: 182, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.106015, mae: 5.776439, mean_q: -7.563982, mean_eps: 0.367970\n",
      "  842969/1200000: episode: 3538, duration: 4.689s, episode steps: 171, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.158 [0.000, 3.000],  loss: 0.098788, mae: 5.748140, mean_q: -7.516641, mean_eps: 0.367838\n",
      "  843136/1200000: episode: 3539, duration: 4.917s, episode steps: 167, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.641 [0.000, 3.000],  loss: 0.103773, mae: 5.833918, mean_q: -7.642398, mean_eps: 0.367711\n",
      "  843325/1200000: episode: 3540, duration: 5.271s, episode steps: 189, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.097091, mae: 5.796748, mean_q: -7.587640, mean_eps: 0.367578\n",
      "  843528/1200000: episode: 3541, duration: 5.486s, episode steps: 203, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.110755, mae: 5.794403, mean_q: -7.576417, mean_eps: 0.367430\n",
      "  843718/1200000: episode: 3542, duration: 5.361s, episode steps: 190, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.086782, mae: 5.777579, mean_q: -7.572720, mean_eps: 0.367283\n",
      "  843883/1200000: episode: 3543, duration: 4.545s, episode steps: 165, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.094062, mae: 5.744338, mean_q: -7.525903, mean_eps: 0.367150\n",
      "  844056/1200000: episode: 3544, duration: 4.911s, episode steps: 173, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.084769, mae: 5.723486, mean_q: -7.501217, mean_eps: 0.367023\n",
      "  844363/1200000: episode: 3545, duration: 8.718s, episode steps: 307, steps per second:  35, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.093556, mae: 5.795689, mean_q: -7.580924, mean_eps: 0.366843\n",
      "  844676/1200000: episode: 3546, duration: 8.776s, episode steps: 313, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.281 [0.000, 3.000],  loss: 0.112785, mae: 5.756729, mean_q: -7.520463, mean_eps: 0.366611\n",
      "  844871/1200000: episode: 3547, duration: 5.503s, episode steps: 195, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.215 [0.000, 3.000],  loss: 0.074471, mae: 5.847513, mean_q: -7.664824, mean_eps: 0.366420\n",
      "  845217/1200000: episode: 3548, duration: 9.827s, episode steps: 346, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.089414, mae: 5.793683, mean_q: -7.581193, mean_eps: 0.366217\n",
      "  845436/1200000: episode: 3549, duration: 6.733s, episode steps: 219, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.082969, mae: 5.774045, mean_q: -7.555331, mean_eps: 0.366005\n",
      "  845719/1200000: episode: 3550, duration: 8.156s, episode steps: 283, steps per second:  35, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.261 [0.000, 3.000],  loss: 0.103260, mae: 5.761203, mean_q: -7.534704, mean_eps: 0.365817\n",
      "  846042/1200000: episode: 3551, duration: 8.992s, episode steps: 323, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.097926, mae: 5.777149, mean_q: -7.560099, mean_eps: 0.365590\n",
      "  846224/1200000: episode: 3552, duration: 5.234s, episode steps: 182, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 0.103909, mae: 5.788908, mean_q: -7.562808, mean_eps: 0.365401\n",
      "  846511/1200000: episode: 3553, duration: 8.188s, episode steps: 287, steps per second:  35, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.085818, mae: 5.749304, mean_q: -7.521774, mean_eps: 0.365225\n",
      "  846771/1200000: episode: 3554, duration: 7.065s, episode steps: 260, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.101384, mae: 5.759234, mean_q: -7.533239, mean_eps: 0.365020\n",
      "  846940/1200000: episode: 3555, duration: 4.718s, episode steps: 169, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 0.087726, mae: 5.834930, mean_q: -7.618005, mean_eps: 0.364859\n",
      "  847140/1200000: episode: 3556, duration: 5.559s, episode steps: 200, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.125800, mae: 5.739383, mean_q: -7.496627, mean_eps: 0.364720\n",
      "  847320/1200000: episode: 3557, duration: 5.137s, episode steps: 180, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.344 [0.000, 3.000],  loss: 0.057174, mae: 5.775352, mean_q: -7.571873, mean_eps: 0.364578\n",
      "  847549/1200000: episode: 3558, duration: 6.452s, episode steps: 229, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.101596, mae: 5.784855, mean_q: -7.561378, mean_eps: 0.364424\n",
      "  847867/1200000: episode: 3559, duration: 8.878s, episode steps: 318, steps per second:  36, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.098202, mae: 5.778698, mean_q: -7.560315, mean_eps: 0.364219\n",
      "  848048/1200000: episode: 3560, duration: 5.083s, episode steps: 181, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.077003, mae: 5.684663, mean_q: -7.445737, mean_eps: 0.364032\n",
      "  848236/1200000: episode: 3561, duration: 5.278s, episode steps: 188, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.112591, mae: 5.738035, mean_q: -7.484484, mean_eps: 0.363894\n",
      "  848454/1200000: episode: 3562, duration: 6.287s, episode steps: 218, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.107592, mae: 5.846214, mean_q: -7.632962, mean_eps: 0.363742\n",
      "  848624/1200000: episode: 3563, duration: 5.101s, episode steps: 170, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.090374, mae: 5.760094, mean_q: -7.531801, mean_eps: 0.363596\n",
      "  848792/1200000: episode: 3564, duration: 5.079s, episode steps: 168, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.109128, mae: 5.798485, mean_q: -7.589312, mean_eps: 0.363469\n",
      "  849227/1200000: episode: 3565, duration: 12.383s, episode steps: 435, steps per second:  35, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.349 [0.000, 3.000],  loss: 0.105836, mae: 5.775029, mean_q: -7.548456, mean_eps: 0.363243\n",
      "  849565/1200000: episode: 3566, duration: 9.506s, episode steps: 338, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.257 [0.000, 3.000],  loss: 0.094831, mae: 5.764185, mean_q: -7.542859, mean_eps: 0.362953\n",
      "  849783/1200000: episode: 3567, duration: 6.772s, episode steps: 218, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.271 [0.000, 3.000],  loss: 0.104462, mae: 5.753906, mean_q: -7.527689, mean_eps: 0.362745\n",
      "  849947/1200000: episode: 3568, duration: 4.771s, episode steps: 164, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.274 [0.000, 3.000],  loss: 0.079387, mae: 5.805591, mean_q: -7.601751, mean_eps: 0.362602\n",
      "  850168/1200000: episode: 3569, duration: 6.280s, episode steps: 221, steps per second:  35, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.079059, mae: 5.690357, mean_q: -7.441121, mean_eps: 0.362457\n",
      "  850358/1200000: episode: 3570, duration: 5.850s, episode steps: 190, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.089328, mae: 5.649121, mean_q: -7.395618, mean_eps: 0.362303\n",
      "  850593/1200000: episode: 3571, duration: 7.633s, episode steps: 235, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.089450, mae: 5.692793, mean_q: -7.449468, mean_eps: 0.362144\n",
      "  850841/1200000: episode: 3572, duration: 7.520s, episode steps: 248, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.096637, mae: 5.632008, mean_q: -7.362540, mean_eps: 0.361963\n",
      "  851017/1200000: episode: 3573, duration: 5.245s, episode steps: 176, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.443 [0.000, 3.000],  loss: 0.093955, mae: 5.734142, mean_q: -7.508897, mean_eps: 0.361804\n",
      "  851285/1200000: episode: 3574, duration: 8.002s, episode steps: 268, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.209 [0.000, 3.000],  loss: 0.079109, mae: 5.615094, mean_q: -7.345990, mean_eps: 0.361637\n",
      "  851552/1200000: episode: 3575, duration: 8.087s, episode steps: 267, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.082214, mae: 5.704049, mean_q: -7.460203, mean_eps: 0.361436\n",
      "  851807/1200000: episode: 3576, duration: 7.811s, episode steps: 255, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.084946, mae: 5.637740, mean_q: -7.373116, mean_eps: 0.361241\n",
      "  852104/1200000: episode: 3577, duration: 8.883s, episode steps: 297, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.087169, mae: 5.607202, mean_q: -7.337811, mean_eps: 0.361034\n",
      "  852289/1200000: episode: 3578, duration: 5.361s, episode steps: 185, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.086208, mae: 5.644284, mean_q: -7.373211, mean_eps: 0.360853\n",
      "  852537/1200000: episode: 3579, duration: 7.087s, episode steps: 248, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.080175, mae: 5.642386, mean_q: -7.368191, mean_eps: 0.360691\n",
      "  852704/1200000: episode: 3580, duration: 4.988s, episode steps: 167, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.089211, mae: 5.657308, mean_q: -7.393533, mean_eps: 0.360535\n",
      "  852886/1200000: episode: 3581, duration: 5.197s, episode steps: 182, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.122353, mae: 5.687406, mean_q: -7.432236, mean_eps: 0.360404\n",
      "  853066/1200000: episode: 3582, duration: 5.103s, episode steps: 180, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.082646, mae: 5.632523, mean_q: -7.363987, mean_eps: 0.360268\n",
      "  853298/1200000: episode: 3583, duration: 6.483s, episode steps: 232, steps per second:  36, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.076030, mae: 5.590677, mean_q: -7.306242, mean_eps: 0.360114\n",
      "  853527/1200000: episode: 3584, duration: 6.755s, episode steps: 229, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.072551, mae: 5.579795, mean_q: -7.298719, mean_eps: 0.359941\n",
      "  853705/1200000: episode: 3585, duration: 5.148s, episode steps: 178, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.084610, mae: 5.751046, mean_q: -7.521129, mean_eps: 0.359788\n",
      "  854017/1200000: episode: 3586, duration: 9.176s, episode steps: 312, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.096036, mae: 5.652624, mean_q: -7.398494, mean_eps: 0.359605\n",
      "  854307/1200000: episode: 3587, duration: 8.769s, episode steps: 290, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.093979, mae: 5.677594, mean_q: -7.416402, mean_eps: 0.359379\n",
      "  854787/1200000: episode: 3588, duration: 13.550s, episode steps: 480, steps per second:  35, episode reward:  8.000, mean reward:  0.017 [ 0.000,  4.000], mean action: 1.569 [0.000, 3.000],  loss: 0.086153, mae: 5.629556, mean_q: -7.352257, mean_eps: 0.359090\n",
      "  855013/1200000: episode: 3589, duration: 6.782s, episode steps: 226, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.099348, mae: 5.711373, mean_q: -7.470497, mean_eps: 0.358825\n",
      "  855183/1200000: episode: 3590, duration: 4.966s, episode steps: 170, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.553 [0.000, 3.000],  loss: 0.107980, mae: 5.636114, mean_q: -7.368854, mean_eps: 0.358677\n",
      "  855360/1200000: episode: 3591, duration: 5.063s, episode steps: 177, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.061721, mae: 5.693136, mean_q: -7.454505, mean_eps: 0.358547\n",
      "  855546/1200000: episode: 3592, duration: 5.344s, episode steps: 186, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.683 [0.000, 3.000],  loss: 0.078742, mae: 5.692576, mean_q: -7.432258, mean_eps: 0.358411\n",
      "  855775/1200000: episode: 3593, duration: 6.468s, episode steps: 229, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.104965, mae: 5.659148, mean_q: -7.387531, mean_eps: 0.358255\n",
      "  855977/1200000: episode: 3594, duration: 5.855s, episode steps: 202, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.076721, mae: 5.747533, mean_q: -7.511079, mean_eps: 0.358093\n",
      "  856314/1200000: episode: 3595, duration: 9.547s, episode steps: 337, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.362 [0.000, 3.000],  loss: 0.082379, mae: 5.659575, mean_q: -7.397603, mean_eps: 0.357891\n",
      "  856554/1200000: episode: 3596, duration: 7.068s, episode steps: 240, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.091596, mae: 5.607332, mean_q: -7.319430, mean_eps: 0.357675\n",
      "  856862/1200000: episode: 3597, duration: 8.789s, episode steps: 308, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.080276, mae: 5.609229, mean_q: -7.340569, mean_eps: 0.357469\n",
      "  857174/1200000: episode: 3598, duration: 8.979s, episode steps: 312, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.100578, mae: 5.633770, mean_q: -7.351476, mean_eps: 0.357237\n",
      "  857492/1200000: episode: 3599, duration: 9.028s, episode steps: 318, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.102078, mae: 5.666360, mean_q: -7.387787, mean_eps: 0.357001\n",
      "  857842/1200000: episode: 3600, duration: 9.771s, episode steps: 350, steps per second:  36, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.082416, mae: 5.647636, mean_q: -7.379450, mean_eps: 0.356750\n",
      "  858028/1200000: episode: 3601, duration: 5.506s, episode steps: 186, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.105721, mae: 5.659618, mean_q: -7.389405, mean_eps: 0.356549\n",
      "  858258/1200000: episode: 3602, duration: 6.275s, episode steps: 230, steps per second:  37, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.343 [0.000, 3.000],  loss: 0.088999, mae: 5.651330, mean_q: -7.378444, mean_eps: 0.356393\n",
      "  858437/1200000: episode: 3603, duration: 5.114s, episode steps: 179, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.425 [0.000, 3.000],  loss: 0.088485, mae: 5.655734, mean_q: -7.388974, mean_eps: 0.356240\n",
      "  858678/1200000: episode: 3604, duration: 6.885s, episode steps: 241, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.091597, mae: 5.682153, mean_q: -7.427609, mean_eps: 0.356082\n",
      "  858990/1200000: episode: 3605, duration: 9.561s, episode steps: 312, steps per second:  33, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.094531, mae: 5.650899, mean_q: -7.387457, mean_eps: 0.355875\n",
      "  859321/1200000: episode: 3606, duration: 9.645s, episode steps: 331, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.067560, mae: 5.604680, mean_q: -7.344650, mean_eps: 0.355634\n",
      "  859573/1200000: episode: 3607, duration: 7.223s, episode steps: 252, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.078999, mae: 5.646551, mean_q: -7.377332, mean_eps: 0.355415\n",
      "  859749/1200000: episode: 3608, duration: 5.142s, episode steps: 176, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.284 [0.000, 3.000],  loss: 0.117803, mae: 5.669802, mean_q: -7.398967, mean_eps: 0.355255\n",
      "  859945/1200000: episode: 3609, duration: 5.629s, episode steps: 196, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.255 [0.000, 3.000],  loss: 0.087266, mae: 5.601691, mean_q: -7.326018, mean_eps: 0.355115\n",
      "  860174/1200000: episode: 3610, duration: 6.490s, episode steps: 229, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.089057, mae: 5.542128, mean_q: -7.239824, mean_eps: 0.354956\n",
      "  860364/1200000: episode: 3611, duration: 5.422s, episode steps: 190, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.076380, mae: 5.496055, mean_q: -7.186021, mean_eps: 0.354799\n",
      "  860560/1200000: episode: 3612, duration: 5.933s, episode steps: 196, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.291 [0.000, 3.000],  loss: 0.099347, mae: 5.510413, mean_q: -7.190905, mean_eps: 0.354654\n",
      "  861007/1200000: episode: 3613, duration: 13.598s, episode steps: 447, steps per second:  33, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.078452, mae: 5.551618, mean_q: -7.253223, mean_eps: 0.354413\n",
      "  861193/1200000: episode: 3614, duration: 5.432s, episode steps: 186, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.070049, mae: 5.608202, mean_q: -7.349367, mean_eps: 0.354175\n",
      "  861403/1200000: episode: 3615, duration: 6.694s, episode steps: 210, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.748 [0.000, 3.000],  loss: 0.101153, mae: 5.535978, mean_q: -7.231920, mean_eps: 0.354027\n",
      "  861655/1200000: episode: 3616, duration: 7.629s, episode steps: 252, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.081302, mae: 5.511895, mean_q: -7.211612, mean_eps: 0.353854\n",
      "  862009/1200000: episode: 3617, duration: 10.061s, episode steps: 354, steps per second:  35, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 0.065719, mae: 5.561993, mean_q: -7.284685, mean_eps: 0.353626\n",
      "  862206/1200000: episode: 3618, duration: 6.345s, episode steps: 197, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.070147, mae: 5.532647, mean_q: -7.237423, mean_eps: 0.353420\n",
      "  862581/1200000: episode: 3619, duration: 11.126s, episode steps: 375, steps per second:  34, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.076532, mae: 5.596356, mean_q: -7.321273, mean_eps: 0.353205\n",
      "  862776/1200000: episode: 3620, duration: 5.644s, episode steps: 195, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.090019, mae: 5.589322, mean_q: -7.306237, mean_eps: 0.352991\n",
      "  862955/1200000: episode: 3621, duration: 5.254s, episode steps: 179, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.093671, mae: 5.521293, mean_q: -7.188408, mean_eps: 0.352851\n",
      "  863136/1200000: episode: 3622, duration: 5.279s, episode steps: 181, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.101466, mae: 5.571957, mean_q: -7.269713, mean_eps: 0.352716\n",
      "  863384/1200000: episode: 3623, duration: 7.090s, episode steps: 248, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.093220, mae: 5.571820, mean_q: -7.285623, mean_eps: 0.352555\n",
      "  863700/1200000: episode: 3624, duration: 9.161s, episode steps: 316, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.074564, mae: 5.513911, mean_q: -7.213527, mean_eps: 0.352344\n",
      "  864026/1200000: episode: 3625, duration: 9.262s, episode steps: 326, steps per second:  35, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.089213, mae: 5.583106, mean_q: -7.296552, mean_eps: 0.352103\n",
      "  864272/1200000: episode: 3626, duration: 7.093s, episode steps: 246, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.088505, mae: 5.512242, mean_q: -7.198530, mean_eps: 0.351889\n",
      "  864559/1200000: episode: 3627, duration: 8.271s, episode steps: 287, steps per second:  35, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.096635, mae: 5.550557, mean_q: -7.249711, mean_eps: 0.351689\n",
      "  864851/1200000: episode: 3628, duration: 8.465s, episode steps: 292, steps per second:  34, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.092584, mae: 5.548304, mean_q: -7.237861, mean_eps: 0.351472\n",
      "  865025/1200000: episode: 3629, duration: 5.097s, episode steps: 174, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.098596, mae: 5.529551, mean_q: -7.234785, mean_eps: 0.351297\n",
      "  865199/1200000: episode: 3630, duration: 5.173s, episode steps: 174, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.402 [0.000, 3.000],  loss: 0.075270, mae: 5.503244, mean_q: -7.204262, mean_eps: 0.351166\n",
      "  865397/1200000: episode: 3631, duration: 5.618s, episode steps: 198, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.101008, mae: 5.624958, mean_q: -7.351721, mean_eps: 0.351027\n",
      "  865802/1200000: episode: 3632, duration: 11.665s, episode steps: 405, steps per second:  35, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.069663, mae: 5.547144, mean_q: -7.253508, mean_eps: 0.350801\n",
      "  865987/1200000: episode: 3633, duration: 5.410s, episode steps: 185, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.077410, mae: 5.472142, mean_q: -7.146223, mean_eps: 0.350579\n",
      "  866333/1200000: episode: 3634, duration: 9.836s, episode steps: 346, steps per second:  35, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.070713, mae: 5.557862, mean_q: -7.267633, mean_eps: 0.350380\n",
      "  866544/1200000: episode: 3635, duration: 7.421s, episode steps: 211, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.070023, mae: 5.506251, mean_q: -7.203375, mean_eps: 0.350172\n",
      "  866889/1200000: episode: 3636, duration: 11.255s, episode steps: 345, steps per second:  31, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.339 [0.000, 3.000],  loss: 0.090370, mae: 5.536082, mean_q: -7.238209, mean_eps: 0.349963\n",
      "  867080/1200000: episode: 3637, duration: 6.063s, episode steps: 191, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.093990, mae: 5.510940, mean_q: -7.216719, mean_eps: 0.349762\n",
      "  867278/1200000: episode: 3638, duration: 6.166s, episode steps: 198, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.712 [0.000, 3.000],  loss: 0.094249, mae: 5.509933, mean_q: -7.205460, mean_eps: 0.349616\n",
      "  867488/1200000: episode: 3639, duration: 6.139s, episode steps: 210, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.071012, mae: 5.508580, mean_q: -7.203933, mean_eps: 0.349463\n",
      "  867673/1200000: episode: 3640, duration: 5.878s, episode steps: 185, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.378 [0.000, 3.000],  loss: 0.076383, mae: 5.506981, mean_q: -7.205617, mean_eps: 0.349315\n",
      "  867988/1200000: episode: 3641, duration: 9.446s, episode steps: 315, steps per second:  33, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.069080, mae: 5.537969, mean_q: -7.249703, mean_eps: 0.349127\n",
      "  868172/1200000: episode: 3642, duration: 5.704s, episode steps: 184, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.074813, mae: 5.565432, mean_q: -7.279352, mean_eps: 0.348940\n",
      "  868346/1200000: episode: 3643, duration: 5.331s, episode steps: 174, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.096969, mae: 5.556943, mean_q: -7.255108, mean_eps: 0.348806\n",
      "  868542/1200000: episode: 3644, duration: 5.684s, episode steps: 196, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.096113, mae: 5.587111, mean_q: -7.295500, mean_eps: 0.348667\n",
      "  868773/1200000: episode: 3645, duration: 6.947s, episode steps: 231, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.108034, mae: 5.496473, mean_q: -7.187023, mean_eps: 0.348507\n",
      "  869191/1200000: episode: 3646, duration: 12.248s, episode steps: 418, steps per second:  34, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.096653, mae: 5.552796, mean_q: -7.253477, mean_eps: 0.348264\n",
      "  869381/1200000: episode: 3647, duration: 5.547s, episode steps: 190, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.768 [0.000, 3.000],  loss: 0.261029, mae: 5.576607, mean_q: -7.219350, mean_eps: 0.348036\n",
      "  869648/1200000: episode: 3648, duration: 8.090s, episode steps: 267, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.087320, mae: 5.566672, mean_q: -7.280126, mean_eps: 0.347864\n",
      "  870075/1200000: episode: 3649, duration: 12.390s, episode steps: 427, steps per second:  34, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.084788, mae: 5.535645, mean_q: -7.246144, mean_eps: 0.347604\n",
      "  870410/1200000: episode: 3650, duration: 9.805s, episode steps: 335, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.066632, mae: 5.510091, mean_q: -7.211369, mean_eps: 0.347318\n",
      "  870644/1200000: episode: 3651, duration: 6.756s, episode steps: 234, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.071725, mae: 5.520113, mean_q: -7.230254, mean_eps: 0.347105\n",
      "  871090/1200000: episode: 3652, duration: 13.069s, episode steps: 446, steps per second:  34, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.078428, mae: 5.504939, mean_q: -7.209443, mean_eps: 0.346850\n",
      "  871327/1200000: episode: 3653, duration: 6.955s, episode steps: 237, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.103375, mae: 5.509526, mean_q: -7.204807, mean_eps: 0.346594\n",
      "  871542/1200000: episode: 3654, duration: 6.320s, episode steps: 215, steps per second:  34, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.076191, mae: 5.466012, mean_q: -7.148442, mean_eps: 0.346425\n",
      "  871784/1200000: episode: 3655, duration: 6.960s, episode steps: 242, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.077958, mae: 5.507541, mean_q: -7.206775, mean_eps: 0.346253\n",
      "  871956/1200000: episode: 3656, duration: 5.071s, episode steps: 172, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: 0.055558, mae: 5.553120, mean_q: -7.284880, mean_eps: 0.346098\n",
      "  872131/1200000: episode: 3657, duration: 5.192s, episode steps: 175, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.095201, mae: 5.538627, mean_q: -7.244204, mean_eps: 0.345968\n",
      "  872325/1200000: episode: 3658, duration: 6.445s, episode steps: 194, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.084558, mae: 5.507726, mean_q: -7.198800, mean_eps: 0.345829\n",
      "  872584/1200000: episode: 3659, duration: 8.549s, episode steps: 259, steps per second:  30, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.085185, mae: 5.577785, mean_q: -7.292811, mean_eps: 0.345660\n",
      "  872910/1200000: episode: 3660, duration: 10.491s, episode steps: 326, steps per second:  31, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.078428, mae: 5.503343, mean_q: -7.208993, mean_eps: 0.345440\n",
      "  873227/1200000: episode: 3661, duration: 9.297s, episode steps: 317, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.076479, mae: 5.532388, mean_q: -7.232054, mean_eps: 0.345199\n",
      "  873554/1200000: episode: 3662, duration: 9.552s, episode steps: 327, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.073427, mae: 5.521950, mean_q: -7.233060, mean_eps: 0.344957\n",
      "  873836/1200000: episode: 3663, duration: 8.252s, episode steps: 282, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.079288, mae: 5.632378, mean_q: -7.371948, mean_eps: 0.344729\n",
      "  874075/1200000: episode: 3664, duration: 7.259s, episode steps: 239, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.093604, mae: 5.522789, mean_q: -7.227059, mean_eps: 0.344534\n",
      "  874371/1200000: episode: 3665, duration: 9.015s, episode steps: 296, steps per second:  33, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.073072, mae: 5.510509, mean_q: -7.208688, mean_eps: 0.344333\n",
      "  874644/1200000: episode: 3666, duration: 8.003s, episode steps: 273, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.077223, mae: 5.519800, mean_q: -7.226424, mean_eps: 0.344120\n",
      "  874807/1200000: episode: 3667, duration: 4.757s, episode steps: 163, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.106446, mae: 5.503996, mean_q: -7.195698, mean_eps: 0.343956\n",
      "  875083/1200000: episode: 3668, duration: 8.846s, episode steps: 276, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.078152, mae: 5.492321, mean_q: -7.194382, mean_eps: 0.343792\n",
      "  875403/1200000: episode: 3669, duration: 10.551s, episode steps: 320, steps per second:  30, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.061495, mae: 5.528504, mean_q: -7.244650, mean_eps: 0.343568\n",
      "  875638/1200000: episode: 3670, duration: 6.924s, episode steps: 235, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.050408, mae: 5.508026, mean_q: -7.222090, mean_eps: 0.343360\n",
      "  875897/1200000: episode: 3671, duration: 7.715s, episode steps: 259, steps per second:  34, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.068989, mae: 5.562764, mean_q: -7.279599, mean_eps: 0.343175\n",
      "  876070/1200000: episode: 3672, duration: 5.448s, episode steps: 173, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.249 [0.000, 3.000],  loss: 0.064660, mae: 5.508215, mean_q: -7.210141, mean_eps: 0.343013\n",
      "  876248/1200000: episode: 3673, duration: 5.604s, episode steps: 178, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.657 [0.000, 3.000],  loss: 0.108647, mae: 5.531690, mean_q: -7.212645, mean_eps: 0.342881\n",
      "  876543/1200000: episode: 3674, duration: 8.630s, episode steps: 295, steps per second:  34, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.093307, mae: 5.515213, mean_q: -7.215632, mean_eps: 0.342704\n",
      "  876761/1200000: episode: 3675, duration: 6.838s, episode steps: 218, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.066561, mae: 5.516758, mean_q: -7.222427, mean_eps: 0.342511\n",
      "  877052/1200000: episode: 3676, duration: 8.830s, episode steps: 291, steps per second:  33, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.080046, mae: 5.552193, mean_q: -7.265367, mean_eps: 0.342320\n",
      "  877232/1200000: episode: 3677, duration: 5.382s, episode steps: 180, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.911 [0.000, 3.000],  loss: 0.072555, mae: 5.545077, mean_q: -7.257990, mean_eps: 0.342144\n",
      "  877421/1200000: episode: 3678, duration: 5.633s, episode steps: 189, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.085569, mae: 5.555544, mean_q: -7.268264, mean_eps: 0.342005\n",
      "  877663/1200000: episode: 3679, duration: 7.206s, episode steps: 242, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.080763, mae: 5.586244, mean_q: -7.313603, mean_eps: 0.341844\n",
      "  877903/1200000: episode: 3680, duration: 7.173s, episode steps: 240, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.090609, mae: 5.573942, mean_q: -7.290666, mean_eps: 0.341663\n",
      "  878073/1200000: episode: 3681, duration: 4.994s, episode steps: 170, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.082575, mae: 5.561852, mean_q: -7.272591, mean_eps: 0.341509\n",
      "  878246/1200000: episode: 3682, duration: 5.132s, episode steps: 173, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.746 [0.000, 3.000],  loss: 0.091687, mae: 5.558198, mean_q: -7.271146, mean_eps: 0.341381\n",
      "  878480/1200000: episode: 3683, duration: 6.781s, episode steps: 234, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.080209, mae: 5.492199, mean_q: -7.181927, mean_eps: 0.341228\n",
      "  878733/1200000: episode: 3684, duration: 7.331s, episode steps: 253, steps per second:  35, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.088167, mae: 5.531344, mean_q: -7.241005, mean_eps: 0.341046\n",
      "  878928/1200000: episode: 3685, duration: 5.898s, episode steps: 195, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.338 [0.000, 3.000],  loss: 0.082689, mae: 5.561111, mean_q: -7.279151, mean_eps: 0.340877\n",
      "  879282/1200000: episode: 3686, duration: 10.344s, episode steps: 354, steps per second:  34, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.076314, mae: 5.564875, mean_q: -7.287810, mean_eps: 0.340672\n",
      "  879607/1200000: episode: 3687, duration: 9.466s, episode steps: 325, steps per second:  34, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.066137, mae: 5.556643, mean_q: -7.279589, mean_eps: 0.340417\n",
      "  879982/1200000: episode: 3688, duration: 10.892s, episode steps: 375, steps per second:  34, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.079133, mae: 5.552266, mean_q: -7.268368, mean_eps: 0.340155\n",
      "  880158/1200000: episode: 3689, duration: 5.102s, episode steps: 176, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.540 [0.000, 3.000],  loss: 0.069804, mae: 5.571119, mean_q: -7.300822, mean_eps: 0.339948\n",
      "  880376/1200000: episode: 3690, duration: 6.520s, episode steps: 218, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.073758, mae: 5.584242, mean_q: -7.322200, mean_eps: 0.339800\n",
      "  880639/1200000: episode: 3691, duration: 7.699s, episode steps: 263, steps per second:  34, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.057020, mae: 5.585121, mean_q: -7.335130, mean_eps: 0.339620\n",
      "  880969/1200000: episode: 3692, duration: 9.668s, episode steps: 330, steps per second:  34, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.076272, mae: 5.648253, mean_q: -7.405898, mean_eps: 0.339397\n",
      "  881136/1200000: episode: 3693, duration: 5.153s, episode steps: 167, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.317 [0.000, 3.000],  loss: 0.104585, mae: 5.634149, mean_q: -7.374092, mean_eps: 0.339211\n",
      "  881388/1200000: episode: 3694, duration: 7.339s, episode steps: 252, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.270 [0.000, 3.000],  loss: 0.102809, mae: 5.687198, mean_q: -7.449836, mean_eps: 0.339054\n",
      "  881590/1200000: episode: 3695, duration: 6.126s, episode steps: 202, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.079512, mae: 5.545217, mean_q: -7.259027, mean_eps: 0.338884\n",
      "  881911/1200000: episode: 3696, duration: 9.306s, episode steps: 321, steps per second:  34, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.084261, mae: 5.555833, mean_q: -7.274079, mean_eps: 0.338687\n",
      "  882142/1200000: episode: 3697, duration: 6.739s, episode steps: 231, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.055114, mae: 5.514569, mean_q: -7.230167, mean_eps: 0.338480\n",
      "  882360/1200000: episode: 3698, duration: 6.880s, episode steps: 218, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.844 [0.000, 3.000],  loss: 0.082044, mae: 5.613327, mean_q: -7.356074, mean_eps: 0.338312\n",
      "  882707/1200000: episode: 3699, duration: 10.412s, episode steps: 347, steps per second:  33, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.320 [0.000, 3.000],  loss: 0.057849, mae: 5.580286, mean_q: -7.322169, mean_eps: 0.338100\n",
      "  882960/1200000: episode: 3700, duration: 7.800s, episode steps: 253, steps per second:  32, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.079711, mae: 5.577607, mean_q: -7.305316, mean_eps: 0.337875\n",
      "  883248/1200000: episode: 3701, duration: 9.265s, episode steps: 288, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.091947, mae: 5.581111, mean_q: -7.304517, mean_eps: 0.337672\n",
      "  883463/1200000: episode: 3702, duration: 6.676s, episode steps: 215, steps per second:  32, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.274 [0.000, 3.000],  loss: 0.081149, mae: 5.646840, mean_q: -7.400556, mean_eps: 0.337484\n",
      "  883673/1200000: episode: 3703, duration: 6.580s, episode steps: 210, steps per second:  32, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.083298, mae: 5.593957, mean_q: -7.323366, mean_eps: 0.337324\n",
      "  883886/1200000: episode: 3704, duration: 6.520s, episode steps: 213, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.085056, mae: 5.507274, mean_q: -7.209531, mean_eps: 0.337166\n",
      "  884162/1200000: episode: 3705, duration: 8.828s, episode steps: 276, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.084041, mae: 5.606458, mean_q: -7.347360, mean_eps: 0.336982\n",
      "  884374/1200000: episode: 3706, duration: 6.437s, episode steps: 212, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.081119, mae: 5.576128, mean_q: -7.303265, mean_eps: 0.336799\n",
      "  884635/1200000: episode: 3707, duration: 7.907s, episode steps: 261, steps per second:  33, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.261 [0.000, 3.000],  loss: 0.065806, mae: 5.602098, mean_q: -7.347583, mean_eps: 0.336622\n",
      "  884884/1200000: episode: 3708, duration: 7.505s, episode steps: 249, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.079749, mae: 5.614892, mean_q: -7.355444, mean_eps: 0.336431\n",
      "  885092/1200000: episode: 3709, duration: 6.738s, episode steps: 208, steps per second:  31, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.061634, mae: 5.619736, mean_q: -7.367675, mean_eps: 0.336259\n",
      "  885273/1200000: episode: 3710, duration: 5.590s, episode steps: 181, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.271 [0.000, 3.000],  loss: 0.085533, mae: 5.596110, mean_q: -7.320625, mean_eps: 0.336114\n",
      "  885540/1200000: episode: 3711, duration: 8.459s, episode steps: 267, steps per second:  32, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.064615, mae: 5.657253, mean_q: -7.417323, mean_eps: 0.335945\n",
      "  886039/1200000: episode: 3712, duration: 14.931s, episode steps: 499, steps per second:  33, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.087646, mae: 5.582601, mean_q: -7.303867, mean_eps: 0.335658\n",
      "  886380/1200000: episode: 3713, duration: 10.438s, episode steps: 341, steps per second:  33, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.078402, mae: 5.621659, mean_q: -7.371534, mean_eps: 0.335343\n",
      "  886622/1200000: episode: 3714, duration: 7.833s, episode steps: 242, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.077127, mae: 5.613349, mean_q: -7.360461, mean_eps: 0.335125\n",
      "  886791/1200000: episode: 3715, duration: 5.159s, episode steps: 169, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.070507, mae: 5.573960, mean_q: -7.310671, mean_eps: 0.334970\n",
      "  886958/1200000: episode: 3716, duration: 5.299s, episode steps: 167, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.081195, mae: 5.608339, mean_q: -7.350420, mean_eps: 0.334844\n",
      "  887246/1200000: episode: 3717, duration: 8.615s, episode steps: 288, steps per second:  33, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.065761, mae: 5.624829, mean_q: -7.376689, mean_eps: 0.334674\n",
      "  887560/1200000: episode: 3718, duration: 9.541s, episode steps: 314, steps per second:  33, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.084973, mae: 5.567599, mean_q: -7.281901, mean_eps: 0.334448\n",
      "  887755/1200000: episode: 3719, duration: 5.856s, episode steps: 195, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.569 [0.000, 3.000],  loss: 0.084700, mae: 5.579620, mean_q: -7.316852, mean_eps: 0.334257\n",
      "  887929/1200000: episode: 3720, duration: 5.106s, episode steps: 174, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 0.093583, mae: 5.628217, mean_q: -7.366900, mean_eps: 0.334119\n",
      "  888174/1200000: episode: 3721, duration: 7.356s, episode steps: 245, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.071841, mae: 5.527407, mean_q: -7.241247, mean_eps: 0.333962\n",
      "  888450/1200000: episode: 3722, duration: 9.367s, episode steps: 276, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.092138, mae: 5.640546, mean_q: -7.385981, mean_eps: 0.333766\n",
      "  888727/1200000: episode: 3723, duration: 8.992s, episode steps: 277, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.098108, mae: 5.601904, mean_q: -7.335892, mean_eps: 0.333559\n",
      "  889068/1200000: episode: 3724, duration: 10.594s, episode steps: 341, steps per second:  32, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.073701, mae: 5.569696, mean_q: -7.306575, mean_eps: 0.333327\n",
      "  889244/1200000: episode: 3725, duration: 5.248s, episode steps: 176, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.060731, mae: 5.561655, mean_q: -7.291117, mean_eps: 0.333133\n",
      "  889479/1200000: episode: 3726, duration: 6.973s, episode steps: 235, steps per second:  34, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.066878, mae: 5.609871, mean_q: -7.354378, mean_eps: 0.332979\n",
      "  889708/1200000: episode: 3727, duration: 6.936s, episode steps: 229, steps per second:  33, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.103774, mae: 5.575543, mean_q: -7.288708, mean_eps: 0.332805\n",
      "  890022/1200000: episode: 3728, duration: 9.292s, episode steps: 314, steps per second:  34, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.067607, mae: 5.565678, mean_q: -7.297867, mean_eps: 0.332602\n",
      "  890224/1200000: episode: 3729, duration: 6.122s, episode steps: 202, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 0.075144, mae: 5.403605, mean_q: -7.079413, mean_eps: 0.332408\n",
      "  890430/1200000: episode: 3730, duration: 6.258s, episode steps: 206, steps per second:  33, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.272 [0.000, 3.000],  loss: 0.093478, mae: 5.457924, mean_q: -7.155001, mean_eps: 0.332255\n",
      "  890783/1200000: episode: 3731, duration: 10.972s, episode steps: 353, steps per second:  32, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.083658, mae: 5.478871, mean_q: -7.178136, mean_eps: 0.332045\n",
      "  891164/1200000: episode: 3732, duration: 11.797s, episode steps: 381, steps per second:  32, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.076667, mae: 5.411735, mean_q: -7.089985, mean_eps: 0.331770\n",
      "  891483/1200000: episode: 3733, duration: 12.087s, episode steps: 319, steps per second:  26, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.266 [0.000, 3.000],  loss: 0.093410, mae: 5.436911, mean_q: -7.110067, mean_eps: 0.331508\n",
      "  891815/1200000: episode: 3734, duration: 11.087s, episode steps: 332, steps per second:  30, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.295 [0.000, 3.000],  loss: 0.072263, mae: 5.417643, mean_q: -7.094576, mean_eps: 0.331264\n",
      "  891988/1200000: episode: 3735, duration: 5.873s, episode steps: 173, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.081260, mae: 5.396555, mean_q: -7.068194, mean_eps: 0.331074\n",
      "  892394/1200000: episode: 3736, duration: 13.510s, episode steps: 406, steps per second:  30, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.283 [0.000, 3.000],  loss: 0.076306, mae: 5.389294, mean_q: -7.056301, mean_eps: 0.330857\n",
      "  892731/1200000: episode: 3737, duration: 11.110s, episode steps: 337, steps per second:  30, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.086529, mae: 5.445053, mean_q: -7.119254, mean_eps: 0.330578\n",
      "  893094/1200000: episode: 3738, duration: 12.058s, episode steps: 363, steps per second:  30, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.075256, mae: 5.371193, mean_q: -7.035527, mean_eps: 0.330316\n",
      "  893305/1200000: episode: 3739, duration: 7.007s, episode steps: 211, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.299 [0.000, 3.000],  loss: 0.087507, mae: 5.426169, mean_q: -7.101299, mean_eps: 0.330101\n",
      "  893603/1200000: episode: 3740, duration: 10.183s, episode steps: 298, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.084634, mae: 5.419294, mean_q: -7.093992, mean_eps: 0.329910\n",
      "  893795/1200000: episode: 3741, duration: 6.508s, episode steps: 192, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.077835, mae: 5.453054, mean_q: -7.142774, mean_eps: 0.329726\n",
      "  893996/1200000: episode: 3742, duration: 6.736s, episode steps: 201, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.061398, mae: 5.472263, mean_q: -7.172479, mean_eps: 0.329579\n",
      "  894228/1200000: episode: 3743, duration: 7.922s, episode steps: 232, steps per second:  29, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.075924, mae: 5.383780, mean_q: -7.049212, mean_eps: 0.329416\n",
      "  894399/1200000: episode: 3744, duration: 5.843s, episode steps: 171, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.073850, mae: 5.522830, mean_q: -7.246475, mean_eps: 0.329265\n",
      "  894587/1200000: episode: 3745, duration: 6.400s, episode steps: 188, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.718 [0.000, 3.000],  loss: 0.070329, mae: 5.492791, mean_q: -7.200682, mean_eps: 0.329131\n",
      "  894863/1200000: episode: 3746, duration: 9.595s, episode steps: 276, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.068769, mae: 5.437336, mean_q: -7.122438, mean_eps: 0.328957\n",
      "  895142/1200000: episode: 3747, duration: 9.220s, episode steps: 279, steps per second:  30, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.061305, mae: 5.411883, mean_q: -7.092995, mean_eps: 0.328748\n",
      "  895353/1200000: episode: 3748, duration: 7.264s, episode steps: 211, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.085613, mae: 5.455542, mean_q: -7.134456, mean_eps: 0.328565\n",
      "  895717/1200000: episode: 3749, duration: 12.221s, episode steps: 364, steps per second:  30, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.076676, mae: 5.478965, mean_q: -7.180053, mean_eps: 0.328349\n",
      "  895896/1200000: episode: 3750, duration: 6.149s, episode steps: 179, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.068828, mae: 5.411243, mean_q: -7.088228, mean_eps: 0.328145\n",
      "  896088/1200000: episode: 3751, duration: 6.582s, episode steps: 192, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 0.059543, mae: 5.439374, mean_q: -7.130882, mean_eps: 0.328006\n",
      "  896355/1200000: episode: 3752, duration: 9.034s, episode steps: 267, steps per second:  30, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.063867, mae: 5.420109, mean_q: -7.104064, mean_eps: 0.327834\n",
      "  896754/1200000: episode: 3753, duration: 13.690s, episode steps: 399, steps per second:  29, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.078867, mae: 5.403393, mean_q: -7.077706, mean_eps: 0.327584\n",
      "  896929/1200000: episode: 3754, duration: 6.512s, episode steps: 175, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.088090, mae: 5.468854, mean_q: -7.159806, mean_eps: 0.327369\n",
      "  897238/1200000: episode: 3755, duration: 11.571s, episode steps: 309, steps per second:  27, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.079698, mae: 5.442018, mean_q: -7.127884, mean_eps: 0.327188\n",
      "  897534/1200000: episode: 3756, duration: 11.104s, episode steps: 296, steps per second:  27, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 0.070662, mae: 5.441810, mean_q: -7.126159, mean_eps: 0.326961\n",
      "  897717/1200000: episode: 3757, duration: 6.797s, episode steps: 183, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.284 [0.000, 3.000],  loss: 0.079193, mae: 5.370909, mean_q: -7.033039, mean_eps: 0.326781\n",
      "  898077/1200000: episode: 3758, duration: 13.305s, episode steps: 360, steps per second:  27, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.082829, mae: 5.367326, mean_q: -7.034003, mean_eps: 0.326578\n",
      "  898302/1200000: episode: 3759, duration: 8.284s, episode steps: 225, steps per second:  27, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.065085, mae: 5.459873, mean_q: -7.160150, mean_eps: 0.326358\n",
      "  898513/1200000: episode: 3760, duration: 7.908s, episode steps: 211, steps per second:  27, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.085768, mae: 5.398679, mean_q: -7.065641, mean_eps: 0.326195\n",
      "  898697/1200000: episode: 3761, duration: 6.834s, episode steps: 184, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.064097, mae: 5.403865, mean_q: -7.095093, mean_eps: 0.326047\n",
      "  899047/1200000: episode: 3762, duration: 12.948s, episode steps: 350, steps per second:  27, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.076024, mae: 5.416066, mean_q: -7.092794, mean_eps: 0.325846\n",
      "  899350/1200000: episode: 3763, duration: 10.975s, episode steps: 303, steps per second:  28, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.277 [0.000, 3.000],  loss: 0.071124, mae: 5.453816, mean_q: -7.150319, mean_eps: 0.325601\n",
      "  899651/1200000: episode: 3764, duration: 11.428s, episode steps: 301, steps per second:  26, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.083016, mae: 5.445764, mean_q: -7.130737, mean_eps: 0.325375\n",
      "  899836/1200000: episode: 3765, duration: 6.911s, episode steps: 185, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.303 [0.000, 3.000],  loss: 0.095510, mae: 5.476529, mean_q: -7.166563, mean_eps: 0.325193\n",
      "  900015/1200000: episode: 3766, duration: 6.978s, episode steps: 179, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.246 [0.000, 3.000],  loss: 0.068122, mae: 5.432151, mean_q: -7.111662, mean_eps: 0.325056\n",
      "  900289/1200000: episode: 3767, duration: 10.322s, episode steps: 274, steps per second:  27, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.083136, mae: 5.303064, mean_q: -6.948365, mean_eps: 0.324886\n",
      "  900479/1200000: episode: 3768, duration: 7.001s, episode steps: 190, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.086692, mae: 5.329444, mean_q: -6.970131, mean_eps: 0.324712\n",
      "  900669/1200000: episode: 3769, duration: 7.214s, episode steps: 190, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.068137, mae: 5.320889, mean_q: -6.967095, mean_eps: 0.324570\n",
      "  900833/1200000: episode: 3770, duration: 6.073s, episode steps: 164, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.086590, mae: 5.319781, mean_q: -6.964683, mean_eps: 0.324437\n",
      "  901002/1200000: episode: 3771, duration: 6.338s, episode steps: 169, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.081261, mae: 5.301181, mean_q: -6.937404, mean_eps: 0.324312\n",
      "  901331/1200000: episode: 3772, duration: 12.478s, episode steps: 329, steps per second:  26, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.082564, mae: 5.337835, mean_q: -6.987322, mean_eps: 0.324125\n",
      "  901507/1200000: episode: 3773, duration: 6.935s, episode steps: 176, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.077962, mae: 5.280763, mean_q: -6.911508, mean_eps: 0.323936\n",
      "  901682/1200000: episode: 3774, duration: 5.992s, episode steps: 175, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.093378, mae: 5.296878, mean_q: -6.935334, mean_eps: 0.323804\n",
      "  901998/1200000: episode: 3775, duration: 11.107s, episode steps: 316, steps per second:  28, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.228 [0.000, 3.000],  loss: 0.082255, mae: 5.317768, mean_q: -6.969933, mean_eps: 0.323620\n",
      "  902174/1200000: episode: 3776, duration: 7.585s, episode steps: 176, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 0.074489, mae: 5.352655, mean_q: -7.007503, mean_eps: 0.323436\n",
      "  902466/1200000: episode: 3777, duration: 10.964s, episode steps: 292, steps per second:  27, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.076083, mae: 5.325093, mean_q: -6.978463, mean_eps: 0.323260\n",
      "  902770/1200000: episode: 3778, duration: 12.997s, episode steps: 304, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.057568, mae: 5.297610, mean_q: -6.937898, mean_eps: 0.323037\n",
      "  903101/1200000: episode: 3779, duration: 11.830s, episode steps: 331, steps per second:  28, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.066374, mae: 5.312229, mean_q: -6.950253, mean_eps: 0.322799\n",
      "  903274/1200000: episode: 3780, duration: 6.240s, episode steps: 173, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.077139, mae: 5.317023, mean_q: -6.958703, mean_eps: 0.322610\n",
      "  903479/1200000: episode: 3781, duration: 7.055s, episode steps: 205, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.673 [0.000, 3.000],  loss: 0.074107, mae: 5.346316, mean_q: -6.997279, mean_eps: 0.322468\n",
      "  903684/1200000: episode: 3782, duration: 7.022s, episode steps: 205, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.054416, mae: 5.295076, mean_q: -6.939342, mean_eps: 0.322314\n",
      "  904013/1200000: episode: 3783, duration: 10.774s, episode steps: 329, steps per second:  31, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.070626, mae: 5.322155, mean_q: -6.963813, mean_eps: 0.322114\n",
      "  904450/1200000: episode: 3784, duration: 14.521s, episode steps: 437, steps per second:  30, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.226216, mae: 5.379324, mean_q: -6.966097, mean_eps: 0.321827\n",
      "  904789/1200000: episode: 3785, duration: 11.270s, episode steps: 339, steps per second:  30, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.077867, mae: 5.285329, mean_q: -6.918407, mean_eps: 0.321536\n",
      "  905087/1200000: episode: 3786, duration: 10.073s, episode steps: 298, steps per second:  30, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 0.068825, mae: 5.280382, mean_q: -6.925000, mean_eps: 0.321297\n",
      "  905337/1200000: episode: 3787, duration: 8.540s, episode steps: 250, steps per second:  29, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.073870, mae: 5.338945, mean_q: -7.005006, mean_eps: 0.321091\n",
      "  905500/1200000: episode: 3788, duration: 5.415s, episode steps: 163, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.319 [0.000, 3.000],  loss: 0.076756, mae: 5.294655, mean_q: -6.940596, mean_eps: 0.320936\n",
      "  905874/1200000: episode: 3789, duration: 12.930s, episode steps: 374, steps per second:  29, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.055286, mae: 5.268568, mean_q: -6.912502, mean_eps: 0.320735\n",
      "  906192/1200000: episode: 3790, duration: 11.561s, episode steps: 318, steps per second:  28, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.069170, mae: 5.338514, mean_q: -7.001690, mean_eps: 0.320476\n",
      "  906519/1200000: episode: 3791, duration: 11.261s, episode steps: 327, steps per second:  29, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.066151, mae: 5.385936, mean_q: -7.059540, mean_eps: 0.320234\n",
      "  906732/1200000: episode: 3792, duration: 7.645s, episode steps: 213, steps per second:  28, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.063758, mae: 5.318907, mean_q: -6.974776, mean_eps: 0.320031\n",
      "  906913/1200000: episode: 3793, duration: 5.980s, episode steps: 181, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.073368, mae: 5.299159, mean_q: -6.943842, mean_eps: 0.319883\n",
      "  907081/1200000: episode: 3794, duration: 5.687s, episode steps: 168, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.086655, mae: 5.331573, mean_q: -6.963982, mean_eps: 0.319753\n",
      "  907266/1200000: episode: 3795, duration: 5.926s, episode steps: 185, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 0.090976, mae: 5.358254, mean_q: -7.014300, mean_eps: 0.319620\n",
      "  907436/1200000: episode: 3796, duration: 5.538s, episode steps: 170, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.065567, mae: 5.360342, mean_q: -7.035099, mean_eps: 0.319487\n",
      "  907671/1200000: episode: 3797, duration: 7.390s, episode steps: 235, steps per second:  32, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.080683, mae: 5.351119, mean_q: -7.012316, mean_eps: 0.319335\n",
      "  907888/1200000: episode: 3798, duration: 7.243s, episode steps: 217, steps per second:  30, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.067014, mae: 5.313765, mean_q: -6.959627, mean_eps: 0.319166\n",
      "  908152/1200000: episode: 3799, duration: 8.524s, episode steps: 264, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.060990, mae: 5.330505, mean_q: -6.989342, mean_eps: 0.318985\n",
      "  908333/1200000: episode: 3800, duration: 5.656s, episode steps: 181, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.065908, mae: 5.297723, mean_q: -6.936947, mean_eps: 0.318819\n",
      "  908674/1200000: episode: 3801, duration: 10.883s, episode steps: 341, steps per second:  31, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.084729, mae: 5.300268, mean_q: -6.928980, mean_eps: 0.318623\n",
      "  908960/1200000: episode: 3802, duration: 9.885s, episode steps: 286, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.294 [0.000, 3.000],  loss: 0.076298, mae: 5.371991, mean_q: -7.028916, mean_eps: 0.318388\n",
      "  909151/1200000: episode: 3803, duration: 6.378s, episode steps: 191, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 0.065141, mae: 5.277613, mean_q: -6.910794, mean_eps: 0.318209\n",
      "  909381/1200000: episode: 3804, duration: 7.342s, episode steps: 230, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.370 [0.000, 3.000],  loss: 0.063237, mae: 5.328545, mean_q: -6.987967, mean_eps: 0.318051\n",
      "  909649/1200000: episode: 3805, duration: 8.797s, episode steps: 268, steps per second:  30, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.081647, mae: 5.325365, mean_q: -6.965248, mean_eps: 0.317864\n",
      "  909937/1200000: episode: 3806, duration: 9.251s, episode steps: 288, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.319 [0.000, 3.000],  loss: 0.066216, mae: 5.365553, mean_q: -7.028662, mean_eps: 0.317656\n",
      "  910182/1200000: episode: 3807, duration: 8.011s, episode steps: 245, steps per second:  31, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 0.085955, mae: 5.226991, mean_q: -6.838728, mean_eps: 0.317456\n",
      "  910469/1200000: episode: 3808, duration: 9.126s, episode steps: 287, steps per second:  31, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.068279, mae: 5.184508, mean_q: -6.794196, mean_eps: 0.317256\n",
      "  910782/1200000: episode: 3809, duration: 10.433s, episode steps: 313, steps per second:  30, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.079279, mae: 5.193674, mean_q: -6.805420, mean_eps: 0.317031\n",
      "  911018/1200000: episode: 3810, duration: 8.652s, episode steps: 236, steps per second:  27, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.064550, mae: 5.203241, mean_q: -6.820529, mean_eps: 0.316825\n",
      "  911200/1200000: episode: 3811, duration: 6.422s, episode steps: 182, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.086217, mae: 5.103121, mean_q: -6.683378, mean_eps: 0.316669\n",
      "  911395/1200000: episode: 3812, duration: 7.512s, episode steps: 195, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.631 [0.000, 3.000],  loss: 0.080680, mae: 5.200802, mean_q: -6.798275, mean_eps: 0.316527\n",
      "  911740/1200000: episode: 3813, duration: 14.649s, episode steps: 345, steps per second:  24, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.076091, mae: 5.174876, mean_q: -6.775293, mean_eps: 0.316325\n",
      "  912048/1200000: episode: 3814, duration: 10.830s, episode steps: 308, steps per second:  28, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.328 [0.000, 3.000],  loss: 0.072755, mae: 5.154333, mean_q: -6.747806, mean_eps: 0.316080\n",
      "  912348/1200000: episode: 3815, duration: 10.241s, episode steps: 300, steps per second:  29, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.061110, mae: 5.146646, mean_q: -6.741200, mean_eps: 0.315852\n"
     ]
    }
   ],
   "source": [
    "# compile fit and evaluate teh agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae']) \n",
    "train_history = dqn.fit(env, nb_steps=NB_STEPS,callbacks=[logger,checkpoint_callback], visualize=False, verbose=2)\n",
    "\n",
    "# save the weights\n",
    "dqn.save_weights(f'model/{file_name}_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 285.000, steps: 951\n",
      "Episode 2: reward: 285.000, steps: 966\n",
      "Episode 3: reward: 285.000, steps: 951\n",
      "Episode 4: reward: 285.000, steps: 983\n",
      "Episode 5: reward: 285.000, steps: 978\n"
     ]
    }
   ],
   "source": [
    "#test the agent\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the text file\n",
    "file = open(f\"training_logs_{file_name}.txt\").readlines()\n",
    "# its a list so just get the first and only text output\n",
    "text = file[0]\n",
    "\n",
    "# list of things in the file to remove such that i will be left with lists then loop to delete them\n",
    "char_del = ['{\"loss\": ',', \"mae\": ',', \"mean_q\": ',', \"mean_eps\": ',', \"episode_reward\": ',', \"nb_episode_steps\": ',', \"nb_steps\": ',', \"episode\": ',', \"duration\": ','}']\n",
    "textProc = []\n",
    "for i in range(10):\n",
    "    text = text.replace(char_del[i],\"\")\n",
    "#print(textProc)\n",
    "text = text.replace(\"[\",\"\")\n",
    "text = text.replace(\"NaN\",\"0\")\n",
    "\n",
    "# this turns it from a text thing to a list\n",
    "textProc = text.split(\"]\")[:-1]\n",
    "for i in range(len(textProc)):\n",
    "    textProc[i] = textProc[i].split(\",\")\n",
    "\n",
    "# list of the data columns\n",
    "cols = [\"loss\", \"mae\", \"mean_q\", \"mean_eps\", \"episode_reward\", \"nb_episode_steps\", \"nb_steps\", \"episode\", \"duration\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(textProc)):\n",
    "    df[cols[i]] = textProc[i]\n",
    "\n",
    "df.to_csv(f\"{file_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
