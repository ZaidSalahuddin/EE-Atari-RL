{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Visualizer, TrainIntervalLogger, TestLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 6    observations: (128,)\n"
     ]
    }
   ],
   "source": [
    "# makes the enviroment\n",
    "env = gym.make('SpaceInvaders-ram-v4')\n",
    "# old rom name: 'SpaceInvaders-ram-v4' or 'Breakout-ram-v4'\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "nb_obs = env.observation_space.shape\n",
    "\n",
    "print(\"actions:\", nb_actions, \"   observations:\", nb_obs)\n",
    "\n",
    "file_name = \"test_1\"\n",
    "window_size = 4\n",
    "NB_STEPS = 5000000\n",
    "NB_STEPS_POL = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "#file logger\n",
    "logger = FileLogger(f'training_logs_{file_name}.txt', interval=1) \n",
    "\n",
    "# saver callback\n",
    "weights_filename = f\"model/{file_name}_weights.h5f\"\n",
    "checkpoint_filename = f\"model/{file_name}_checkpoint.h5f\"\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_filename,interval=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 175,278\n",
      "Trainable params: 175,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(window_size,) + nb_obs))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the agent\n",
    "\n",
    "# use this line if training is starting from a checkpoint\n",
    "# model.load_weights(\"model/test_1_checkpoint.h5f\")\n",
    "# # or from the actual thing\n",
    "# model.load_weights(\"model/test_1_weights.h5f\")\n",
    "\n",
    "# setup the memory buffer\n",
    "memory = SequentialMemory(limit=1000000,window_length=window_size)\n",
    "\n",
    "# create the policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=.3,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=1000000) \n",
    "# create the agent\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100000,\n",
    "               target_model_update=10000, policy=policy, gamma=0.99) #removed batch size thing, maybe add back later if its actually important ig idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "     846/5000000: episode: 1, duration: 1.904s, episode steps: 846, steps per second: 444, episode reward: 270.000, mean reward:  0.319 [ 0.000, 30.000], mean action: 3.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1522/5000000: episode: 2, duration: 1.609s, episode steps: 676, steps per second: 420, episode reward: 245.000, mean reward:  0.362 [ 0.000, 30.000], mean action: 3.629 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2449/5000000: episode: 3, duration: 2.019s, episode steps: 927, steps per second: 459, episode reward: 310.000, mean reward:  0.334 [ 0.000, 30.000], mean action: 3.576 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3079/5000000: episode: 4, duration: 1.459s, episode steps: 630, steps per second: 432, episode reward: 185.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 3.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3704/5000000: episode: 5, duration: 1.422s, episode steps: 625, steps per second: 440, episode reward: 125.000, mean reward:  0.200 [ 0.000, 25.000], mean action: 3.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4910/5000000: episode: 6, duration: 2.624s, episode steps: 1206, steps per second: 460, episode reward: 500.000, mean reward:  0.415 [ 0.000, 200.000], mean action: 3.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5477/5000000: episode: 7, duration: 1.258s, episode steps: 567, steps per second: 451, episode reward: 115.000, mean reward:  0.203 [ 0.000, 25.000], mean action: 3.653 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6180/5000000: episode: 8, duration: 1.607s, episode steps: 703, steps per second: 437, episode reward: 245.000, mean reward:  0.349 [ 0.000, 25.000], mean action: 3.639 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6682/5000000: episode: 9, duration: 1.165s, episode steps: 502, steps per second: 431, episode reward: 105.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 3.663 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7926/5000000: episode: 10, duration: 2.690s, episode steps: 1244, steps per second: 463, episode reward: 430.000, mean reward:  0.346 [ 0.000, 30.000], mean action: 3.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8345/5000000: episode: 11, duration: 0.912s, episode steps: 419, steps per second: 460, episode reward: 95.000, mean reward:  0.227 [ 0.000, 25.000], mean action: 3.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8968/5000000: episode: 12, duration: 1.389s, episode steps: 623, steps per second: 448, episode reward: 60.000, mean reward:  0.096 [ 0.000, 15.000], mean action: 3.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9942/5000000: episode: 13, duration: 2.135s, episode steps: 974, steps per second: 456, episode reward: 195.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 3.640 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10636/5000000: episode: 14, duration: 1.500s, episode steps: 694, steps per second: 463, episode reward: 205.000, mean reward:  0.295 [ 0.000, 30.000], mean action: 3.648 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11441/5000000: episode: 15, duration: 1.843s, episode steps: 805, steps per second: 437, episode reward: 210.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 3.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12311/5000000: episode: 16, duration: 1.969s, episode steps: 870, steps per second: 442, episode reward: 325.000, mean reward:  0.374 [ 0.000, 30.000], mean action: 3.491 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13408/5000000: episode: 17, duration: 2.591s, episode steps: 1097, steps per second: 423, episode reward: 210.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 3.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14675/5000000: episode: 18, duration: 2.981s, episode steps: 1267, steps per second: 425, episode reward: 345.000, mean reward:  0.272 [ 0.000, 30.000], mean action: 3.509 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15351/5000000: episode: 19, duration: 1.479s, episode steps: 676, steps per second: 457, episode reward: 110.000, mean reward:  0.163 [ 0.000, 30.000], mean action: 3.680 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15882/5000000: episode: 20, duration: 1.164s, episode steps: 531, steps per second: 456, episode reward: 55.000, mean reward:  0.104 [ 0.000, 15.000], mean action: 3.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16571/5000000: episode: 21, duration: 1.516s, episode steps: 689, steps per second: 455, episode reward: 140.000, mean reward:  0.203 [ 0.000, 20.000], mean action: 3.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17208/5000000: episode: 22, duration: 1.390s, episode steps: 637, steps per second: 458, episode reward: 180.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 3.551 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17851/5000000: episode: 23, duration: 1.410s, episode steps: 643, steps per second: 456, episode reward: 135.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 3.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18491/5000000: episode: 24, duration: 1.408s, episode steps: 640, steps per second: 454, episode reward: 155.000, mean reward:  0.242 [ 0.000, 25.000], mean action: 3.566 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19644/5000000: episode: 25, duration: 2.436s, episode steps: 1153, steps per second: 473, episode reward: 620.000, mean reward:  0.538 [ 0.000, 200.000], mean action: 3.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20362/5000000: episode: 26, duration: 1.544s, episode steps: 718, steps per second: 465, episode reward: 115.000, mean reward:  0.160 [ 0.000, 20.000], mean action: 3.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21196/5000000: episode: 27, duration: 1.763s, episode steps: 834, steps per second: 473, episode reward: 445.000, mean reward:  0.534 [ 0.000, 200.000], mean action: 3.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22930/5000000: episode: 28, duration: 3.566s, episode steps: 1734, steps per second: 486, episode reward: 610.000, mean reward:  0.352 [ 0.000, 200.000], mean action: 3.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23766/5000000: episode: 29, duration: 1.794s, episode steps: 836, steps per second: 466, episode reward: 275.000, mean reward:  0.329 [ 0.000, 30.000], mean action: 3.591 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24611/5000000: episode: 30, duration: 1.959s, episode steps: 845, steps per second: 431, episode reward: 275.000, mean reward:  0.325 [ 0.000, 30.000], mean action: 3.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25448/5000000: episode: 31, duration: 1.873s, episode steps: 837, steps per second: 447, episode reward: 305.000, mean reward:  0.364 [ 0.000, 30.000], mean action: 3.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26203/5000000: episode: 32, duration: 1.629s, episode steps: 755, steps per second: 463, episode reward: 165.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 3.702 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26846/5000000: episode: 33, duration: 1.387s, episode steps: 643, steps per second: 464, episode reward: 165.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 3.644 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27361/5000000: episode: 34, duration: 1.117s, episode steps: 515, steps per second: 461, episode reward: 105.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 3.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28067/5000000: episode: 35, duration: 1.628s, episode steps: 706, steps per second: 434, episode reward: 175.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 3.564 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29167/5000000: episode: 36, duration: 2.432s, episode steps: 1100, steps per second: 452, episode reward: 425.000, mean reward:  0.386 [ 0.000, 200.000], mean action: 3.592 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29807/5000000: episode: 37, duration: 1.432s, episode steps: 640, steps per second: 447, episode reward: 170.000, mean reward:  0.266 [ 0.000, 30.000], mean action: 3.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30996/5000000: episode: 38, duration: 2.514s, episode steps: 1189, steps per second: 473, episode reward: 345.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 3.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31726/5000000: episode: 39, duration: 1.578s, episode steps: 730, steps per second: 462, episode reward: 185.000, mean reward:  0.253 [ 0.000, 20.000], mean action: 3.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33082/5000000: episode: 40, duration: 2.910s, episode steps: 1356, steps per second: 466, episode reward: 385.000, mean reward:  0.284 [ 0.000, 30.000], mean action: 3.607 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33708/5000000: episode: 41, duration: 1.449s, episode steps: 626, steps per second: 432, episode reward: 95.000, mean reward:  0.152 [ 0.000, 30.000], mean action: 3.591 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34421/5000000: episode: 42, duration: 1.586s, episode steps: 713, steps per second: 450, episode reward: 140.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 3.620 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34721/5000000: episode: 43, duration: 0.724s, episode steps: 300, steps per second: 414, episode reward: 40.000, mean reward:  0.133 [ 0.000, 25.000], mean action: 3.613 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35371/5000000: episode: 44, duration: 1.452s, episode steps: 650, steps per second: 448, episode reward: 300.000, mean reward:  0.462 [ 0.000, 30.000], mean action: 3.669 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36803/5000000: episode: 45, duration: 3.098s, episode steps: 1432, steps per second: 462, episode reward: 285.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 3.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37480/5000000: episode: 46, duration: 1.543s, episode steps: 677, steps per second: 439, episode reward: 215.000, mean reward:  0.318 [ 0.000, 30.000], mean action: 3.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38307/5000000: episode: 47, duration: 2.085s, episode steps: 827, steps per second: 397, episode reward: 270.000, mean reward:  0.326 [ 0.000, 30.000], mean action: 3.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38948/5000000: episode: 48, duration: 1.480s, episode steps: 641, steps per second: 433, episode reward: 195.000, mean reward:  0.304 [ 0.000, 30.000], mean action: 3.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39532/5000000: episode: 49, duration: 1.423s, episode steps: 584, steps per second: 410, episode reward: 170.000, mean reward:  0.291 [ 0.000, 25.000], mean action: 3.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40153/5000000: episode: 50, duration: 1.544s, episode steps: 621, steps per second: 402, episode reward: 95.000, mean reward:  0.153 [ 0.000, 15.000], mean action: 3.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41056/5000000: episode: 51, duration: 2.193s, episode steps: 903, steps per second: 412, episode reward: 300.000, mean reward:  0.332 [ 0.000, 30.000], mean action: 3.626 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41418/5000000: episode: 52, duration: 0.851s, episode steps: 362, steps per second: 425, episode reward: 35.000, mean reward:  0.097 [ 0.000, 25.000], mean action: 3.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42635/5000000: episode: 53, duration: 2.597s, episode steps: 1217, steps per second: 469, episode reward: 365.000, mean reward:  0.300 [ 0.000, 30.000], mean action: 3.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42976/5000000: episode: 54, duration: 0.758s, episode steps: 341, steps per second: 450, episode reward: 35.000, mean reward:  0.103 [ 0.000, 25.000], mean action: 3.639 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43633/5000000: episode: 55, duration: 1.436s, episode steps: 657, steps per second: 458, episode reward: 160.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 3.553 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44922/5000000: episode: 56, duration: 2.887s, episode steps: 1289, steps per second: 447, episode reward: 305.000, mean reward:  0.237 [ 0.000, 30.000], mean action: 3.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45699/5000000: episode: 57, duration: 1.797s, episode steps: 777, steps per second: 432, episode reward: 320.000, mean reward:  0.412 [ 0.000, 30.000], mean action: 3.620 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46334/5000000: episode: 58, duration: 1.469s, episode steps: 635, steps per second: 432, episode reward: 175.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 3.581 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46850/5000000: episode: 59, duration: 1.142s, episode steps: 516, steps per second: 452, episode reward: 70.000, mean reward:  0.136 [ 0.000, 20.000], mean action: 3.626 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47777/5000000: episode: 60, duration: 1.945s, episode steps: 927, steps per second: 477, episode reward: 455.000, mean reward:  0.491 [ 0.000, 200.000], mean action: 3.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48289/5000000: episode: 61, duration: 1.124s, episode steps: 512, steps per second: 456, episode reward: 25.000, mean reward:  0.049 [ 0.000, 10.000], mean action: 3.652 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49140/5000000: episode: 62, duration: 1.823s, episode steps: 851, steps per second: 467, episode reward: 220.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 3.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   50017/5000000: episode: 63, duration: 1.915s, episode steps: 877, steps per second: 458, episode reward: 320.000, mean reward:  0.365 [ 0.000, 30.000], mean action: 3.588 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   50817/5000000: episode: 64, duration: 1.847s, episode steps: 800, steps per second: 433, episode reward: 220.000, mean reward:  0.275 [ 0.000, 30.000], mean action: 3.506 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   51454/5000000: episode: 65, duration: 1.426s, episode steps: 637, steps per second: 447, episode reward: 185.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 3.576 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   52056/5000000: episode: 66, duration: 1.344s, episode steps: 602, steps per second: 448, episode reward: 110.000, mean reward:  0.183 [ 0.000, 20.000], mean action: 3.694 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   52706/5000000: episode: 67, duration: 1.431s, episode steps: 650, steps per second: 454, episode reward: 140.000, mean reward:  0.215 [ 0.000, 25.000], mean action: 3.609 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   53362/5000000: episode: 68, duration: 1.489s, episode steps: 656, steps per second: 440, episode reward: 180.000, mean reward:  0.274 [ 0.000, 25.000], mean action: 3.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   53903/5000000: episode: 69, duration: 1.281s, episode steps: 541, steps per second: 422, episode reward: 215.000, mean reward:  0.397 [ 0.000, 30.000], mean action: 3.562 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   54679/5000000: episode: 70, duration: 1.704s, episode steps: 776, steps per second: 456, episode reward: 220.000, mean reward:  0.284 [ 0.000, 25.000], mean action: 3.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   55515/5000000: episode: 71, duration: 1.981s, episode steps: 836, steps per second: 422, episode reward: 265.000, mean reward:  0.317 [ 0.000, 30.000], mean action: 3.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   56174/5000000: episode: 72, duration: 1.458s, episode steps: 659, steps per second: 452, episode reward: 220.000, mean reward:  0.334 [ 0.000, 30.000], mean action: 3.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   57009/5000000: episode: 73, duration: 1.861s, episode steps: 835, steps per second: 449, episode reward: 490.000, mean reward:  0.587 [ 0.000, 200.000], mean action: 3.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   57575/5000000: episode: 74, duration: 1.388s, episode steps: 566, steps per second: 408, episode reward: 95.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 3.678 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   58504/5000000: episode: 75, duration: 2.021s, episode steps: 929, steps per second: 460, episode reward: 315.000, mean reward:  0.339 [ 0.000, 30.000], mean action: 3.609 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   59209/5000000: episode: 76, duration: 1.530s, episode steps: 705, steps per second: 461, episode reward: 200.000, mean reward:  0.284 [ 0.000, 30.000], mean action: 3.600 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   59880/5000000: episode: 77, duration: 1.450s, episode steps: 671, steps per second: 463, episode reward: 250.000, mean reward:  0.373 [ 0.000, 30.000], mean action: 3.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   60379/5000000: episode: 78, duration: 1.073s, episode steps: 499, steps per second: 465, episode reward: 100.000, mean reward:  0.200 [ 0.000, 20.000], mean action: 3.631 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   61511/5000000: episode: 79, duration: 2.424s, episode steps: 1132, steps per second: 467, episode reward: 410.000, mean reward:  0.362 [ 0.000, 200.000], mean action: 3.564 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   62455/5000000: episode: 80, duration: 2.137s, episode steps: 944, steps per second: 442, episode reward: 225.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 3.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   63103/5000000: episode: 81, duration: 1.416s, episode steps: 648, steps per second: 458, episode reward: 175.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 3.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   63728/5000000: episode: 82, duration: 1.310s, episode steps: 625, steps per second: 477, episode reward: 160.000, mean reward:  0.256 [ 0.000, 30.000], mean action: 3.518 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   64422/5000000: episode: 83, duration: 1.673s, episode steps: 694, steps per second: 415, episode reward: 165.000, mean reward:  0.238 [ 0.000, 25.000], mean action: 3.641 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   64833/5000000: episode: 84, duration: 0.905s, episode steps: 411, steps per second: 454, episode reward: 115.000, mean reward:  0.280 [ 0.000, 30.000], mean action: 3.659 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   65481/5000000: episode: 85, duration: 1.405s, episode steps: 648, steps per second: 461, episode reward: 230.000, mean reward:  0.355 [ 0.000, 30.000], mean action: 3.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   66186/5000000: episode: 86, duration: 1.505s, episode steps: 705, steps per second: 469, episode reward: 90.000, mean reward:  0.128 [ 0.000, 20.000], mean action: 3.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   66584/5000000: episode: 87, duration: 0.872s, episode steps: 398, steps per second: 456, episode reward: 75.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 3.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   67722/5000000: episode: 88, duration: 2.321s, episode steps: 1138, steps per second: 490, episode reward: 280.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 3.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   68575/5000000: episode: 89, duration: 1.799s, episode steps: 853, steps per second: 474, episode reward: 265.000, mean reward:  0.311 [ 0.000, 30.000], mean action: 3.569 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   69212/5000000: episode: 90, duration: 1.400s, episode steps: 637, steps per second: 455, episode reward: 120.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 3.656 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   69875/5000000: episode: 91, duration: 1.465s, episode steps: 663, steps per second: 453, episode reward: 170.000, mean reward:  0.256 [ 0.000, 25.000], mean action: 3.578 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   70706/5000000: episode: 92, duration: 1.736s, episode steps: 831, steps per second: 479, episode reward: 330.000, mean reward:  0.397 [ 0.000, 30.000], mean action: 3.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   71125/5000000: episode: 93, duration: 0.997s, episode steps: 419, steps per second: 420, episode reward: 60.000, mean reward:  0.143 [ 0.000, 30.000], mean action: 3.740 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   71955/5000000: episode: 94, duration: 1.759s, episode steps: 830, steps per second: 472, episode reward: 250.000, mean reward:  0.301 [ 0.000, 30.000], mean action: 3.659 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   72476/5000000: episode: 95, duration: 1.086s, episode steps: 521, steps per second: 480, episode reward: 85.000, mean reward:  0.163 [ 0.000, 15.000], mean action: 3.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   72974/5000000: episode: 96, duration: 1.084s, episode steps: 498, steps per second: 459, episode reward: 125.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 3.687 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   73698/5000000: episode: 97, duration: 1.511s, episode steps: 724, steps per second: 479, episode reward: 230.000, mean reward:  0.318 [ 0.000, 25.000], mean action: 3.633 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   74325/5000000: episode: 98, duration: 1.346s, episode steps: 627, steps per second: 466, episode reward: 185.000, mean reward:  0.295 [ 0.000, 25.000], mean action: 3.636 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   75018/5000000: episode: 99, duration: 1.516s, episode steps: 693, steps per second: 457, episode reward: 170.000, mean reward:  0.245 [ 0.000, 25.000], mean action: 3.577 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   76110/5000000: episode: 100, duration: 2.340s, episode steps: 1092, steps per second: 467, episode reward: 430.000, mean reward:  0.394 [ 0.000, 200.000], mean action: 3.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   76769/5000000: episode: 101, duration: 1.412s, episode steps: 659, steps per second: 467, episode reward: 95.000, mean reward:  0.144 [ 0.000, 30.000], mean action: 3.528 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   77726/5000000: episode: 102, duration: 1.964s, episode steps: 957, steps per second: 487, episode reward: 310.000, mean reward:  0.324 [ 0.000, 30.000], mean action: 3.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   78824/5000000: episode: 103, duration: 2.271s, episode steps: 1098, steps per second: 483, episode reward: 200.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 3.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   79765/5000000: episode: 104, duration: 1.931s, episode steps: 941, steps per second: 487, episode reward: 390.000, mean reward:  0.414 [ 0.000, 200.000], mean action: 3.645 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   80777/5000000: episode: 105, duration: 2.120s, episode steps: 1012, steps per second: 477, episode reward: 205.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 3.518 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   81271/5000000: episode: 106, duration: 1.155s, episode steps: 494, steps per second: 428, episode reward: 75.000, mean reward:  0.152 [ 0.000, 15.000], mean action: 3.686 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   81898/5000000: episode: 107, duration: 1.539s, episode steps: 627, steps per second: 407, episode reward: 145.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 3.576 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   83268/5000000: episode: 108, duration: 2.945s, episode steps: 1370, steps per second: 465, episode reward: 445.000, mean reward:  0.325 [ 0.000, 30.000], mean action: 3.567 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   84219/5000000: episode: 109, duration: 2.050s, episode steps: 951, steps per second: 464, episode reward: 170.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 3.616 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   84844/5000000: episode: 110, duration: 1.265s, episode steps: 625, steps per second: 494, episode reward: 190.000, mean reward:  0.304 [ 0.000, 25.000], mean action: 3.606 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   85342/5000000: episode: 111, duration: 1.169s, episode steps: 498, steps per second: 426, episode reward: 85.000, mean reward:  0.171 [ 0.000, 15.000], mean action: 3.703 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   86027/5000000: episode: 112, duration: 1.556s, episode steps: 685, steps per second: 440, episode reward: 170.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 3.672 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   87228/5000000: episode: 113, duration: 2.637s, episode steps: 1201, steps per second: 455, episode reward: 655.000, mean reward:  0.545 [ 0.000, 200.000], mean action: 3.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   87875/5000000: episode: 114, duration: 1.462s, episode steps: 647, steps per second: 443, episode reward: 215.000, mean reward:  0.332 [ 0.000, 30.000], mean action: 3.638 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   88749/5000000: episode: 115, duration: 1.902s, episode steps: 874, steps per second: 460, episode reward: 255.000, mean reward:  0.292 [ 0.000, 30.000], mean action: 3.621 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   89273/5000000: episode: 116, duration: 1.140s, episode steps: 524, steps per second: 460, episode reward: 85.000, mean reward:  0.162 [ 0.000, 25.000], mean action: 3.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   90745/5000000: episode: 117, duration: 3.195s, episode steps: 1472, steps per second: 461, episode reward: 300.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 3.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   91444/5000000: episode: 118, duration: 1.528s, episode steps: 699, steps per second: 457, episode reward: 285.000, mean reward:  0.408 [ 0.000, 30.000], mean action: 3.618 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   92085/5000000: episode: 119, duration: 1.364s, episode steps: 641, steps per second: 470, episode reward: 210.000, mean reward:  0.328 [ 0.000, 30.000], mean action: 3.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   92863/5000000: episode: 120, duration: 1.708s, episode steps: 778, steps per second: 455, episode reward: 205.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 3.631 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   93951/5000000: episode: 121, duration: 2.336s, episode steps: 1088, steps per second: 466, episode reward: 205.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 3.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   94641/5000000: episode: 122, duration: 1.521s, episode steps: 690, steps per second: 454, episode reward: 100.000, mean reward:  0.145 [ 0.000, 20.000], mean action: 3.567 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   95247/5000000: episode: 123, duration: 1.355s, episode steps: 606, steps per second: 447, episode reward: 110.000, mean reward:  0.182 [ 0.000, 20.000], mean action: 3.619 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   96193/5000000: episode: 124, duration: 2.067s, episode steps: 946, steps per second: 458, episode reward: 175.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 3.648 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   96921/5000000: episode: 125, duration: 1.607s, episode steps: 728, steps per second: 453, episode reward: 365.000, mean reward:  0.501 [ 0.000, 200.000], mean action: 3.592 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   97752/5000000: episode: 126, duration: 1.870s, episode steps: 831, steps per second: 444, episode reward: 235.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 3.641 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   98706/5000000: episode: 127, duration: 2.021s, episode steps: 954, steps per second: 472, episode reward: 285.000, mean reward:  0.299 [ 0.000, 30.000], mean action: 3.570 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   99415/5000000: episode: 128, duration: 1.509s, episode steps: 709, steps per second: 470, episode reward: 130.000, mean reward:  0.183 [ 0.000, 20.000], mean action: 3.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  100761/5000000: episode: 129, duration: 9.237s, episode steps: 1346, steps per second: 146, episode reward: 505.000, mean reward:  0.375 [ 0.000, 200.000], mean action: 3.608 [0.000, 5.000],  loss: 150.544169, mae: 21.232859, mean_q: 39.743644, mean_eps: 0.279924\n",
      "  101566/5000000: episode: 130, duration: 7.784s, episode steps: 805, steps per second: 103, episode reward: 155.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 1.648 [0.000, 5.000],  loss: 32.053353, mae: 31.967245, mean_q: 42.555348, mean_eps: 0.279767\n",
      "  102360/5000000: episode: 131, duration: 7.766s, episode steps: 794, steps per second: 102, episode reward: 320.000, mean reward:  0.403 [ 0.000, 200.000], mean action: 2.131 [0.000, 5.000],  loss: 23.098462, mae: 33.649746, mean_q: 41.015138, mean_eps: 0.279608\n",
      "  102953/5000000: episode: 132, duration: 5.828s, episode steps: 593, steps per second: 102, episode reward: 95.000, mean reward:  0.160 [ 0.000, 25.000], mean action: 2.369 [0.000, 5.000],  loss: 21.188504, mae: 33.299236, mean_q: 40.227113, mean_eps: 0.279469\n",
      "  103660/5000000: episode: 133, duration: 6.866s, episode steps: 707, steps per second: 103, episode reward: 160.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.402 [0.000, 5.000],  loss: 16.792696, mae: 33.399221, mean_q: 40.403841, mean_eps: 0.279339\n",
      "  104256/5000000: episode: 134, duration: 5.850s, episode steps: 596, steps per second: 102, episode reward: 105.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.253 [0.000, 5.000],  loss: 22.941955, mae: 33.444150, mean_q: 40.696524, mean_eps: 0.279208\n",
      "  104698/5000000: episode: 135, duration: 4.344s, episode steps: 442, steps per second: 102, episode reward: 110.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.285 [0.000, 5.000],  loss: 18.513064, mae: 33.425453, mean_q: 40.432336, mean_eps: 0.279105\n",
      "  105452/5000000: episode: 136, duration: 7.406s, episode steps: 754, steps per second: 102, episode reward: 105.000, mean reward:  0.139 [ 0.000, 30.000], mean action: 2.309 [0.000, 5.000],  loss: 16.412631, mae: 33.337487, mean_q: 40.351380, mean_eps: 0.278985\n",
      "  105988/5000000: episode: 137, duration: 5.292s, episode steps: 536, steps per second: 101, episode reward: 85.000, mean reward:  0.159 [ 0.000, 25.000], mean action: 2.205 [0.000, 5.000],  loss: 16.792503, mae: 33.526299, mean_q: 40.462223, mean_eps: 0.278856\n",
      "  106862/5000000: episode: 138, duration: 8.726s, episode steps: 874, steps per second: 100, episode reward: 195.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.423 [0.000, 5.000],  loss: 16.347544, mae: 33.408061, mean_q: 40.317783, mean_eps: 0.278715\n",
      "  107860/5000000: episode: 139, duration: 10.095s, episode steps: 998, steps per second:  99, episode reward: 165.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.097 [0.000, 5.000],  loss: 15.178538, mae: 33.417434, mean_q: 40.356496, mean_eps: 0.278528\n",
      "  108814/5000000: episode: 140, duration: 9.470s, episode steps: 954, steps per second: 101, episode reward: 165.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.505 [0.000, 5.000],  loss: 15.682478, mae: 33.258443, mean_q: 40.119770, mean_eps: 0.278333\n",
      "  109423/5000000: episode: 141, duration: 5.997s, episode steps: 609, steps per second: 102, episode reward: 50.000, mean reward:  0.082 [ 0.000, 15.000], mean action: 2.271 [0.000, 5.000],  loss: 15.400673, mae: 33.309256, mean_q: 40.083987, mean_eps: 0.278176\n",
      "  110068/5000000: episode: 142, duration: 6.403s, episode steps: 645, steps per second: 101, episode reward: 85.000, mean reward:  0.132 [ 0.000, 25.000], mean action: 2.124 [0.000, 5.000],  loss: 18.594986, mae: 33.495946, mean_q: 40.376358, mean_eps: 0.278051\n",
      "  110891/5000000: episode: 143, duration: 8.148s, episode steps: 823, steps per second: 101, episode reward: 155.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 2.435 [0.000, 5.000],  loss: 7.553276, mae: 34.867208, mean_q: 42.059176, mean_eps: 0.277904\n",
      "  111985/5000000: episode: 144, duration: 10.834s, episode steps: 1094, steps per second: 101, episode reward: 480.000, mean reward:  0.439 [ 0.000, 200.000], mean action: 2.387 [0.000, 5.000],  loss: 11.293969, mae: 34.832531, mean_q: 42.149022, mean_eps: 0.277713\n",
      "  112721/5000000: episode: 145, duration: 7.330s, episode steps: 736, steps per second: 100, episode reward: 135.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.484 [0.000, 5.000],  loss: 9.149409, mae: 34.895376, mean_q: 42.234190, mean_eps: 0.277529\n",
      "  113893/5000000: episode: 146, duration: 11.636s, episode steps: 1172, steps per second: 101, episode reward: 400.000, mean reward:  0.341 [ 0.000, 200.000], mean action: 2.502 [0.000, 5.000],  loss: 10.380824, mae: 34.775834, mean_q: 41.947367, mean_eps: 0.277339\n",
      "  115045/5000000: episode: 147, duration: 11.557s, episode steps: 1152, steps per second: 100, episode reward: 145.000, mean reward:  0.126 [ 0.000, 30.000], mean action: 2.460 [0.000, 5.000],  loss: 8.776471, mae: 34.810374, mean_q: 42.039774, mean_eps: 0.277106\n",
      "  116217/5000000: episode: 148, duration: 11.675s, episode steps: 1172, steps per second: 100, episode reward: 320.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.330 [0.000, 5.000],  loss: 7.454890, mae: 34.774823, mean_q: 41.940722, mean_eps: 0.276874\n",
      "  117172/5000000: episode: 149, duration: 9.541s, episode steps: 955, steps per second: 100, episode reward: 215.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 7.343322, mae: 34.723428, mean_q: 41.834433, mean_eps: 0.276661\n",
      "  118025/5000000: episode: 150, duration: 8.566s, episode steps: 853, steps per second: 100, episode reward: 360.000, mean reward:  0.422 [ 0.000, 200.000], mean action: 2.422 [0.000, 5.000],  loss: 10.131312, mae: 34.806968, mean_q: 42.248871, mean_eps: 0.276480\n",
      "  118722/5000000: episode: 151, duration: 7.092s, episode steps: 697, steps per second:  98, episode reward: 120.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.350 [0.000, 5.000],  loss: 7.390886, mae: 34.751192, mean_q: 41.776960, mean_eps: 0.276325\n",
      "  119991/5000000: episode: 152, duration: 13.313s, episode steps: 1269, steps per second:  95, episode reward: 200.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.787 [0.000, 5.000],  loss: 7.638411, mae: 34.731375, mean_q: 41.898996, mean_eps: 0.276129\n",
      "  120667/5000000: episode: 153, duration: 7.002s, episode steps: 676, steps per second:  97, episode reward: 135.000, mean reward:  0.200 [ 0.000, 20.000], mean action: 2.250 [0.000, 5.000],  loss: 14.203597, mae: 34.388140, mean_q: 41.619447, mean_eps: 0.275934\n",
      "  121478/5000000: episode: 154, duration: 7.873s, episode steps: 811, steps per second: 103, episode reward: 195.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.094 [0.000, 5.000],  loss: 9.882193, mae: 34.354462, mean_q: 41.464082, mean_eps: 0.275786\n",
      "  122065/5000000: episode: 155, duration: 5.920s, episode steps: 587, steps per second:  99, episode reward: 110.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.339 [0.000, 5.000],  loss: 7.628311, mae: 34.308868, mean_q: 41.312901, mean_eps: 0.275646\n",
      "  122729/5000000: episode: 156, duration: 7.046s, episode steps: 664, steps per second:  94, episode reward: 95.000, mean reward:  0.143 [ 0.000, 25.000], mean action: 2.021 [0.000, 5.000],  loss: 11.332150, mae: 34.414376, mean_q: 41.532193, mean_eps: 0.275521\n",
      "  123099/5000000: episode: 157, duration: 4.194s, episode steps: 370, steps per second:  88, episode reward: 20.000, mean reward:  0.054 [ 0.000, 10.000], mean action: 2.338 [0.000, 5.000],  loss: 10.177220, mae: 34.153465, mean_q: 41.246665, mean_eps: 0.275417\n",
      "  123764/5000000: episode: 158, duration: 8.333s, episode steps: 665, steps per second:  80, episode reward: 110.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 1.986 [0.000, 5.000],  loss: 8.440722, mae: 34.364525, mean_q: 41.442650, mean_eps: 0.275314\n",
      "  124824/5000000: episode: 159, duration: 12.084s, episode steps: 1060, steps per second:  88, episode reward: 205.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.439 [0.000, 5.000],  loss: 7.691364, mae: 34.343331, mean_q: 41.437271, mean_eps: 0.275141\n",
      "  125704/5000000: episode: 160, duration: 9.123s, episode steps: 880, steps per second:  96, episode reward: 220.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.357 [0.000, 5.000],  loss: 10.711636, mae: 34.320073, mean_q: 41.543677, mean_eps: 0.274947\n",
      "  126431/5000000: episode: 161, duration: 7.754s, episode steps: 727, steps per second:  94, episode reward: 155.000, mean reward:  0.213 [ 0.000, 30.000], mean action: 2.132 [0.000, 5.000],  loss: 7.029286, mae: 34.241948, mean_q: 41.280274, mean_eps: 0.274787\n",
      "  127298/5000000: episode: 162, duration: 9.218s, episode steps: 867, steps per second:  94, episode reward: 315.000, mean reward:  0.363 [ 0.000, 25.000], mean action: 2.467 [0.000, 5.000],  loss: 8.146683, mae: 34.258463, mean_q: 41.249394, mean_eps: 0.274627\n",
      "  128350/5000000: episode: 163, duration: 11.060s, episode steps: 1052, steps per second:  95, episode reward: 245.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.260 [0.000, 5.000],  loss: 8.025001, mae: 34.276278, mean_q: 41.401511, mean_eps: 0.274435\n",
      "  129242/5000000: episode: 164, duration: 9.343s, episode steps: 892, steps per second:  95, episode reward: 425.000, mean reward:  0.476 [ 0.000, 200.000], mean action: 2.368 [0.000, 5.000],  loss: 10.772443, mae: 34.314165, mean_q: 41.403895, mean_eps: 0.274241\n",
      "  130072/5000000: episode: 165, duration: 8.771s, episode steps: 830, steps per second:  95, episode reward: 165.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.116 [0.000, 5.000],  loss: 6.946400, mae: 34.252213, mean_q: 41.334395, mean_eps: 0.274069\n",
      "  130910/5000000: episode: 166, duration: 8.853s, episode steps: 838, steps per second:  95, episode reward: 180.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 1.659 [0.000, 5.000],  loss: 9.364113, mae: 33.688498, mean_q: 40.721859, mean_eps: 0.273902\n",
      "  131686/5000000: episode: 167, duration: 8.248s, episode steps: 776, steps per second:  94, episode reward: 155.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 1.629 [0.000, 5.000],  loss: 10.861869, mae: 33.542971, mean_q: 40.513963, mean_eps: 0.273740\n",
      "  132521/5000000: episode: 168, duration: 9.120s, episode steps: 835, steps per second:  92, episode reward: 135.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 1.726 [0.000, 5.000],  loss: 11.219468, mae: 33.633426, mean_q: 40.763210, mean_eps: 0.273579\n",
      "  133226/5000000: episode: 169, duration: 8.428s, episode steps: 705, steps per second:  84, episode reward: 20.000, mean reward:  0.028 [ 0.000, 10.000], mean action: 1.726 [0.000, 5.000],  loss: 10.836613, mae: 33.622325, mean_q: 40.554640, mean_eps: 0.273425\n",
      "  134075/5000000: episode: 170, duration: 10.625s, episode steps: 849, steps per second:  80, episode reward: 210.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 1.717 [0.000, 5.000],  loss: 10.208671, mae: 33.547422, mean_q: 40.556294, mean_eps: 0.273270\n",
      "  134895/5000000: episode: 171, duration: 9.784s, episode steps: 820, steps per second:  84, episode reward: 180.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 1.573 [0.000, 5.000],  loss: 10.368285, mae: 33.665377, mean_q: 40.653273, mean_eps: 0.273103\n",
      "  135761/5000000: episode: 172, duration: 9.641s, episode steps: 866, steps per second:  90, episode reward: 210.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 1.684 [0.000, 5.000],  loss: 7.536167, mae: 33.493156, mean_q: 40.401125, mean_eps: 0.272934\n",
      "  136385/5000000: episode: 173, duration: 7.078s, episode steps: 624, steps per second:  88, episode reward: 65.000, mean reward:  0.104 [ 0.000, 15.000], mean action: 1.894 [0.000, 5.000],  loss: 10.066074, mae: 33.555015, mean_q: 40.536493, mean_eps: 0.272786\n",
      "  137051/5000000: episode: 174, duration: 7.064s, episode steps: 666, steps per second:  94, episode reward: 75.000, mean reward:  0.113 [ 0.000, 25.000], mean action: 1.429 [0.000, 5.000],  loss: 8.668986, mae: 33.549616, mean_q: 40.419791, mean_eps: 0.272656\n",
      "  137846/5000000: episode: 175, duration: 8.558s, episode steps: 795, steps per second:  93, episode reward: 155.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 1.605 [0.000, 5.000],  loss: 9.613035, mae: 33.567421, mean_q: 40.526764, mean_eps: 0.272510\n",
      "  139341/5000000: episode: 176, duration: 15.746s, episode steps: 1495, steps per second:  95, episode reward: 415.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 1.553 [0.000, 5.000],  loss: 8.906147, mae: 33.599396, mean_q: 40.548718, mean_eps: 0.272281\n",
      "  139978/5000000: episode: 177, duration: 6.784s, episode steps: 637, steps per second:  94, episode reward: 115.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 1.436 [0.000, 5.000],  loss: 8.490428, mae: 33.594009, mean_q: 40.513868, mean_eps: 0.272068\n",
      "  140629/5000000: episode: 178, duration: 6.961s, episode steps: 651, steps per second:  94, episode reward: 110.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 1.814 [0.000, 5.000],  loss: 10.228885, mae: 33.941579, mean_q: 40.936278, mean_eps: 0.271939\n",
      "  141084/5000000: episode: 179, duration: 4.886s, episode steps: 455, steps per second:  93, episode reward: 75.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.114 [0.000, 5.000],  loss: 9.989977, mae: 34.136934, mean_q: 41.364477, mean_eps: 0.271829\n",
      "  141811/5000000: episode: 180, duration: 7.791s, episode steps: 727, steps per second:  93, episode reward: 95.000, mean reward:  0.131 [ 0.000, 25.000], mean action: 1.849 [0.000, 5.000],  loss: 7.378715, mae: 33.955053, mean_q: 40.916561, mean_eps: 0.271711\n",
      "  142750/5000000: episode: 181, duration: 10.050s, episode steps: 939, steps per second:  93, episode reward: 325.000, mean reward:  0.346 [ 0.000, 200.000], mean action: 1.872 [0.000, 5.000],  loss: 7.286342, mae: 34.001199, mean_q: 40.972506, mean_eps: 0.271544\n",
      "  143750/5000000: episode: 182, duration: 10.758s, episode steps: 1000, steps per second:  93, episode reward: 410.000, mean reward:  0.410 [ 0.000, 200.000], mean action: 1.989 [0.000, 5.000],  loss: 7.233752, mae: 33.907054, mean_q: 40.911448, mean_eps: 0.271350\n",
      "  144907/5000000: episode: 183, duration: 12.448s, episode steps: 1157, steps per second:  93, episode reward: 420.000, mean reward:  0.363 [ 0.000, 200.000], mean action: 2.261 [0.000, 5.000],  loss: 6.771857, mae: 34.022348, mean_q: 40.989171, mean_eps: 0.271134\n",
      "  145486/5000000: episode: 184, duration: 6.222s, episode steps: 579, steps per second:  93, episode reward: 165.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 2.361 [0.000, 5.000],  loss: 7.489911, mae: 33.991255, mean_q: 40.884848, mean_eps: 0.270961\n",
      "  146351/5000000: episode: 185, duration: 9.339s, episode steps: 865, steps per second:  93, episode reward: 425.000, mean reward:  0.491 [ 0.000, 200.000], mean action: 1.743 [0.000, 5.000],  loss: 5.918050, mae: 33.910918, mean_q: 40.864611, mean_eps: 0.270816\n",
      "  146984/5000000: episode: 186, duration: 7.743s, episode steps: 633, steps per second:  82, episode reward: 125.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.449 [0.000, 5.000],  loss: 10.196939, mae: 34.003710, mean_q: 41.084780, mean_eps: 0.270667\n",
      "  148030/5000000: episode: 187, duration: 15.245s, episode steps: 1046, steps per second:  69, episode reward: 275.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 1.897 [0.000, 5.000],  loss: 6.744096, mae: 33.906732, mean_q: 40.859348, mean_eps: 0.270499\n",
      "  148988/5000000: episode: 188, duration: 14.084s, episode steps: 958, steps per second:  68, episode reward: 225.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 1.762 [0.000, 5.000],  loss: 9.427371, mae: 33.925545, mean_q: 41.004669, mean_eps: 0.270298\n",
      "  149688/5000000: episode: 189, duration: 9.948s, episode steps: 700, steps per second:  70, episode reward: 115.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.390 [0.000, 5.000],  loss: 7.990780, mae: 33.928907, mean_q: 40.797985, mean_eps: 0.270133\n",
      "  150370/5000000: episode: 190, duration: 9.759s, episode steps: 682, steps per second:  70, episode reward: 155.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 1.870 [0.000, 5.000],  loss: 6.354610, mae: 33.818310, mean_q: 40.697675, mean_eps: 0.269994\n",
      "  151162/5000000: episode: 191, duration: 11.327s, episode steps: 792, steps per second:  70, episode reward: 160.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.149 [0.000, 5.000],  loss: 6.071670, mae: 33.877735, mean_q: 40.798339, mean_eps: 0.269847\n",
      "  151870/5000000: episode: 192, duration: 10.163s, episode steps: 708, steps per second:  70, episode reward: 150.000, mean reward:  0.212 [ 0.000, 25.000], mean action: 2.109 [0.000, 5.000],  loss: 5.182531, mae: 33.827177, mean_q: 40.713804, mean_eps: 0.269697\n",
      "  152535/5000000: episode: 193, duration: 9.706s, episode steps: 665, steps per second:  69, episode reward: 135.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.430 [0.000, 5.000],  loss: 4.339901, mae: 33.801992, mean_q: 40.690893, mean_eps: 0.269560\n",
      "  152886/5000000: episode: 194, duration: 5.280s, episode steps: 351, steps per second:  66, episode reward: 30.000, mean reward:  0.085 [ 0.000, 15.000], mean action: 2.547 [0.000, 5.000],  loss: 5.432912, mae: 33.812706, mean_q: 40.675873, mean_eps: 0.269458\n",
      "  153266/5000000: episode: 195, duration: 5.559s, episode steps: 380, steps per second:  68, episode reward: 35.000, mean reward:  0.092 [ 0.000, 10.000], mean action: 2.484 [0.000, 5.000],  loss: 8.724826, mae: 33.920597, mean_q: 40.882996, mean_eps: 0.269385\n",
      "  153947/5000000: episode: 196, duration: 9.784s, episode steps: 681, steps per second:  70, episode reward: 210.000, mean reward:  0.308 [ 0.000, 30.000], mean action: 2.700 [0.000, 5.000],  loss: 7.902108, mae: 33.915815, mean_q: 40.995731, mean_eps: 0.269279\n",
      "  154739/5000000: episode: 197, duration: 11.758s, episode steps: 792, steps per second:  67, episode reward: 160.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.124 [0.000, 5.000],  loss: 7.454492, mae: 33.855594, mean_q: 40.783402, mean_eps: 0.269132\n",
      "  155806/5000000: episode: 198, duration: 15.478s, episode steps: 1067, steps per second:  69, episode reward: 445.000, mean reward:  0.417 [ 0.000, 200.000], mean action: 2.412 [0.000, 5.000],  loss: 7.134198, mae: 33.914037, mean_q: 40.882943, mean_eps: 0.268946\n",
      "  156773/5000000: episode: 199, duration: 14.174s, episode steps: 967, steps per second:  68, episode reward: 330.000, mean reward:  0.341 [ 0.000, 200.000], mean action: 2.111 [0.000, 5.000],  loss: 6.832006, mae: 33.886513, mean_q: 40.805146, mean_eps: 0.268742\n",
      "  157856/5000000: episode: 200, duration: 15.830s, episode steps: 1083, steps per second:  68, episode reward: 210.000, mean reward:  0.194 [ 0.000, 30.000], mean action: 2.322 [0.000, 5.000],  loss: 6.685297, mae: 33.846615, mean_q: 40.798143, mean_eps: 0.268537\n",
      "  158948/5000000: episode: 201, duration: 15.723s, episode steps: 1092, steps per second:  69, episode reward: 315.000, mean reward:  0.288 [ 0.000, 30.000], mean action: 2.648 [0.000, 5.000],  loss: 6.280972, mae: 33.860075, mean_q: 40.768687, mean_eps: 0.268320\n",
      "  159624/5000000: episode: 202, duration: 9.750s, episode steps: 676, steps per second:  69, episode reward: 135.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 1.922 [0.000, 5.000],  loss: 6.897695, mae: 33.921984, mean_q: 40.854485, mean_eps: 0.268143\n",
      "  160153/5000000: episode: 203, duration: 7.058s, episode steps: 529, steps per second:  75, episode reward: 65.000, mean reward:  0.123 [ 0.000, 20.000], mean action: 2.093 [0.000, 5.000],  loss: 5.854220, mae: 33.556687, mean_q: 40.423138, mean_eps: 0.268022\n",
      "  160917/5000000: episode: 204, duration: 9.046s, episode steps: 764, steps per second:  84, episode reward: 135.000, mean reward:  0.177 [ 0.000, 30.000], mean action: 2.877 [0.000, 5.000],  loss: 7.632409, mae: 33.100636, mean_q: 39.862155, mean_eps: 0.267893\n",
      "  161564/5000000: episode: 205, duration: 7.857s, episode steps: 647, steps per second:  82, episode reward: 160.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.612 [0.000, 5.000],  loss: 5.439331, mae: 32.983713, mean_q: 39.760257, mean_eps: 0.267752\n",
      "  162223/5000000: episode: 206, duration: 7.902s, episode steps: 659, steps per second:  83, episode reward: 115.000, mean reward:  0.175 [ 0.000, 20.000], mean action: 2.432 [0.000, 5.000],  loss: 7.144937, mae: 33.084702, mean_q: 39.861061, mean_eps: 0.267621\n",
      "  162697/5000000: episode: 207, duration: 5.690s, episode steps: 474, steps per second:  83, episode reward: 105.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.300 [0.000, 5.000],  loss: 9.856961, mae: 33.104431, mean_q: 40.034818, mean_eps: 0.267508\n",
      "  163012/5000000: episode: 208, duration: 3.791s, episode steps: 315, steps per second:  83, episode reward: 20.000, mean reward:  0.063 [ 0.000, 10.000], mean action: 2.067 [0.000, 5.000],  loss: 9.856189, mae: 33.191455, mean_q: 40.135843, mean_eps: 0.267429\n",
      "  163659/5000000: episode: 209, duration: 7.838s, episode steps: 647, steps per second:  83, episode reward: 105.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.998 [0.000, 5.000],  loss: 8.550010, mae: 33.148915, mean_q: 40.026207, mean_eps: 0.267333\n",
      "  164747/5000000: episode: 210, duration: 13.076s, episode steps: 1088, steps per second:  83, episode reward: 145.000, mean reward:  0.133 [ 0.000, 30.000], mean action: 2.812 [0.000, 5.000],  loss: 9.110067, mae: 33.102606, mean_q: 39.993913, mean_eps: 0.267159\n",
      "  165942/5000000: episode: 211, duration: 14.365s, episode steps: 1195, steps per second:  83, episode reward: 515.000, mean reward:  0.431 [ 0.000, 200.000], mean action: 2.365 [0.000, 5.000],  loss: 6.391169, mae: 32.999315, mean_q: 39.746053, mean_eps: 0.266931\n",
      "  166467/5000000: episode: 212, duration: 6.594s, episode steps: 525, steps per second:  80, episode reward: 80.000, mean reward:  0.152 [ 0.000, 25.000], mean action: 2.341 [0.000, 5.000],  loss: 6.161435, mae: 33.158970, mean_q: 39.862232, mean_eps: 0.266759\n",
      "  167359/5000000: episode: 213, duration: 10.755s, episode steps: 892, steps per second:  83, episode reward: 420.000, mean reward:  0.471 [ 0.000, 200.000], mean action: 2.483 [0.000, 5.000],  loss: 8.113435, mae: 33.155597, mean_q: 40.074431, mean_eps: 0.266618\n",
      "  168308/5000000: episode: 214, duration: 11.475s, episode steps: 949, steps per second:  83, episode reward: 245.000, mean reward:  0.258 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 9.878872, mae: 33.050506, mean_q: 39.907359, mean_eps: 0.266433\n",
      "  169106/5000000: episode: 215, duration: 9.762s, episode steps: 798, steps per second:  82, episode reward: 250.000, mean reward:  0.313 [ 0.000, 30.000], mean action: 2.462 [0.000, 5.000],  loss: 8.164707, mae: 33.084529, mean_q: 39.864388, mean_eps: 0.266259\n",
      "  169677/5000000: episode: 216, duration: 6.975s, episode steps: 571, steps per second:  82, episode reward: 75.000, mean reward:  0.131 [ 0.000, 25.000], mean action: 2.292 [0.000, 5.000],  loss: 4.290652, mae: 32.978005, mean_q: 39.715151, mean_eps: 0.266122\n",
      "  170119/5000000: episode: 217, duration: 5.438s, episode steps: 442, steps per second:  81, episode reward: 135.000, mean reward:  0.305 [ 0.000, 30.000], mean action: 2.235 [0.000, 5.000],  loss: 7.750878, mae: 32.654082, mean_q: 39.377449, mean_eps: 0.266020\n",
      "  170543/5000000: episode: 218, duration: 5.035s, episode steps: 424, steps per second:  84, episode reward: 10.000, mean reward:  0.024 [ 0.000,  5.000], mean action: 2.200 [0.000, 5.000],  loss: 3.313088, mae: 31.916969, mean_q: 38.393252, mean_eps: 0.265934\n",
      "  171202/5000000: episode: 219, duration: 7.807s, episode steps: 659, steps per second:  84, episode reward: 55.000, mean reward:  0.083 [ 0.000, 20.000], mean action: 2.677 [0.000, 5.000],  loss: 7.690219, mae: 32.066966, mean_q: 38.630444, mean_eps: 0.265826\n",
      "  171613/5000000: episode: 220, duration: 4.991s, episode steps: 411, steps per second:  82, episode reward: 75.000, mean reward:  0.182 [ 0.000, 25.000], mean action: 2.893 [0.000, 5.000],  loss: 7.009659, mae: 31.860484, mean_q: 38.434900, mean_eps: 0.265719\n",
      "  172337/5000000: episode: 221, duration: 8.892s, episode steps: 724, steps per second:  81, episode reward: 150.000, mean reward:  0.207 [ 0.000, 25.000], mean action: 2.308 [0.000, 5.000],  loss: 6.525354, mae: 32.070701, mean_q: 38.669432, mean_eps: 0.265605\n",
      "  172699/5000000: episode: 222, duration: 4.691s, episode steps: 362, steps per second:  77, episode reward: 40.000, mean reward:  0.110 [ 0.000, 10.000], mean action: 2.663 [0.000, 5.000],  loss: 12.649419, mae: 31.958496, mean_q: 38.693833, mean_eps: 0.265496\n",
      "  173595/5000000: episode: 223, duration: 11.037s, episode steps: 896, steps per second:  81, episode reward: 160.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.335 [0.000, 5.000],  loss: 5.496064, mae: 32.035075, mean_q: 38.593875, mean_eps: 0.265371\n",
      "  174265/5000000: episode: 224, duration: 7.916s, episode steps: 670, steps per second:  85, episode reward: 120.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.599 [0.000, 5.000],  loss: 5.841605, mae: 31.979856, mean_q: 38.582817, mean_eps: 0.265214\n",
      "  175392/5000000: episode: 225, duration: 13.249s, episode steps: 1127, steps per second:  85, episode reward: 275.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.559 [0.000, 5.000],  loss: 7.462551, mae: 32.046340, mean_q: 38.750169, mean_eps: 0.265034\n",
      "  176539/5000000: episode: 226, duration: 13.384s, episode steps: 1147, steps per second:  86, episode reward: 195.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.677 [0.000, 5.000],  loss: 5.291499, mae: 32.008420, mean_q: 38.553322, mean_eps: 0.264807\n",
      "  178205/5000000: episode: 227, duration: 19.145s, episode steps: 1666, steps per second:  87, episode reward: 600.000, mean reward:  0.360 [ 0.000, 30.000], mean action: 2.410 [0.000, 5.000],  loss: 6.541389, mae: 32.059126, mean_q: 38.637673, mean_eps: 0.264526\n",
      "  179291/5000000: episode: 228, duration: 12.786s, episode steps: 1086, steps per second:  85, episode reward: 235.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.305 [0.000, 5.000],  loss: 4.487138, mae: 32.054598, mean_q: 38.525936, mean_eps: 0.264250\n",
      "  179973/5000000: episode: 229, duration: 8.290s, episode steps: 682, steps per second:  82, episode reward: 140.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 4.635812, mae: 32.148744, mean_q: 38.715268, mean_eps: 0.264074\n",
      "  180788/5000000: episode: 230, duration: 10.328s, episode steps: 815, steps per second:  79, episode reward: 155.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.956 [0.000, 5.000],  loss: 6.679672, mae: 31.687851, mean_q: 38.216796, mean_eps: 0.263924\n",
      "  181286/5000000: episode: 231, duration: 6.866s, episode steps: 498, steps per second:  73, episode reward: 55.000, mean reward:  0.110 [ 0.000, 15.000], mean action: 3.050 [0.000, 5.000],  loss: 3.401662, mae: 31.542526, mean_q: 37.933472, mean_eps: 0.263793\n",
      "  182558/5000000: episode: 232, duration: 16.951s, episode steps: 1272, steps per second:  75, episode reward: 355.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.731 [0.000, 5.000],  loss: 6.185655, mae: 31.699873, mean_q: 38.199007, mean_eps: 0.263616\n",
      "  183564/5000000: episode: 233, duration: 12.898s, episode steps: 1006, steps per second:  78, episode reward: 285.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 2.345 [0.000, 5.000],  loss: 6.236441, mae: 31.712801, mean_q: 38.236504, mean_eps: 0.263388\n",
      "  184531/5000000: episode: 234, duration: 12.489s, episode steps: 967, steps per second:  77, episode reward: 295.000, mean reward:  0.305 [ 0.000, 30.000], mean action: 2.494 [0.000, 5.000],  loss: 6.699823, mae: 31.714687, mean_q: 38.152728, mean_eps: 0.263191\n",
      "  185225/5000000: episode: 235, duration: 8.676s, episode steps: 694, steps per second:  80, episode reward: 90.000, mean reward:  0.130 [ 0.000, 15.000], mean action: 2.344 [0.000, 5.000],  loss: 8.824954, mae: 31.705579, mean_q: 38.253548, mean_eps: 0.263024\n",
      "  186363/5000000: episode: 236, duration: 14.555s, episode steps: 1138, steps per second:  78, episode reward: 215.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.254 [0.000, 5.000],  loss: 6.602002, mae: 31.748793, mean_q: 38.286554, mean_eps: 0.262841\n",
      "  187147/5000000: episode: 237, duration: 9.249s, episode steps: 784, steps per second:  85, episode reward: 105.000, mean reward:  0.134 [ 0.000, 25.000], mean action: 2.263 [0.000, 5.000],  loss: 5.085832, mae: 31.747077, mean_q: 38.231086, mean_eps: 0.262649\n",
      "  187857/5000000: episode: 238, duration: 8.230s, episode steps: 710, steps per second:  86, episode reward: 110.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.152 [0.000, 5.000],  loss: 5.105830, mae: 31.705830, mean_q: 38.127554, mean_eps: 0.262500\n",
      "  189035/5000000: episode: 239, duration: 13.606s, episode steps: 1178, steps per second:  87, episode reward: 615.000, mean reward:  0.522 [ 0.000, 200.000], mean action: 2.565 [0.000, 5.000],  loss: 7.619219, mae: 31.752782, mean_q: 38.305389, mean_eps: 0.262311\n",
      "  189672/5000000: episode: 240, duration: 7.276s, episode steps: 637, steps per second:  88, episode reward: 130.000, mean reward:  0.204 [ 0.000, 25.000], mean action: 2.799 [0.000, 5.000],  loss: 6.100696, mae: 31.753857, mean_q: 38.250434, mean_eps: 0.262129\n",
      "  190048/5000000: episode: 241, duration: 4.322s, episode steps: 376, steps per second:  87, episode reward: 20.000, mean reward:  0.053 [ 0.000, 10.000], mean action: 2.699 [0.000, 5.000],  loss: 3.042379, mae: 31.750478, mean_q: 38.138139, mean_eps: 0.262028\n",
      "  190913/5000000: episode: 242, duration: 10.025s, episode steps: 865, steps per second:  86, episode reward: 465.000, mean reward:  0.538 [ 0.000, 200.000], mean action: 2.690 [0.000, 5.000],  loss: 4.585970, mae: 32.251029, mean_q: 38.859714, mean_eps: 0.261904\n",
      "  191533/5000000: episode: 243, duration: 7.274s, episode steps: 620, steps per second:  85, episode reward: 65.000, mean reward:  0.105 [ 0.000, 30.000], mean action: 2.311 [0.000, 5.000],  loss: 3.153343, mae: 32.320227, mean_q: 38.858952, mean_eps: 0.261755\n",
      "  192167/5000000: episode: 244, duration: 7.733s, episode steps: 634, steps per second:  82, episode reward: 85.000, mean reward:  0.134 [ 0.000, 25.000], mean action: 2.295 [0.000, 5.000],  loss: 7.875683, mae: 32.252242, mean_q: 39.015811, mean_eps: 0.261630\n",
      "  192639/5000000: episode: 245, duration: 5.836s, episode steps: 472, steps per second:  81, episode reward: 35.000, mean reward:  0.074 [ 0.000, 10.000], mean action: 2.434 [0.000, 5.000],  loss: 8.341594, mae: 32.304013, mean_q: 38.914891, mean_eps: 0.261520\n",
      "  193257/5000000: episode: 246, duration: 7.746s, episode steps: 618, steps per second:  80, episode reward: 95.000, mean reward:  0.154 [ 0.000, 25.000], mean action: 2.489 [0.000, 5.000],  loss: 5.921979, mae: 32.294385, mean_q: 38.871997, mean_eps: 0.261410\n",
      "  193946/5000000: episode: 247, duration: 8.770s, episode steps: 689, steps per second:  79, episode reward: 215.000, mean reward:  0.312 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 6.336994, mae: 32.319728, mean_q: 38.983871, mean_eps: 0.261280\n",
      "  194637/5000000: episode: 248, duration: 8.516s, episode steps: 691, steps per second:  81, episode reward: 135.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.612 [0.000, 5.000],  loss: 3.314295, mae: 32.206895, mean_q: 38.755200, mean_eps: 0.261142\n",
      "  195048/5000000: episode: 249, duration: 4.954s, episode steps: 411, steps per second:  83, episode reward: 80.000, mean reward:  0.195 [ 0.000, 25.000], mean action: 2.326 [0.000, 5.000],  loss: 4.425602, mae: 32.251192, mean_q: 38.836248, mean_eps: 0.261032\n",
      "  195659/5000000: episode: 250, duration: 7.318s, episode steps: 611, steps per second:  83, episode reward: 80.000, mean reward:  0.131 [ 0.000, 20.000], mean action: 2.321 [0.000, 5.000],  loss: 5.401320, mae: 32.221293, mean_q: 38.766275, mean_eps: 0.260929\n",
      "  196451/5000000: episode: 251, duration: 9.956s, episode steps: 792, steps per second:  80, episode reward: 160.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.874 [0.000, 5.000],  loss: 4.451578, mae: 32.246687, mean_q: 38.844513, mean_eps: 0.260789\n",
      "  197241/5000000: episode: 252, duration: 9.559s, episode steps: 790, steps per second:  83, episode reward: 200.000, mean reward:  0.253 [ 0.000, 25.000], mean action: 2.592 [0.000, 5.000],  loss: 6.590708, mae: 32.232274, mean_q: 38.856344, mean_eps: 0.260631\n",
      "  197945/5000000: episode: 253, duration: 9.084s, episode steps: 704, steps per second:  77, episode reward: 90.000, mean reward:  0.128 [ 0.000, 25.000], mean action: 2.455 [0.000, 5.000],  loss: 5.845295, mae: 32.252317, mean_q: 38.841542, mean_eps: 0.260481\n",
      "  198505/5000000: episode: 254, duration: 7.110s, episode steps: 560, steps per second:  79, episode reward: 110.000, mean reward:  0.196 [ 0.000, 25.000], mean action: 2.057 [0.000, 5.000],  loss: 8.795053, mae: 32.282843, mean_q: 38.895508, mean_eps: 0.260355\n",
      "  199308/5000000: episode: 255, duration: 11.557s, episode steps: 803, steps per second:  69, episode reward: 170.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.534 [0.000, 5.000],  loss: 6.385970, mae: 32.251257, mean_q: 38.920824, mean_eps: 0.260219\n",
      "  199993/5000000: episode: 256, duration: 8.485s, episode steps: 685, steps per second:  81, episode reward: 165.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.635 [0.000, 5.000],  loss: 5.942865, mae: 32.214234, mean_q: 38.716682, mean_eps: 0.260070\n",
      "  200627/5000000: episode: 257, duration: 7.987s, episode steps: 634, steps per second:  79, episode reward: 70.000, mean reward:  0.110 [ 0.000, 25.000], mean action: 2.776 [0.000, 5.000],  loss: 5.216173, mae: 32.858681, mean_q: 39.600616, mean_eps: 0.259938\n",
      "  201183/5000000: episode: 258, duration: 7.205s, episode steps: 556, steps per second:  77, episode reward: 65.000, mean reward:  0.117 [ 0.000, 20.000], mean action: 2.559 [0.000, 5.000],  loss: 11.214089, mae: 32.897510, mean_q: 39.789666, mean_eps: 0.259819\n",
      "  201754/5000000: episode: 259, duration: 7.026s, episode steps: 571, steps per second:  81, episode reward: 80.000, mean reward:  0.140 [ 0.000, 25.000], mean action: 2.086 [0.000, 5.000],  loss: 5.391817, mae: 32.791770, mean_q: 39.409483, mean_eps: 0.259706\n",
      "  202122/5000000: episode: 260, duration: 4.414s, episode steps: 368, steps per second:  83, episode reward: 120.000, mean reward:  0.326 [ 0.000, 30.000], mean action: 2.625 [0.000, 5.000],  loss: 11.710407, mae: 32.824793, mean_q: 39.746251, mean_eps: 0.259612\n",
      "  202762/5000000: episode: 261, duration: 7.605s, episode steps: 640, steps per second:  84, episode reward: 145.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.928 [0.000, 5.000],  loss: 6.403161, mae: 32.773604, mean_q: 39.413422, mean_eps: 0.259512\n",
      "  203539/5000000: episode: 262, duration: 9.975s, episode steps: 777, steps per second:  78, episode reward: 115.000, mean reward:  0.148 [ 0.000, 25.000], mean action: 2.668 [0.000, 5.000],  loss: 3.832248, mae: 32.768421, mean_q: 39.387715, mean_eps: 0.259370\n",
      "  203920/5000000: episode: 263, duration: 4.775s, episode steps: 381, steps per second:  80, episode reward: 15.000, mean reward:  0.039 [ 0.000, 10.000], mean action: 3.055 [0.000, 5.000],  loss: 4.735814, mae: 32.743709, mean_q: 39.341827, mean_eps: 0.259254\n",
      "  204309/5000000: episode: 264, duration: 4.805s, episode steps: 389, steps per second:  81, episode reward: 45.000, mean reward:  0.116 [ 0.000, 15.000], mean action: 2.257 [0.000, 5.000],  loss: 6.692794, mae: 32.765916, mean_q: 39.660624, mean_eps: 0.259177\n"
     ]
    }
   ],
   "source": [
    "# compile fit and evaluate teh agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae']) \n",
    "train_history = dqn.fit(env, nb_steps=NB_STEPS,callbacks=[logger,checkpoint_callback], visualize=False, verbose=2)\n",
    "\n",
    "# save the weights\n",
    "dqn.save_weights(f'model/{file_name}_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae']) \n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the text file\n",
    "file = open(f\"training_logs_{file_name}.txt\").readlines()\n",
    "# its a list so just get the first and only text output\n",
    "text = file[0]\n",
    "\n",
    "# list of things in the file to remove such that i will be left with lists then loop to delete them\n",
    "char_del = ['{\"loss\": ',', \"mae\": ',', \"mean_q\": ',', \"mean_eps\": ',', \"episode_reward\": ',', \"nb_episode_steps\": ',', \"nb_steps\": ',', \"episode\": ',', \"duration\": ','}']\n",
    "textProc = []\n",
    "for i in range(10):\n",
    "    text = text.replace(char_del[i],\"\")\n",
    "#print(textProc)\n",
    "text = text.replace(\"[\",\"\")\n",
    "text = text.replace(\"NaN\",\"0\")\n",
    "\n",
    "# this turns it from a text thing to a list\n",
    "textProc = text.split(\"]\")[:-1]\n",
    "for i in range(len(textProc)):\n",
    "    textProc[i] = textProc[i].split(\",\")\n",
    "\n",
    "# list of the data columns\n",
    "cols = [\"loss\", \"mae\", \"mean_q\", \"mean_eps\", \"episode_reward\", \"nb_episode_steps\", \"nb_steps\", \"episode\", \"duration\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(textProc)):\n",
    "    df[cols[i]] = textProc[i]\n",
    "\n",
    "df.to_csv(f\"{file_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
